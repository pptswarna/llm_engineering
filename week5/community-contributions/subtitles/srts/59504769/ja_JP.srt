WEBVTT

00:00.590 --> 00:03.320
前置きはこれくらいにして､ さっそく本題に入ろう｡ 

00:03.350 --> 00:05.180
ローラについて語る

00:05.210 --> 00:07.430
低ランクの適応｡ 

00:07.670 --> 00:13.520
そしてこのセクションでは､ 数枚のスライドで少し理論的な話をするが､

00:13.550 --> 00:16.160
恐れることはない｡

00:16.190 --> 00:19.250
こうしたことを理解する最善の方法は､ 実際に見てみることだ｡ 

00:19.250 --> 00:24.350
だから､ スライドを2､ 3枚見た後､ Colabに行き､ これらのことを実際に見てみるつもりだ｡ 

00:24.350 --> 00:26.480
しかし､ まず文脈を整理する必要がある｡ 

00:26.480 --> 00:31.520
これからllama 3を使うんだ｡  今週はこれに1点｡ 

00:31.850 --> 00:34.340
そしてラマ3世｡  1には3つのサイズがある｡ 

00:34.340 --> 00:42.680
パラメータサイズは80億､ 700億､ そして4,050億という途方もないサイズがある｡ 

00:42.890 --> 00:46.100
ええと､ もちろん､ 小さいほうの80億を取ります｡ 

00:46.250 --> 01:01.370
GPUが1つで､ 80億ウェイトとなると､ すでに32GBのラムを搭載していることになります｡

01:01.400 --> 01:07.280
これを足すと､ そうなります｡ これは､ トレーニングを開始するときにモデルをメモリに保存しておくためで､

01:07.280 --> 01:14.210
最適化を実行することになります｡

01:14.270 --> 01:18.080
ええと､ それはあまりにも多くのメモリを消費するものだ｡ 

01:18.110 --> 01:19.880
希望はない｡ 

01:20.240 --> 01:30.470
80億のウェイトを最適化するのに膨大な時間がかかるからだ｡

01:30.470 --> 01:32.870
処理量が多いんだ｡ 

01:33.200 --> 01:38.630
もちろん､ フロンティア・ラボやメタのようなところは､

01:38.630 --> 01:46.850
最大級のモデルには1億ドル以上の巨費を投じている｡

01:46.850 --> 01:49.970
だから､ おそらく我々が使うようなお金ではない｡ 

01:50.150 --> 01:59.870
ベースモデルから訓練することで､ 驚くほど低コストで､

01:59.900 --> 02:11.030
特定のタスクを達成するのに優れたものを作ることができます｡

02:11.270 --> 02:18.500
ええと､ ローラが何なのかを説明する前に､ ラマ建築について簡単にまとめておこう｡ 

02:18.530 --> 02:22.790
さて､ このコースではニューラルネットワーク・アーキテクチャについて深入りするつもりはない｡ 

02:23.060 --> 02:28.340
これは､ 詳細には触れないが､ いくつかの洞察や直感をお伝えするものだ｡

02:28.340 --> 02:35.510
しかし､ ラマは3. 1 アーキテクチャは､ ニューロンの層のスタックとスタックで構成されている｡ 

02:35.660 --> 02:42.860
このレイヤーには32のグループがあり､ それぞれのグループが構成されているんだ｡ 

02:42.860 --> 02:45.740
つまり､ 各グループはラマ・デコーダー層と呼ばれる｡ 

02:45.740 --> 02:53.240
その中には､ 自己注意層､ 多層パーセプトロン層､ サイロ活性化層､ ノルム層がある｡

02:53.240 --> 02:55.220
これはすぐにわかる｡ 

02:55.250 --> 02:58.040
まさか......もしかしたら､ これが何なのか､ もう知っているかもしれない｡ 

02:58.070 --> 02:59.840
理論的なバックグラウンドがあればね｡ 

02:59.840 --> 03:03.410
そうでなければ､ よりリアルに､ より具体的になる｡ 

03:03.410 --> 03:07.040
Colabでこの建築を見るのはほんの一瞬だ｡ 

03:07.370 --> 03:14.480
そして､ これらのパラメーターはすべて､ この大きな､ あー､ あー､ この､ あー､ レイヤード・アーキテクチャーに張り付いていて､

03:14.510 --> 03:17.720
32ギガのメモリーを消費する｡

03:17.840 --> 03:22.520
これが今､ ローラを支える大きなアイデアだ｡ 

03:22.550 --> 03:30.020
つまり､ 私たちにできることは､ まずこれらのウェイトをすべて凍結させることだ｡ 

03:30.050 --> 03:35.360
通常､ 最適化の際にはニューラルネットワークをフォワードパスする｡ 

03:35.570 --> 03:46.700
ネットワークが予測した次のトークンを見て､ トークンがどうあるべきかを比較する｡

03:46.700 --> 03:51.620
そして､ それに基づいて､ 次に正しいトークンを予測するために､

03:51.650 --> 03:58.610
それぞれの重みをどれくらいずらせばいいかを考える｡

03:58.610 --> 04:00.530
それが最適化の考え方だ｡ 

04:00.560 --> 04:02.570
少し手探りだが､ おわかりだろう｡ 

04:02.600 --> 04:03.110
文字通りだ｡ 

04:03.140 --> 04:03.830
ハンドウェービー｡ 

04:03.920 --> 04:09.230
ええと､ でも......ローラのコンセプトは､ まず第一に､ すべての重りを解放することなんだ｡ 

04:09.230 --> 04:19.080
80億ものウェイトを最適化するのは､ あまりに多すぎるし､ 回すノブやグラデーションが多すぎるからだ｡

04:19.380 --> 04:27.330
その代わりに､ 私たちがトレーニングしたいと思う重要なレイヤーをいくつか選ぶ｡ 

04:27.330 --> 04:36.840
そして､ これらのレイヤー､ つまり､ この積み重ねられた､ あー､ レイヤーアーキテクチャーのこれらのモジュールは､ ターゲットモジュールとして知られている｡

04:36.840 --> 04:39.960
だから､ このターゲット・モジュールという表現はそこからきている｡ 

04:39.960 --> 04:45.480
と言うと､ 『スタートレック』に出てきそうだが､ これはトレーニングのために注目するニューラルネットワークのレイヤーを意味するだけで､

04:45.480 --> 04:52.200
重みは凍結されたままである｡

04:52.890 --> 05:05.700
その代わりに､ アダプター行列と呼ばれる次元数の少ない行列を新たに作成する｡

05:05.730 --> 05:09.840
これらは次元が小さいか､ ランクが低くなる｡ 

05:09.840 --> 05:24.420
そして､ これらのマトリックスをターゲット・モジュールに適用するテクニックを身につけるのだ｡

05:24.420 --> 05:27.030
だから､ 彼らはターゲット・モジュールに合わせるだろう｡ 

05:27.030 --> 05:30.510
その方法については後で説明する｡ 

05:30.510 --> 05:37.950
しかし､ この計算式は､ 将来､ 青い低ランクのアダプターにどんな値が入っていたとしても､ 微妙にずれることを意味する｡

05:37.950 --> 05:42.060
ターゲット・モジュールの中身を少し変えてみる｡ 

05:42.060 --> 05:49.800
より低いランク､ より低い次元､ より少ない重みで､ これらのターゲット・モジュールに適用されるようにするのだ｡

05:50.820 --> 05:54.450
そして､ ちょっとした技術的な問題が1つある｡ 

05:54.480 --> 06:00.000
このようなニューラルネットワークでは次元が機能するため､ 実際には低ランクの行列が2つあり､

06:00.000 --> 06:06.120
1つはa､ もう1つはBとして知られている｡

06:06.420 --> 06:09.270
コードを見てもらえばわかるが､ これらはローラaとローラbと呼ばれている｡ 

06:09.300 --> 06:11.310
つまり､ 2つのマトリックスがある｡ 

06:11.310 --> 06:16.230
それを知ることはそれほど重要なことではないが､ コードの中でそれを見たときに確認してもらいたい｡ 

06:16.230 --> 06:18.780
これを見て､ 2つの行列があることがわかるだろう｡ 

06:18.780 --> 06:20.910
それらはターゲットモジュールに適用される｡ 

06:20.910 --> 06:22.290
これは理にかなっている｡ 

06:22.710 --> 06:27.420
そして､ それがローラ・フリアスの物語なのだ｡ 

06:27.420 --> 06:34.290
メインモデルは､ 次元数の少ない小さな行列の束で構成される｡ 

06:34.290 --> 06:36.060
これらはトレーニングの対象となる｡ 

06:36.060 --> 06:42.660
彼らはトレーニングを受けた後､ 簡単な計算式を使ってターゲット・モジュールに適用される｡ 

06:42.990 --> 06:49.380
そうすれば､ 学習すればするほど良くなるベースモデルを作ることができる｡ 

06:49.380 --> 06:53.910
これらのローラ行列が適用されるからだ｡ 

06:53.910 --> 07:02.130
ローラとはローランク・アダプテーションの略で､ 低ランク､ 低次元､ そしてターゲット・モジュールに適応することを意味する｡

07:02.400 --> 07:02.970
これでよし｡ 

07:02.970 --> 07:07.650
多くのことを話し､ 多くの言葉を並べたが､ 願わくば､ これがどのように組み合わされているのか､

07:07.650 --> 07:11.970
直感的に理解してほしい｡

07:12.150 --> 07:19.230
うーん､ でも次のセッションでは､ もうひとつだけ､ Q（量子化）について手短にお話ししましょう｡ 
