WEBVTT

00:00.590 --> 00:03.320
그럼 지체 없이 get it로 들어가 보죠

00:03.350 --> 00:05.180
로라 말이에요

00:05.210 --> 00:07.430
낮은 순위 적응력이죠

00:07.670 --> 00:13.520
이 섹션에서 슬라이드 몇 개 동안 이론에 관해 얘기할 거예요 하지만 걱정 마세요 늘 그렇듯 바로

00:13.550 --> 00:16.160
연습할 거니까요 Get in get

00:16.190 --> 00:19.250
이런 걸 이해하는 가장 좋은 방법은 직접 보는 거예요

00:19.250 --> 00:24.350
슬라이드 몇 개 본 후에 Colab을 누르고 실제로 살펴보죠

00:24.350 --> 00:26.480
하지만 먼저 컨텍스트를 설정하죠

00:26.480 --> 00:31.520
자, llama 3을 사용할 거예요 이번 주에 1달러예요

00:31.850 --> 00:34.340
라마 3도요 1은 세 가지 사이즈가 있어요

00:34.340 --> 00:42.680
80억 매개 변수로 700억에 달하는 거대한 규모입니다 무려 4050억이죠

00:42.890 --> 00:46.100
물론 더 작은 80억 달러도 받을 거예요

00:46.250 --> 00:53.630
하지만 그것도 너무 커질 거예요 현실적으로 훈련하기에는요 하나의 GPU

00:53.660 --> 00:59.450
박스에 지불하고 싶은 비용으로요 80억 개의 무게라면 램

00:59.450 --> 01:01.370
32GB죠

01:01.400 --> 01:07.280
모두 더하면 메모리 안에 있는 모델이 됩니다. 훈련을 시작할 때 최적화

01:07.280 --> 01:14.210
실행을 할 수 있습니다. 각각의 값에 따라 그러데이션을 얻을 수 있어야 하죠.

01:14.270 --> 01:18.080
메모리를 너무 많이 쓰는 거예요

01:18.110 --> 01:19.880
희망이 없었을 거예요

01:20.240 --> 01:27.110
시간도 엄청나게 오래 걸릴 거예요 최적화할 게 너무 많으니까요 80억

01:27.110 --> 01:30.470
개의 무게를 최적화하려면요

01:30.470 --> 01:32.870
처리할 게 많네요

01:33.200 --> 01:38.630
물론 이런 프런티어 연구소와 메타 연구소들이 대형 모델에

01:38.630 --> 01:46.850
많은 돈을 투자한 덕분이죠 한 대를 훈련하는 데 1억 달러 이상이 들었어요

01:46.850 --> 01:49.970
그래서 아마 우리가 쓸 만한 돈은 아닐 거예요

01:50.150 --> 01:59.870
그래서 기본 모델을 훈련하는 데 놀라울 정도로 적은 비용과 요령이 필요합니다 그래야 특정

01:59.900 --> 02:05.390
작업을 더 잘 수행할 수 있으니까요 기본 모델이 원래

02:05.390 --> 02:11.030
훈련받은 것과 공통점이 많다고 가정하면요

02:11.270 --> 02:18.500
로라가 뭔지 설명하기 전에 라마 건축 양식을 간단히 요약해 볼게요

02:18.530 --> 02:22.790
Get in GAME 이 과정에선 신경망 구조를 깊이 다루진 않을 거예요

02:23.060 --> 02:28.340
아주 자세히는 설명하지 않고 직관적으로 설명해 드릴게요

02:28.340 --> 02:35.510
라마 3은 어때요? 뉴런이 층층이 쌓여 있는 형태예요

02:35.660 --> 02:42.860
각 그룹이 구성된 이런 레이어 그룹이 32개예요

02:42.860 --> 02:45.740
각 그룹을 라마 디코더 층이라고 해요

02:45.740 --> 02:51.980
자기 주의력 계층과 다중 인터셉트론 계층 사일로 활성화 계층과 표준 계층이

02:51.980 --> 02:53.240
있어요

02:53.240 --> 02:55.220
잠시 후에 이걸 보죠

02:55.250 --> 02:58.040
이미 어떤 상황인지 알고 있을지도 몰라요

02:58.070 --> 02:59.840
이론적 배경이 있다면요

02:59.840 --> 03:03.410
그렇지 않다면 더 현실적이고 실재할 거예요

03:03.410 --> 03:07.040
콜랍에 있는 이 건축물을 보면요

03:07.370 --> 03:14.480
이 모든 매개 변수들이 이 큰, 이 계층 아키텍처에 달라붙어 32기가

03:14.510 --> 03:17.720
메모리를 차지해요

03:17.840 --> 03:22.520
이게 로라의 원대한 아이디어예요

03:22.550 --> 03:30.020
우선 이 추들을 전부 얼릴 거예요

03:30.050 --> 03:35.360
보통 최적화 과정에서는 신경망을 통해 전진 패스를 해요

03:35.570 --> 03:41.780
네트워크가 예측한 다음 토큰을 어떻게 분석할지 살펴보고 원래 토큰이 있어야

03:41.780 --> 03:46.700
할 것과 비교해 보세요 진짜 다음 토큰이 무엇일까요?

03:46.700 --> 03:51.620
그런 다음 그걸 바탕으로 각 비트를 조금씩 바꿔서

03:51.650 --> 03:58.610
다음번에는 올바른 토큰을 예측할 수 있도록 하는 거죠

03:58.610 --> 04:00.530
그게 최적화의 개념이죠

04:00.560 --> 04:02.570
약간 웨이브 비트 같지만 느낌은 오죠 get it get it

04:02.600 --> 04:03.110
말 그대로요

04:03.140 --> 04:03.830
수작업으로요

04:03.920 --> 04:09.230
하지만 로라의 콘셉트는 우선 이 모든 역기를 자유롭게 한다는 거죠

04:09.230 --> 04:16.320
80억 개의 무게는 최적화하지 않을 겁니다 너무 많으니까요 너무 많은 노브와

04:16.350 --> 04:19.080
그러데이션이 있어요

04:19.380 --> 04:27.330
대신 훈련하고 싶은 핵심 사항을 몇 가지 골라요

04:27.330 --> 04:35.310
이 층과 모듈 안에 이렇게 층층이 쌓인 구조물을 목표 모듈이라고

04:35.310 --> 04:36.840
해요

04:36.840 --> 04:39.960
그래서 이 표현 대상 모듈이 있는 거죠

04:39.960 --> 04:45.480
스타트렉에 나오는 비트 같다고 했는데 신경망의 층을

04:45.480 --> 04:51.300
말하는 거예요 훈련에 집중할 수 있지만 무게는 여전히 얼어

04:51.300 --> 04:52.200
있죠

04:52.890 --> 05:01.830
대신 새로운 매트릭스인 어댑터 매트릭스를 소개할 거예요 적은 수의 치수죠 실제 치수에

05:01.830 --> 05:05.700
비해서 치수가 많지는 않아요

05:05.730 --> 05:09.840
입체감이 작거나 계급이 낮을 거예요

05:09.840 --> 05:21.750
이 물질을 한쪽으로 몰면 이 물질을 표적 모듈에 바르는 기술을 익힐

05:21.750 --> 05:24.420
수 있어요

05:24.420 --> 05:27.030
그래서 목표 모듈을 수정할 거예요

05:27.030 --> 05:30.510
공식이 있는데 잠시 후에 말씀드릴게요

05:30.510 --> 05:36.390
하지만 이 공식은 미래에는 저 파란색 로 랭크 어댑터에 있는 값이 조금씩 바뀔

05:36.390 --> 05:37.950
거라는 걸 의미하죠

05:37.950 --> 05:42.060
대상 모듈에서 일어나는 일을 약간 바꿀 거예요

05:42.060 --> 05:48.240
등급도 낮고 차원도 낮아서 표적 모듈에 가해지는 무게가 적게 들도록

05:48.240 --> 05:49.800
개조했어요

05:50.820 --> 05:54.450
잠시 후에 보시겠지만 기술적인 문제가 있어요

05:54.480 --> 06:00.000
이 말을 꼭 해야겠네요 이런 신경망에서 치수가 작동하는 방식을

06:00.000 --> 06:06.120
보면 낮은 순위의 행렬은 두 가지예요 하나는 A, 하나는 B죠

06:06.420 --> 06:09.270
코드에는 로라 a와 로라 b라고 적혀 있어요

06:09.300 --> 06:11.310
행렬은 두 가지가 있어요

06:11.310 --> 06:16.230
그걸 아는 게 그리 중요하진 않지만 코드에서 그걸 보시도록 확실히 해두고 싶어요

06:16.230 --> 06:18.780
이걸 보면 두 행렬이 있다고 생각할 거예요

06:18.780 --> 06:20.910
Get을 대상 모듈에 적용해요

06:20.910 --> 06:22.290
말 되네요

06:22.710 --> 06:27.420
로라 프레스의 숨은 이야기가 아주 깊어요

06:27.420 --> 06:34.290
본래는 치수가 적은 작은 행렬을 많이 만들었어요

06:34.290 --> 06:36.060
훈련 대상이죠

06:36.060 --> 06:42.660
get을 훈련한 다음 간단한 공식을 이용해 목표 모듈에 적용할 거예요

06:42.990 --> 06:49.380
그렇게 기본 모델을 만들 수 있어요 get이 학습할수록 더 좋아지는 거죠

06:49.380 --> 06:53.910
로라 매트릭스 응용 프로그램 덕분이죠

06:53.910 --> 07:00.600
로라는 낮은 서열의 적응을 의미해요 서열도 낮고, 공간도 낮고 목표 모듈에

07:00.600 --> 07:02.130
적응하거든요

07:02.400 --> 07:02.970
됐어요

07:02.970 --> 07:07.650
많은 대화와 단어가 오가지만 잘 조합할 수 있는 직관을 가지길

07:07.650 --> 07:11.970
바랍니다 코드를 보면 그 직관이 더 명확해질 거예요

07:12.150 --> 07:19.230
다음 시간엔 하나만 더 간단히 얘기할게요 Q, 퀀타이즈요
