WEBVTT

00:00.710 --> 00:11.810
そして､ いくつかの図をお見せすることで､ このことを実感していただこう｡ 特に今､ チュローラを使ったトレーニングがどのように機能するのかを見てみよう｡

00:12.440 --> 00:16.280
ではまず､ このフォワードパスについて｡ 

00:16.280 --> 00:20.780
というわけで､ 以前私が使ったのと同じ図なので､ お馴染みのはずである｡ 

00:20.810 --> 00:27.350
これがラマ3世だ｡  1ベースモデルは､ 80億のパラメーターと白を持つ4ビットにまで量子化され､

00:27.380 --> 00:31.340
そのすべてが凍結されている｡

00:31.340 --> 00:36.350
トレーニングの一環としてウェイトを変更するつもりはない｡ なぜなら､ 80億ものパラメーターを変化させ､

00:36.380 --> 00:43.400
微調整し､ 最適化しようとするのは､ 作業量が多すぎるし､ メモリも多すぎるし､ 時間もかかりすぎるからだ｡

00:43.850 --> 00:53.180
ここにあるのは凍結されたウェイトの列で､ 黄色いものも凍結されているが､ これはターゲット・モジュールを表している｡

00:53.180 --> 00:55.940
どの色を塗るのか｡ 

00:56.330 --> 00:59.210
そして､ ローラのアダプターを持ち込もう｡ 

00:59.210 --> 00:59.790
あそこだ｡ 

00:59.820 --> 01:10.380
技術的に正確を期すために､ 実際には､ それぞれの異なるウム・ターゲット・モジュールに対してAとBと呼ばれる2つのアダプターがあることを覚えているかもしれない｡

01:10.440 --> 01:15.420
次元はRで与えられ､ アルファはスケーリング係数であることを覚えているだろう｡ 

01:15.420 --> 01:22.380
そして､ これらがターゲット・モジュールに適用される方法は､ アルファ×A×Bという単純なものだ｡

01:22.710 --> 01:24.450
簡単なことだ｡ 

01:24.450 --> 01:27.750
これが私たちがよく知っているニューラルネットワークだ｡ 

01:27.780 --> 01:33.270
そして､ このローラ・アダプターでウェイトトレーニングを行うのだが､ 我々の場合､

01:33.300 --> 01:36.090
約109MB分のウェイトがある｡

01:36.780 --> 01:38.280
ああ､ わかった｡ 

01:38.280 --> 01:40.290
では､ どうなるのか？

01:40.290 --> 01:42.900
左側に入力プロンプトがあります｡ 

01:42.900 --> 01:47.760
値段はドルで､ 次のトークンという感じだ｡ 

01:47.760 --> 01:50.070
私たちは､ モデルが予測を得意にすることを望んでいる｡ 

01:50.520 --> 02:02.400
フォワード・パスとは､ それを推論モードにして､ 次のトークンを予測してくださいというモデルに通すことです｡

02:03.360 --> 02:10.260
そして､ それがモデルを通って､ 反対側に出てくるのが､ 次のトークン価格の予測だ｡

02:10.290 --> 02:11.820
そして99｡ 

02:11.820 --> 02:19.170
そしてまた､ 我々はラマ3のこのシンプルさを利用している｡  1 実際には99は1つのトークンである｡

02:19.320 --> 02:20.400
うーん､ そうじゃない｡ 

02:20.430 --> 02:23.730
そしてそれは､ どんな3桁の数字でも常にそうである｡ 

02:23.730 --> 02:27.300
それが重要だというわけではないが､ 僕らにとっては少しシンプルになる｡ 

02:27.300 --> 02:29.190
それがフォワードパスだ｡ 

02:29.220 --> 02:29.820
分かった｡ 

02:29.850 --> 02:32.040
さて､ 次は損失計算だ｡ 

02:32.610 --> 02:33.780
だからまたここに来た｡ 

02:33.780 --> 02:35.400
私たちは次のトークンを予測した｡ 

02:35.400 --> 02:41.400
これでモデルは､ トレーニングの過程でモデルが何をしたかを調べることができる｡ 

02:41.400 --> 02:44.010
SFTのトレーナーは上を見ている｡ 

02:44.010 --> 02:45.960
次のトークンは何だったのだろう｡ 

02:45.960 --> 02:49.080
トレーニングデータを得たので､ 実際の次のトークンはわかっている｡ 

02:49.080 --> 02:50.100
それは何だったのか？

02:50.100 --> 02:51.810
仮に89歳だとしよう｡ 

02:51.810 --> 02:52.590
もっと低かった｡ 

02:52.590 --> 02:54.000
だから我々は間違っていた｡ 

02:54.000 --> 02:55.800
10ドルの間違い｡ 

02:56.070 --> 02:57.370
うーん､ あるいは間違っている｡ 

02:57.370 --> 03:00.280
別の言い方をすれば､ これが10ドルであることを知らない｡ 

03:00.280 --> 03:02.350
ただ､ それが別のトークンだと知っているだけなのだ｡ 

03:02.710 --> 03:05.650
だから､ 何らかの損失がある｡ 

03:05.650 --> 03:10.180
その損失が何なのか､ なぜトークンが違うだけという単純なものではないのか､

03:10.180 --> 03:11.500
これから説明する｡

03:11.500 --> 03:16.960
技術的なことは後ほど説明するが､ 今は99を予測したと思ってくれればいい｡

03:16.990 --> 03:18.670
実際の値は89である｡ 

03:18.700 --> 03:20.050
負けた｡ 

03:20.680 --> 03:23.140
これが損失計算だ｡ 

03:23.140 --> 03:29.350
ステップ3はバックワードパスで､ バックプロップとかバックワードプロパゲーションと呼ばれている｡ 

03:29.350 --> 03:30.580
逆伝播｡ 

03:30.760 --> 03:38.470
バックプロップでは基本的に､ ネットワークを振り返って､ 戻って､ こう言うんだ｡ もしこの重みを少しいじったら､

03:38.470 --> 03:44.650
どのくらい損失に影響するだろうか？

03:44.650 --> 03:48.850
その重さに対して､ ロスはどの程度敏感なのか｡ 

03:48.880 --> 03:54.280
これにより､ パラメーターの重みの勾配と呼ばれるものが得られる｡ 

03:54.280 --> 03:57.320
だから､ ウェイトというのはパラメーターと同義なんだ｡ 

03:57.560 --> 04:00.770
ええと､ それで､ ええと､ どうやって？

04:00.800 --> 04:02.120
グラデーションとは？

04:02.120 --> 04:05.240
もしウェイトを変えたとしたら､ 損失はどうなる？

04:05.240 --> 04:08.030
少しは改善したいと思っているからだ｡ 

04:08.360 --> 04:16.100
赤い三角形はデルタを表すもので､ そこで起こったグラデーション計算を表している｡

04:17.180 --> 04:22.730
そして最後のステップ､ ステップ4の最適化だ｡ 

04:22.940 --> 04:23.960
来たぞ｡ 

04:23.960 --> 04:26.360
グラデーションができた｡ 

04:26.420 --> 04:30.770
そして今､ 私たちに必要なのは､ 正しい方向に小さな一歩を踏み出すことだ｡ 

04:30.770 --> 04:36.830
そこで､ ローラ行列のパラメータを少しだけ更新したい｡ 

04:36.830 --> 04:41.720
そうすれば､ 次に同じ入力プロンプトが表示されたとしても､ 損失はもう少し少なくなる｡ 

04:41.720 --> 04:42.890
もう少しうまくいくだろう｡ 

04:42.890 --> 04:48.980
私たちは正しい方向に一歩を踏み出し､ 学習率を使ってどの程度踏み出すかを決めているわけです｡

04:48.980 --> 04:53.810
前にも言ったように､ 小さなステップを踏むことにも大きなステップを踏むことにも長所と短所があるからだ｡ 

04:54.090 --> 05:00.690
私たちが使っているオプティマイザー､ アダムWオプティマイザーは､ 単に勾配を使うだけでなく､

05:00.690 --> 05:12.540
事前の勾配のローリング平均のようなものを保持することで､ 物事を改善する可能性が最も高い方法でステップを踏む方法について本当に賢くなるのです｡

05:12.660 --> 05:18.810
また､ オーバーフィッティングなど､ 前にも話したような危険なことをしないようにするためでもある｡

05:19.200 --> 05:20.370
だからそうするんだ｡ 

05:20.370 --> 05:26.430
そして､ ローラ・アダプターが改良され､ 将来的に我々のベースモデルに適用されることで､

05:26.430 --> 05:29.700
少し良い結果が得られるというわけだ｡

05:29.730 --> 05:40.740
そのため､ トレーニングの損失が減少し､ チャートがぐねぐねしているのがわかるだろう｡

05:41.070 --> 05:46.050
もちろん､ ウェイトやパラメータが変更されるのは､ ローラ・アダプタのパラメータ､

05:46.050 --> 05:55.620
つまり緑色のものであり､ ラマ3ではパラメータを変更することはない｡

05:55.620 --> 05:55.620
大きすぎる1ベース､ 多すぎるパラメータ｡ 

05:55.620 --> 06:02.280
普通のトレーニング､ 微調整､ ローラベースではなく､ 普通のトレーニングをしているのであれば､

06:02.280 --> 06:10.320
ラマ3全体になるだろう｡ 勾配を計算する必要があり､ 最適化中にシフトさせる必要がある1つのモデル｡

06:10.320 --> 06:13.650
そしてそれはもちろん､ これらの大企業が好むメタである｡ 

06:13.680 --> 06:25.620
これがラマ3のトレーニング方法だ｡  そのため､ 我々はローラ・アダプターを代わりに使っている｡

06:26.190 --> 06:32.160
これが､ トレーニング・プロセス全体における最適化の簡単なまとめだ｡ 

06:32.160 --> 06:37.530
そして､ それはおそらく､ すでにあなたにとってほとんど明確なものだったと想像している｡ しかし､ この図があなたにとってそれを結晶化させ､

06:37.530 --> 06:41.310
すべてがうまくいったことを意味することを期待している｡

06:41.310 --> 06:47.550
そして次のビデオでは､ もうひとつ技術的なことを説明しようと思う｡ 予想と損失計算の意味について､

06:47.550 --> 06:51.570
とても重要な技術的なことだ｡

06:51.600 --> 06:52.380
ではまた
