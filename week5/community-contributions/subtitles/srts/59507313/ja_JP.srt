WEBVTT

00:00.920 --> 00:06.260
そしてまたこの時期､ 我々のモデルが全体的にどのようなパフォーマンスを見せているかを表彰台で見ることになる｡ 

00:06.260 --> 00:12.050
答えはもうわかっているだろうが､ とにかく見て､ 笑ってみよう｡ 

00:12.320 --> 00:18.590
まず､ トレーニングデータセットから平均値を推測する定数モデルを見てみよう｡

00:18.950 --> 00:22.880
その数字を見てみると､ 146だった｡ 

00:23.000 --> 00:26.150
今は明らかに気温が下がっている｡ 

00:26.150 --> 00:28.070
その理由は､ おそらくあなたも知っていると思う｡ 

00:28.370 --> 00:36.260
従来の機械学習モデルは､ 平均を上回ることができた｡

00:36.470 --> 00:44.510
ランダムフォレスト・アプローチを検討したところ､ 従来の機械学習アプローチでは97点と最も優れていた｡

00:44.510 --> 00:53.870
人間と比較したところ､ 人間は基本的で単純な特徴には勝てたが､ ランダムフォレストモデルには負けた｡

00:54.140 --> 01:00.290
GPTフォーゼロは､ 私たちが調べた様々なフロンティアモデルの中で最も優れていて､

01:00.290 --> 01:04.610
76がこの人間を大きく上回っていた｡

01:04.820 --> 01:15.310
そして今日は､ 3.ファインチューン・バス・ラマを見てきた｡  180億ものパラメーターが4ビットにまで量子化されている｡

01:15.430 --> 01:21.190
ええと､ 396で､ 壊滅的に悪かった｡ 

01:21.280 --> 01:29.950
そして､ 8ビットに量子化するとわずかに良くなり､ 代わりに301ドルの誤差が生じた｡

01:30.160 --> 01:37.780
ええと､ 明らかにラマ3は出だしが悪いね｡  1だが､ ある意味､ 挑戦が始まっているのでエキサイティングだ｡

01:37.870 --> 01:45.640
ええと､ ここにモデルを用意してあるので､ どうすればもっと良くなるかを試行錯誤することができる｡ 

01:45.640 --> 01:54.460
私たちの目標は､ GPT4のような何兆ものパラメーターを持つモデルに負けないようにすることです｡

01:54.460 --> 01:59.530
もし私たちがこの特別なタスクで競争力を持ち､ それを無償のオープンソースモデルで実現できるのであれば､

01:59.530 --> 02:04.240
私たちは偉大なことを成し遂げたことになる｡

02:04.240 --> 02:05.080
オーケー｡ 

02:05.110 --> 02:14.120
まとめると､ この時点で､ この分岐点で､ あなたはこの旅の道のりの80％に到達している｡

02:14.120 --> 02:15.260
とても嬉しいよ｡ 

02:15.290 --> 02:16.190
粘り強くやっている｡ 

02:16.190 --> 02:17.660
あなたがここにいてくれて本当にうれしい｡ 

02:17.660 --> 02:20.690
言っておくが､ まだ20％は残っている｡ 

02:20.690 --> 02:21.740
最高だよ｡ 

02:21.740 --> 02:23.060
最高だよ｡ 

02:23.090 --> 02:25.820
最も魅力的なコンテンツはこれからだ｡ 

02:25.820 --> 02:27.860
このモデルをトレーニングしていく｡ 

02:27.860 --> 02:32.540
そして来週のフィナーレでは､ 本当にすべてをまとめる｡ 

02:32.540 --> 02:35.210
つまり､ クレッシェンドになるんだ｡ 

02:35.210 --> 02:37.160
これからどんどん良くなっていくよ｡ 

02:37.160 --> 02:38.300
頑張るんだ｡ 

02:38.330 --> 02:40.250
いいものはすべてこれからだ｡ 

02:40.250 --> 02:44.870
そこで次回は､ さらにいくつかのハイパーパラメーターを用意する｡ 

02:44.870 --> 02:50.390
これらのハイパーパラメーターは少々過酷なものであることは承知しているが､ 本当に重要なことはここで学ぶことができる｡

02:50.390 --> 02:54.530
トレーニングをコントロールするハイパーパラメーターがいくつかあるので､ それを説明しよう｡ 

02:54.560 --> 03:00.830
その後､ SFTトレーナーというものをセットアップする｡

03:00.830 --> 03:03.650
そしてトレーニングを開始する｡ 

03:03.650 --> 03:11.720
あなたは､ llama 3に基づいて､ 独自の大規模言語モデルをトレーニングすることになる｡  1ベースのモデル｡

03:11.720 --> 03:15.320
そして､ 次のセッションでそれをやるつもりだ｡ 

03:15.320 --> 03:17.210
だから､ そこで会おう｡ 
