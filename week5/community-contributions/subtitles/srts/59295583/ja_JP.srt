WEBVTT

00:00.800 --> 00:02.930
そしてJupyterLabに戻ってきた｡ 

00:02.930 --> 00:03.890
少し時間が経った｡ 

00:03.920 --> 00:13.160
先週はColabで作業していたが､ 今日はJupyterに戻ってローカルで作業している｡

00:13.220 --> 00:18.890
コードを見て､ これからやろうとしていることを確認する前に､

00:18.890 --> 00:27.860
フロンティア・モデルのコーディング能力を調べたリーダーボードを思い出してみよう｡

00:27.860 --> 00:39.650
AI企業であるヴェラムのリーダーボードを覚えているかもしれないが､ ヴェラムには人間による評価､ つまり簡単なPythonテストがある｡

00:39.860 --> 00:45.110
この指標に照らし合わせると､ GPTフォーゼロがリードしていることがわかるだろう｡ 

00:45.170 --> 00:50.420
そして､ クロード・スリー・ソネットを見下ろすと､ ここではあまりうまくいっていない｡ 

00:50.570 --> 00:55.310
ええと､ 今､ いろいろな理由があって､ これはおそらく最新のものではないような気がするんだ｡ 

00:55.310 --> 01:03.140
他のところでは､ ラマ3がないことに気づいた｡  このサイトでは1なので､ おそらくこれは少し古いのではないかと思っている｡

01:03.140 --> 01:12.230
人間による評価がテストのベストではないことも知っているし､ 私はコーディングのシールリーダーボードに興味がある｡

01:12.290 --> 01:17.090
このサイトに来て彼らの紹介文を読むと､ 人間による評価だけでなく､ ライブ・コードベンチやプログラミング・パズルなど､

01:17.090 --> 01:25.310
さまざまな種類のコーディング・テストを行っていることがわかる｡

01:25.310 --> 01:27.620
だから､ これは超包括的だと感じている｡ 

01:27.650 --> 01:33.200
また､ ラマ3世も喜んでいる｡  1がリストに載っているので､ これも最近のものという印象だ｡

01:33.560 --> 01:44.720
そして､ このリーダーボードのトップはクロード3だ｡  5ソネット､ GPTフォーゼロと続き､ 3位にはミストラルのラージとオープンソースモデルのミストラルが入った｡

01:44.900 --> 01:47.570
だから､ これで何が起こっているかがわかる｡ 

01:47.600 --> 01:50.120
GPT4が多くの特徴を持っているか見てみよう｡ 

01:50.120 --> 01:52.130
私には見えない｡ 

01:52.220 --> 01:56.510
GPT4が必要かもしれない｡ 

01:56.540 --> 02:01.340
ああ､ もしこの面でトップモデルを本当に比較したいのなら､ だが｡ 

02:01.340 --> 02:07.500
でも､ もっと質素にしたいのであれば､ GPT2をミニに使ってもかまわない｡ 

02:07.530 --> 02:08.430
少し節約できた｡ 

02:08.610 --> 02:16.140
それはともかく､ Jupyter Labに移動して4週目に入り､

02:16.140 --> 02:21.090
3日目に今週のコードを見てみよう｡

02:21.090 --> 02:25.170
そして､ いつものように､ 今日学ぶことのために､ いろいろなことが起こっている｡ 

02:25.170 --> 02:29.640
そのうちのひとつは､ コードを生成する問題についてのものだ｡ 

02:29.790 --> 02:35.190
しかし､ 私たちはこれを､ さまざまなモデルを比較したり､ 先ほどと同じようにリーダーボードを見たり､

02:35.190 --> 02:40.920
LMソリューションでビジネス上の問題を解決する方法を理解したりする方法としても使うつもりです｡

02:40.920 --> 02:43.650
そして､ ここで気づくかもしれないことがある｡ 

02:43.650 --> 02:53.910
Gradioを使い､ プロトタイプにまとめるということがどういうことなのか､

02:53.910 --> 02:59.490
また別の機会にお見せしたいと思います｡

02:59.490 --> 03:08.910
インポートを実行し､ 通常のload envを使って環境変数を設定する｡

03:08.910 --> 03:12.330
今一度､ EMVファイルを持つことを思い出してほしい｡ 

03:12.330 --> 03:16.560
今回はOpenAIとanthropicを使う｡ 

03:16.560 --> 03:27.090
このセルをセットアップして､ OpenAIとクラウドのインターフェイスを通常通り初期化し､ OpenAIとクラウド3を使います｡

03:27.090 --> 03:27.090
5.

03:27.120 --> 03:27.390
申し訳ない｡ 

03:27.420 --> 03:29.520
GPT4とクラウド3を使う｡  5.

03:29.550 --> 03:35.250
ソネット......そのリーダーボードで上位2位に入っていた｡ 

03:35.520 --> 03:41.160
それでは､ システム・メッセージとユーザー・プロンプトを作成しましょう｡ 

03:41.160 --> 03:44.130
昔と同じやり方でやっている｡ 

03:44.130 --> 03:47.370
システムメッセージがハードコードされていたのは､ もう昔のことのように感じる｡ 

03:47.370 --> 03:54.090
ユーザー・プロンプトは､ 変数を渡して､ その変数に対するユーザー・プロンプトを生成するものだ｡

03:54.090 --> 04:02.550
つまり､ あなたはPythonのコードを高性能のCプラス4とM1マックで再インプリメントするアシスタントだということだ｡

04:02.580 --> 04:05.160
ここで僕が使っているのは明らかにM1マックだ｡ 

04:05.190 --> 04:12.420
特にCプラスプラスのセットアップについては､

04:12.600 --> 04:21.060
微調整が必要かもしれません｡

04:21.120 --> 04:23.400
Cプラスプラスコードでのみ対応｡ 

04:23.400 --> 04:24.720
コメントは控えめに｡ 

04:24.750 --> 04:28.710
時折のコメント以外の説明はしないこと｡ 

04:28.740 --> 04:34.080
Cプラス・プラス・レスポンスは､ 可能な限り最速で同一の出力を生成する必要があるため､

04:34.080 --> 04:39.570
1秒前のスライドでお見せしたプロンプトよりも少し言葉が多くなっていますが､

04:39.570 --> 04:44.370
これは私が微調整を加えて最もうまくいったものです｡

04:44.400 --> 04:47.280
このPythonコードをC言語に書き換え､ さらに可能な限り最速の実装を行う｡ 

04:47.280 --> 04:48.690
ちょっと繰り返しが多い｡ 

04:49.020 --> 04:54.210
ええと､ それから､ ここだけちょっとズルをして実験をしているのがわかるだろう｡ 

04:54.210 --> 05:00.300
実は､ 今にわかると思うけど､ たぶん､ これはリーダーボードが示唆している通りなんだ｡ 

05:00.300 --> 05:04.830
クロードには余計なヒントは必要なかったが､ GPT4には必要だった｡ 

05:04.830 --> 05:08.160
そうでなければ､ 生成されたC＋＋のコードは機能しなかった｡ 

05:08.340 --> 05:13.260
ええと､ オーバーフローがないように､ 数字の型に注意してくださいと言わなければならなかった｡ 

05:13.260 --> 05:20.110
また､ 以下のような必要なcplusplusのパッチをすべて含めることも忘れずに｡ 

05:20.140 --> 05:26.740
もしそうしなければ､ GPT4はcplusplusコードを生成するが､

05:26.740 --> 05:32.560
そのパッケージを正しくインクルードしない｡

05:32.560 --> 05:36.220
だから､ どんな理由であれ､ 結局はそうしなければならなかったんだ｡ 

05:36.220 --> 05:38.980
あー､ たぶんこれを試してみたら､ そうならないことがわかると思うよ｡ 

05:39.010 --> 05:43.540
指示的でなくても､ もっといい促し方があるだろう｡ 

05:43.540 --> 05:47.620
ええと､ GPT4ではちょっとズルい気がします｡ 

05:47.620 --> 05:48.670
でも､ これでいい｡ 

05:48.670 --> 05:55.510
とにかく､ このことを念頭に置いて､ この関数を実行し､ ユーザー・プロンプトを作成する関数を定義します｡

05:55.540 --> 05:59.440
そして､ このセクションはあなたにとって非常に馴染み深いものだろう｡ 

05:59.470 --> 06:03.010
ええと､ ええと......のメッセージは､ リストを作成するところです｡ 

06:03.040 --> 06:11.350
システム・メッセージにはロール・システム､ ユーザー・プロンプトにはロール・ユーザー｡

06:11.500 --> 06:20.890
これで､ Pythonに与えられたメッセージ・リストが生成された｡

06:20.890 --> 06:27.850
これはcplusplusのコードで､ そこにあるべきでないものを取り除いてくれる｡

06:27.880 --> 06:35.350
モデルは､ 上がこのキープ､ 下がこのキープで反応する傾向があるんだ｡ 

06:35.350 --> 06:44.290
それをテキストから削除して､ optimized dot cppというcplusplusファイルに保存する｡

06:44.380 --> 06:49.510
つまり､ これが実行されると､ ディレクトリに最適化されたcppファイルが現れることになる｡ 

06:49.960 --> 06:51.100
と呼ばれたとき｡ 

06:51.250 --> 06:51.820
分かった｡ 

06:51.820 --> 06:59.140
そして､ GPT APIを呼び出すために最適化されたGPT関数です｡ 

06:59.170 --> 07:03.670
OpenAI dot chat dot completions dot createと呼ぶことにする｡ 

07:03.700 --> 07:11.620
モデル・イコールOpenAIのモデル・メッセージで､ Pythonのメッセージを渡して､

07:11.620 --> 07:15.700
ストリーミングに設定します｡

07:15.700 --> 07:18.520
そして､ 私たちはストリームでチャンクのためにプレーする｡ 

07:18.520 --> 07:24.980
つまり､ 結果が戻ってくるたびに､ その小さなかたまりを印刷するのだ｡ 

07:24.980 --> 07:27.830
そして最後にこれをファイルに書き出す｡ 

07:28.100 --> 07:31.850
願わくば､ こんなことをする必要はないのだが......｡ 

07:31.880 --> 07:40.040
クロードが同じことをするのと同じバージョンだ｡

07:40.100 --> 07:41.390
クロードを呼ぶつもりだ｡ 

07:41.570 --> 07:45.080
クロード・モデルのメッセージ・ドット・ストリーム｡ 

07:45.230 --> 07:52.100
ええと､ クロードの場合､ システムメッセージをユーザー・プロンプトとは別に提供しなければならないことを覚えているよね｡

07:52.100 --> 07:52.970
そうだ｡ 

07:53.000 --> 07:55.340
これもまた､ 皆さんよくご存知の構図だ｡ 

07:55.370 --> 07:57.710
トークンの最大数を伝えなければならない｡ 

07:57.710 --> 08:01.040
そして､ これがストリーミングバックのやり方だ｡ 

08:01.070 --> 08:02.360
同じようなことだ｡ 

08:02.390 --> 08:04.340
出力を印刷する｡ 

08:05.060 --> 08:06.020
分かった｡ 

08:06.050 --> 08:11.270
この時点で､ 実際に試してみる準備をしているので､ この2つを実行する｡ 

08:11.300 --> 08:14.240
そして､ 次のビデオのために一時停止します｡ 

08:14.240 --> 08:23.960
次のビデオでは､ GPT4とクロード3を試しているところを見てほしい｡

08:23.960 --> 08:23.960
5ソネットは､ この難題に直面したときに実行する｡ 

08:23.990 --> 08:24.710
ではまた
