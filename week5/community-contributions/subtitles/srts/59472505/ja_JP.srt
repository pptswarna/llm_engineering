WEBVTT

00:00.260 --> 00:05.780
というわけで､ これがデータセット・キュレーションに関する最後のビデオとなる｡ 

00:05.810 --> 00:08.120
データセットのキュレーションにうんざりしていたのだろう｡ 

00:08.120 --> 00:10.970
さあ､ あと1枚で完成だ｡ 

00:11.000 --> 00:16.370
だから我々は､ 誇るべき優れたデータセットを作り上げたのだ｡ 

00:16.370 --> 00:19.610
では､ 最終チェックをしてみよう｡ 

00:19.760 --> 00:23.030
あの､ ひとつ質問していいですか？

00:23.060 --> 00:24.710
まあ､ とにかく聞いてみるよ｡ 

00:24.710 --> 00:30.740
商品の価格が関係している可能性はありますか？

00:30.740 --> 00:35.570
これは､ そのアイテムの説明文の長さに相関している｡ 

00:35.570 --> 00:41.960
高価格のものほど情報量が多いという状況を想像するかもしれない｡ 

00:41.960 --> 00:44.420
そして､ それは私たちが理解する価値のあることだろう｡ 

00:44.600 --> 00:48.170
ああ､ それはモデルがすぐに学ぶことだからね｡ 

00:48.170 --> 00:52.730
そして､ 伝統的なアプローチに目を向けることで､ 私たちがどのようなアプローチを取るべきか､ おそらく良い感覚を与えてくれる｡

00:52.730 --> 01:01.220
これは､ X軸の各サイズを示す､ 小さな散布図だ｡

01:01.250 --> 01:07.970
説明文の長さが表示され､ Y軸には価格が表示される｡ 

01:08.060 --> 01:12.350
では､ 全サンプルのデータを見てみよう｡ 

01:12.740 --> 01:13.700
それでは､ どうぞ｡ 

01:13.700 --> 01:15.950
ここに素敵な写真がある｡ 

01:15.950 --> 01:20.210
だから､ この写真には40万点がある｡ 

01:20.360 --> 01:23.330
そして､ それは見るべきものだ｡ 

01:23.330 --> 01:26.420
消化すべきことがたくさんあるのがわかるだろう｡ 

01:26.450 --> 01:34.490
このような境界点では価格が高くなる傾向があるため､ このような興味深いパターンが起きていることがわかる｡

01:34.490 --> 01:43.370
799ドルの商品､ うーん､ もちろん､ もっと安い商品もたくさんあることはお分かりいただけるだろう｡ 

01:43.370 --> 01:59.480
そして､ 説明文が長い商品ほど､ 高価な商品である傾向があることがわかる｡

01:59.480 --> 02:03.750
しかし､ その点で有意な相関関係があるかどうかは明らかではない｡ 

02:03.750 --> 02:06.750
だから､ 何かはあるんだけど､ 大したことではないんだ｡ 

02:06.750 --> 02:15.270
そのため､ 従来の機械学習では､ このようなことを調べようとしても､ おそらく大きな相関関係は見いだせないと思われる｡

02:15.270 --> 02:23.610
つまり､ データのさまざまな側面を理解するために思いつく図の一例です｡

02:24.330 --> 02:27.510
もうひとつ､ トークンについてもう少し話したい｡ 

02:27.750 --> 02:34.830
オープンソースのモデルに対して実際にトレーニングするようになったら､ トークンをもっとたくさん使うことになるだろうが､

02:34.830 --> 02:38.640
今はトークンに注目する価値がある｡

02:38.640 --> 02:48.630
そこで､ この関数レポートを書きました｡ この関数は､ アイテムを受け取り､ プロンプトを､ まず､ トレーニング中に使用される完全なトレーニングプロンプトを表示します｡

02:48.630 --> 02:54.930
そして､ そのプロンプトの最後の10個のトークンをデコードする｡ 

02:54.930 --> 02:59.010
つまり､ 最後の10個のトークンに対応するテキストの断片を見ることになる｡ 

02:59.010 --> 03:02.250
そして､ なぜ最後の10人なのか不思議に思っているのなら､ すぐにわかるだろう｡ 

03:02.250 --> 03:08.550
では､ 4万番というランダムな数字を選んで実行してみよう｡ 

03:08.580 --> 03:09.060
オーケー｡ 

03:09.090 --> 03:12.840
というわけで､ こんな文章で申し訳ない｡ 

03:12.840 --> 03:18.390
このプロンプトは､ LLMに送られて学ぶことになる｡ 

03:18.630 --> 03:22.170
ええと､ これは1ドル単位でいくらですかと聞かれそうですね｡ 

03:22.170 --> 03:23.790
そして､ それが説明文になる｡ 

03:23.790 --> 03:30.090
そして価格は､ そしてこれは1ドル未満を四捨五入した価格である｡ 

03:30.180 --> 03:37.860
項目コードを見ればわかるが､ トレーニングのプロンプトを作成する際､ 1ドル未満を四捨五入している｡

03:37.950 --> 03:44.520
最後の10個のトークンを見れば､ 何が起こっているかがわかるだろう｡ 

03:44.550 --> 03:46.110
その10トークンとは何ですか？

03:46.110 --> 03:53.010
そして､ 最後の数トークンに関して､ 価格が1つのトークンにマップされることを示したかった｡

03:53.040 --> 04:07.410
それは､ 単語の開始で再びドルになる前に､ 単語の開始スペースのトークンを取得することである｡

04:07.680 --> 04:12.450
そしてこれは､ 私が言うように､ llamaトークナイザーの機能なのだ｡ 

04:12.450 --> 04:18.000
GPTと同様､ 3桁の数字ごとに個別のトークンがある｡ 

04:18.000 --> 04:21.120
他のトークナイザーにはないものもある｡ 

04:21.210 --> 04:27.120
僕らのプロジェクトには必要ないことだけど､ 後々のことを考えると少しシンプルになる｡ 

04:27.180 --> 04:32.250
そして､ その期間はトークンを1つ獲得し､ . 00はトークンを1つ獲得する｡ 

04:32.280 --> 04:35.130
別のサンプルをやってみよう｡ 

04:36.930 --> 04:42.000
全く別の､ あー､ 全く別の場所で何かやろう｡ 

04:42.720 --> 04:44.100
万人｡ 

04:44.280 --> 04:47.550
そして､ これはむしろ安いものだ｡ 

04:47.580 --> 04:51.930
価格は9000ドル｡ 

04:51.960 --> 04:58.110
データセットの最後に近い398,000を選んでみよう｡ 

04:58.620 --> 05:05.740
そして､ これは......コイルオーバー・ダンパー・キット｡ 

05:05.740 --> 05:10.240
そしてこの価格は765ドル｡ 

05:10.240 --> 05:15.430
そして､ 765が1つのトークンにマッピングされていることがもう一度わかるだろう｡ 

05:15.430 --> 05:25.840
というわけで､ このサンプルはもちろん一番安いものから順に並んでいる｡

05:25.840 --> 05:28.990
だから､ 1ドル単位で四捨五入した｡ 

05:28.990 --> 05:36.100
これは､ サンプルの下位､ つまりウムな部分で最も安く､ サンプルの上位で最も高い順にソートされている｡

05:36.100 --> 05:46.780
そして､ ここに書いてあるように､ 1から999までのすべての数字が1つのトークンにマッピングされている､ という効果が得られていることを自分で納得できるだろう｡

05:46.780 --> 05:55.870
そしてもう1度言いますが､ 量､ ガンマ､ ファイの3つのトークナイザーを見れば､ そうではないことがわかるでしょう｡

05:55.960 --> 06:02.350
うーん､ これは後でちょっと便利なんだけど､ 必須じゃないし､ 絶対に後回しなんだ｡ 

06:02.350 --> 06:08.620
クァンタム・ジェミニ・スリーのような他のモデルを使って実験したい場合は､ 単に入れ替えるだけで機能する｡

06:08.650 --> 06:15.430
ここでは､ 3桁の数字に対応する1つのトークンではなく､ 複数のトークンにマッピングされることがわかるだろう｡

06:16.690 --> 06:20.260
よし､ ほっと一息だ｡ 

06:20.260 --> 06:22.630
私たちはデータ・キュレーションを通してそれを成し遂げてきた｡ 

06:22.630 --> 06:27.460
最後の仕上げは､ ハブにアップロードすることだ｡ 

06:27.460 --> 06:35.710
まずはデータセットをシャッフルしてみよう｡ 安い順に並べ替えたのではまったく意味がない｡

06:35.710 --> 06:38.650
まず､ ごちゃごちゃしたデータセットが必要だ｡ 

06:38.800 --> 06:44.350
まず最初に､ ランダムシードを設定します｡ これは､ 常にまったく同じデータセットで作業していることを確認したいからで､

06:44.350 --> 06:51.340
私が行うのとまったく同じことを再現して､ 同じ結果を得ることができます｡

06:51.520 --> 06:59.290
私たちはランダムを使っている｡  shuffleでシャッフルして､ 最初の40万件をトレーニング・データセットとする｡

06:59.290 --> 07:01.900
そして次の2000をテストセットとする｡ 

07:01.930 --> 07:03.220
今､ 聞いたよ｡ 

07:03.250 --> 07:05.230
データサイエンティストたちよ｡ 

07:05.260 --> 07:11.380
これは通常､ 少なくとも5％か10％のテストデータセットを必要とする｡ 

07:11.470 --> 07:17.350
8000ものデータがあるんだから｡

07:17.350 --> 07:21.490
もちろん､ より大きなデータセットを得るためにサンプルを増やすこともできる｡ 

07:21.520 --> 07:27.310
テストに使うのは数百本程度だろうから､ 必要ないだろう｡

07:27.310 --> 07:30.160
そうすることで､ 非常に正確な結果が得られる｡ 

07:30.160 --> 07:34.090
そして､ より多くの相手とテストを続ければ､ 収穫は減っていく｡ 

07:34.090 --> 07:39.340
このプロジェクトの目的にはこれで十分だが､ ベストプラクティスである｡ 

07:39.370 --> 07:40.630
ベストプラクティスかどうかは分からない｡ 

07:40.660 --> 07:47.620
少なくとも5％はテスト用データセットとして確保するのが一般的で､ 前にお話ししたように､

07:47.620 --> 07:52.810
テスト用と検証用に分けて5％を確保することもある｡

07:52.870 --> 08:00.880
この目的には必要ないが､ 望むのであれば､ 追加データセットとして管理することもできる｡

08:01.030 --> 08:02.300
うーん､ でもとにかく､ そうしよう｡ 

08:02.300 --> 08:03.320
それをごちゃ混ぜにする｡ 

08:03.320 --> 08:08.630
400,000のトレーニングデータセットと､ 2000のテストセットに分かれている｡ 

08:08.660 --> 08:14.480
最初のテスト要素を見てみよう｡ 

08:14.480 --> 08:19.640
あなたが覚えているテストのプロンプトは､ 答えを明らかにしないプロンプトです｡ 

08:19.640 --> 08:25.310
まずトレーニングのプロンプトを見て､ 次にテストのプロンプトを見ます｡

08:25.340 --> 08:27.470
トレーニングプロンプトは答えを持っているものだ｡ 

08:27.470 --> 08:31.310
だから､ トレーニングのプロンプトには､ 1ドル単位でいくらと書いてある｡ 

08:31.310 --> 08:35.120
デルファイ製の燃料ポンプモジュールだ｡ 

08:35.390 --> 08:37.430
うーん､ そうだね｡ 

08:37.460 --> 08:37.910
どうだろう｡ 

08:37.910 --> 08:39.470
227ドルである｡ 

08:39.470 --> 08:41.300
私だったら､ そんなことはまったくわからなかっただろう｡ 

08:41.300 --> 08:50.240
つまり､ これはトレーニングの一環としてLMに送られるものの例である｡

08:50.480 --> 08:54.380
では､ テストプロンプトを見てみよう｡ 

08:54.410 --> 09:02.990
今､ テストプロンプトは､ 説明文はあるが値段はない､ 使用されるものを見せようとしている｡

09:02.990 --> 09:07.400
これがテストセットの最初の項目だ｡ 

09:07.400 --> 09:09.350
そうだ｡ 

09:09.470 --> 09:22.430
最初の250のテストポイントの価格分布を見てみましょう｡

09:22.430 --> 09:26.750
そして､ ここにはさまざまな価格の健康的な広がりがあるのがわかるだろう｡ 

09:26.780 --> 09:33.410
高価なものを扱えるモデルかどうかが試されるようなものが､ 高いエリアにはたくさんある｡ 

09:33.410 --> 09:42.530
そして､ 私たちのテスト・データ・セットでは､ 価格のバラエティーに富んでいる｡

09:43.340 --> 09:51.650
さて､ 最後に､ これを一連のトレーニング・プロンプトとテスト・プロンプトに変えますが､ これは､

09:51.650 --> 09:58.910
先ほど見たプロンプトとテスト・プロンプトを､ 価格とともに抜き出すだけです｡

09:59.390 --> 10:03.590
この小さなコードが､ ハグする顔にアップロードする｡ 

10:03.620 --> 10:14.330
データセットのFromdictを呼び出し､ それをデータセットのdictに入れることで､ 抱きつき顔ハブに適したデータセットオブジェクトに変える｡

10:15.050 --> 10:22.940
そして最後に､ この行があなたのデータセットをハギング・フェイス・ハブにアップロードし､ あなたがそれを引き続き使用したり､

10:22.940 --> 10:26.000
将来ダウンロードできるようにします｡

10:26.000 --> 10:33.110
でも､ もう走ったから走らせるつもりはないよ｡ 

10:33.110 --> 10:35.300
そして､ これはあなたのユーザー名を入れるためのものです｡ 

10:35.300 --> 10:46.100
これは僕のユーザーネームにアップロードしてあるんだ｡ 

10:46.100 --> 10:51.800
だから､ その方法でデータを取り出すこともできる｡ 

10:51.830 --> 10:55.820
もし､ このデータキュレーションをすべてショートカットしたいのであれば､ できればそうしたくないだろう｡ 

10:56.210 --> 11:07.230
そして最後に､ この列車をピックルファイルにして､ コレクションをテストします｡

11:07.230 --> 11:12.270
このリストをピクルスにしてファイルに保存しておくと､

11:12.300 --> 11:16.050
将来のために読み込むことができる｡

11:16.050 --> 11:23.520
Pythonのピクルスに慣れている人なら､ Pythonのオブジェクトをファイルにダンプする超簡単な方法だ｡

11:23.520 --> 11:28.710
これを実行すると､ test dot pickleとtrain dot pickleという2つの新しいファイルができ､

11:28.710 --> 11:32.340
トレーニング・データとテスト・データが格納される｡

11:33.090 --> 11:36.900
これでデータのキュレーション作業は完了した｡ 

11:36.900 --> 11:44.820
データセットをもっと調査し､ 異なるデータ・ポイントをトークン化する練習をしたときに､

11:44.970 --> 11:59.490
3桁の数字が常に1つのトークンにトークン化され､ それらのトークンの感覚を得ることができるかどうかを確認してください｡

11:59.820 --> 12:03.720
それでは､ またスライドをご覧いただきながら､ 総括をしたいと思います｡ 
