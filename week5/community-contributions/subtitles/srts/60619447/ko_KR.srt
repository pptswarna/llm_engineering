WEBVTT

00:00.740 --> 00:07.340
LLM의 아주 근본적인 것에 대해 잠시 얘기하고 싶습니다 LLM 매개

00:07.340 --> 00:13.190
변수 안에 있는 매개 변수의 수입니다 웨이트라고도 하죠

00:13.310 --> 00:16.100
보통 매개변수와 무게는 동의어예요

00:16.250 --> 00:19.850
어떤 상황에서는 완전히 다른 세부 사항이 있어요

00:19.850 --> 00:23.060
하지만 기본적으로 중량과 매개 변수는 같아요

00:23.090 --> 00:23.900
모델 웨이트예요

00:23.900 --> 00:33.140
이건 모델 안에 있는 레버로 입력을 받았을 때 출력을 생성하는 것을 통제하죠

00:33.140 --> 00:37.040
다음에 나올 단어를 어떻게 예측하시나요?

00:37.040 --> 00:41.840
이 웨이트는 LLM 훈련 때 쓰는 거예요

00:41.840 --> 00:47.330
수많은 예시를 보고 그 예시를 이용해 무게를 움직입니다 상태가

00:47.330 --> 00:52.820
더 좋아져서 다음 토큰에서 나올 것을 예측할 수 있을 때까지요

00:52.820 --> 00:56.000
토큰에 대해 곧 얘기하겠지만 점점 더 재미있어지죠

00:56.000 --> 00:59.420
모든 무게를 조정하면 더 좋아지죠

00:59.420 --> 01:04.160
데이터 과학을 하는 분들은 처음 보시는 분들에겐 아주 잘 아는

01:04.160 --> 01:04.970
것들이죠

01:04.970 --> 01:09.440
과정을 진행하는 동안 다양한 방식으로 살펴볼 겁니다 보다 나은 직관력을

01:09.440 --> 01:15.710
갖게 될 거예요 getput을 제어하는 매개 변수와 저울이 어떤 의미인지를요

01:15.710 --> 01:22.370
하지만 우선 얼마나 많은 중량이 있는지 알아야 합니다 기존의

01:22.370 --> 01:28.940
데이터 과학이나 머신 러닝에서는 단순하게 접근했기 때문에

01:28.940 --> 01:35.180
선형 회귀 모델이라는 모델을 개발해야 했습니다 가중된 평균을

01:35.180 --> 01:40.110
가지고 20에서 200 정도의 매개 변수를 갖습니다

01:40.110 --> 01:47.630
보통 20에서 200 정도의 중량으로 접근하죠

01:47.750 --> 01:54.470
LMS가 특이한 점 중 하나는 무게의 수가 다르다는

01:54.470 --> 01:56.570
거예요

01:56.600 --> 02:05.280
2018년에 출시된 GPT 1은 1억 1,700만 명이 기다렸어요

02:05.310 --> 02:10.230
개인적으로 좀 화가 났던 부분인데요 LMS@타입의

02:10.230 --> 02:16.740
딥 신경망이 있었을 때 제 스타트업에 하나 있었는데 딥 신경망에 200,000

02:16.740 --> 02:24.210
매개 변수가 있다고 자랑하곤 했어요 엄청나게 큰 숫자라고 생각했죠 200,000

02:24.210 --> 02:31.230
매개 변수가 넘는 모델은 상상할 수도 없었어요

02:31.230 --> 02:35.880
그래서 GPT 1이 매개 변수를 1억 1,700만 개 제시했을 때 당황했죠

02:36.120 --> 02:41.610
GPT 1의 중요성을 절실히 깨닫게 됐죠

02:41.970 --> 02:47.580
그런데 보다시피 여기 보이는 이 범위는 로그식 범위예요 틱을 켤 때마다

02:47.580 --> 02:51.000
한 단계씩 더 올라가는 게 아니죠

02:51.030 --> 02:56.940
매개 변수의 10배를 앞의 체크보다 더 많이 입력해야 해요

02:57.000 --> 02:59.550
이 다이어그램에 레이어를 겹쳐보죠

02:59.610 --> 03:06.720
GPT 2의 후속 버전인 1을 추가했죠 50억 개의 매개 변수가 있어요

03:06.750 --> 03:10.680
GPT 3,750억 매개 변수예요

03:10.710 --> 03:12.600
말로 표현이 안 돼요

03:12.600 --> 03:14.160
변수가 많아요

03:14.190 --> 03:20.670
GPT 41요 7조 6천억 매개 변수예요

03:20.970 --> 03:24.000
최신 개척 시대 모델도 있죠

03:24.000 --> 03:27.210
매개 변수가 몇 개인지 아직 발표하지 않았어요

03:27.240 --> 03:32.520
약 10조 매개 변수가 있는 것으로 추정되죠

03:32.520 --> 03:37.710
이 추의 숫자는 상상할 수 없을 정도예요

03:37.920 --> 03:43.350
이제 이 위에 오픈 소스 모델을 레이어하죠

03:43.500 --> 03:46.110
제마는 20억 달러예요

03:46.140 --> 03:47.580
가벼운 모델이에요

03:47.580 --> 03:55.260
라마 3도 기억하실 거예요 라마를 사용할 때 작업했던 2개의 데이터에도 20억 개의 매개

03:55.320 --> 03:56.670
변수가 있었어요

03:56.820 --> 04:04.450
그럼 라마 3요 1번, 그러니까 더 큰 사촌은 세 종류가 있어요

04:04.480 --> 04:14.470
80억 개, 700억 개 그리고 라마 3이죠 1 405B는 현재 오픈 소스 모델 중 가장 큰

04:14.470 --> 04:21.040
것으로 일부 비공개 소스 모델과 비슷한 기능을 갖고 있죠

04:21.040 --> 04:24.970
혼합 레일도 언급했었죠 전문가들의 혼합 모델이요

04:25.420 --> 04:33.040
이 모델들이 얼마나 거대한지 통찰력을 주는 거예요

04:33.040 --> 04:39.190
10조 개의 무게와 다양한 지렛대 다양한 숫자가 있다는 게 어떤 의미인지 이해하기조차

04:39.220 --> 04:40.870
어려워요

04:40.870 --> 04:49.060
입력된 출력을 조절하는 이 거대한 모델 안의 작은 손잡이라고 생각하면 돼요

04:49.060 --> 04:54.740
그리고 다시 한 번 구식 선형 회귀 모델과 비교해 보세요 20에서 200

04:54.740 --> 04:57.280
매개 변수가 있을 거예요

04:57.280 --> 05:00.970
Get in get 이 거대한 언어 모델의 방대함을 느껴보려고요
