WEBVTT

00:00.800 --> 00:06.560
지난 시간에는 기능 공학의 단순한 선형 회귀 모델을 살펴봤는데요

00:06.560 --> 00:12.080
이제 기능과는 작별하고 자연 언어 프로세싱을 시작해보죠

00:12.080 --> 00:19.490
NLP의 다소 단순한 유형으로 시작하겠습니다 앞서 짧게 언급한 단어 주머니라고

00:19.490 --> 00:21.110
하죠

00:21.140 --> 00:24.950
이 두 가지 유용한 것부터 만들어 볼게요

00:24.950 --> 00:33.920
하나는 가격이고 다른 하나는 문서인데 가격은 훈련 데이터 집합에 있는 모든 가격

00:33.920 --> 00:42.020
목록입니다 문서는 훈련 데이터 집합에 있는 모든 테스트 프롬프트고요

00:42.230 --> 00:47.480
비트 텍스트죠 이제 우리가 할 건 문서에 텍스트를 사용하는 모델을 만드는

00:47.480 --> 00:51.440
겁니다 우리가 만든 기능을 사용하는 대신에요

00:51.440 --> 00:52.940
그냥 글자를 써요

00:53.330 --> 01:01.440
눈치채셨겠지만 작은 트릭이 하나 있어요 훈련용이 아니라 테스트 프롬프트죠

01:01.590 --> 01:06.000
훈련 시간을 이용하면 가격도 포함되어 있죠

01:06.120 --> 01:12.210
그건 작동하지 않을 겁니다 모델은 당연히 가격 자체를 알아내는 법을 배우게 될 테니까요 프롬프트

01:12.210 --> 01:13.470
안에 있죠

01:13.470 --> 01:16.470
테스트 시간이 되면 처참하게 실패하죠

01:16.470 --> 01:20.220
조심해야 할 트릭이죠

01:20.460 --> 01:22.980
제가 더 조심해야 할 함정이죠

01:22.980 --> 01:25.710
그래서 준비하죠

01:25.710 --> 01:31.770
이제 countvectorizer를 사용할 거예요 이름만 들어도 근사하죠

01:31.800 --> 01:36.330
오늘 카운트 오브 벡터라이저로 작업한다고 생각한다면 매우

01:36.330 --> 01:41.850
복잡한 무언가를 만들 거라고 생각하지만 실제로는 그렇지 않아요

01:41.940 --> 01:49.740
단순히 단어 수를 세는 것을 찾는 것입니다 그리고 벡터 내의 각 위치들이 특정 단어를

01:49.740 --> 01:57.740
나타내는 곳에 벡터를 구축합니다 그리고 그 단어가 여러분 문서에 몇 번 나타날지

01:57.740 --> 01:59.420
확인하죠

01:59.540 --> 02:06.110
각 문서는 벡터입니다 벡터의 모든 행은 여러분 어휘의 단어를 나타내죠

02:06.110 --> 02:09.170
카운터는 몇 번 왔는지 세어보고요

02:09.170 --> 02:15.050
이런 사고방식을 단어 자루 모델이라고 해요 단어 자루를 들고 숫자를

02:15.050 --> 02:19.880
세는 것과 같죠 단어 순서는 상관없어요 특정 횟수만큼만

02:19.880 --> 02:23.840
단어에 대한 개념이 있는 거죠

02:24.170 --> 02:31.700
그래서 가장 흔하거나 가장 중요한 단어 1,000개까지 셀 거예요

02:31.700 --> 02:41.150
여기 이 매개 변수를 이용해 확실히 하려는 건 일반적인 스톱 단어를 제거하는 겁니다 사람들이

02:41.150 --> 02:47.270
앤 앤 더 앤 인이라고 부르는 거죠 모델에 유용하지 않고 방해만

02:47.270 --> 02:49.820
되는 것들이죠

02:49.820 --> 02:54.320
get이 뽑히면 즙이 흐르는 단어만 남죠

02:54.320 --> 03:00.000
이제 우리가 할 일은 보다시피 훨씬 빨라졌어요

03:00.000 --> 03:05.340
이 문서에 기반해 데이터 세트를 생성할 거예요

03:05.340 --> 03:09.240
아까처럼 선형 회귀 모델을 다시 만들 거예요

03:09.270 --> 03:14.610
우리 단어를 가격에 맞춰야 해요

03:14.610 --> 03:21.090
다시 말해, 이번엔 기능을 사용하는 대신 이 단어들을 사용하는

03:21.090 --> 03:29.130
겁니다 벡터 단어로요 단어 개수를 세는 거죠 가장 흔한 사전 용어에 있는

03:29.130 --> 03:32.850
단어 천 개의 개수를요

03:32.880 --> 03:34.650
그렇게 할 거예요

03:34.680 --> 03:36.480
선형 회귀를 실행할 거예요

03:36.480 --> 03:37.980
지금 진행 중이에요

03:37.980 --> 03:46.110
모든 데이터 집합에 걸친 벡터 포인트 1,000개를 세고 있어요

03:46.440 --> 03:53.250
그 작업이 끝나면 그걸 단어 모음에 넣습니다 선형 회귀, 프라이서죠

03:53.250 --> 03:54.180
Put

03:54.180 --> 03:58.130
그리고 테스트할 거예요 다시 테스트하죠

03:58.160 --> 03:59.660
방금 실행이 완료됐어요

03:59.690 --> 04:07.280
이건 우리 함수예요 시험해 볼 간단한 함수죠 테스터.테스트라는 기기로 테스트할

04:07.310 --> 04:08.570
거예요

04:09.320 --> 04:10.790
말주변이 좋네요

04:10.820 --> 04:13.520
선형 회귀 프라이저예요

04:16.010 --> 04:17.630
어떻게 되나 보죠

04:19.550 --> 04:21.050
채소가 많네요

04:21.050 --> 04:22.730
붉은색도 많고요

04:23.300 --> 04:23.960
보이시죠?

04:23.990 --> 04:25.340
하나만 뽑죠

04:25.370 --> 04:27.140
74달러가 들었죠

04:27.140 --> 04:29.030
사실 46달러였어요

04:29.030 --> 04:31.640
그래서 제대로 된 것들이 있는 걸 볼 수 있죠

04:31.640 --> 04:33.530
너무 많이 벗어난 부분도 있어요

04:33.560 --> 04:35.540
그래프가 어떻게 보일까요?

04:35.570 --> 04:36.830
어디 보죠

04:37.430 --> 04:39.350
네, 좋아요

04:39.350 --> 04:42.680
뭔가 좋은 일이 일어날 것 같아요

04:42.680 --> 04:46.130
점점 더 가까워지고 있어요

04:46.130 --> 04:48.080
녹색 점이 더 많이 보여요

04:48.080 --> 04:51.140
평균 113달러죠

04:51.140 --> 04:59.310
특징이 있는 선형 퇴행보다 훨씬 낫고 추측보다 훨씬 낫네요

04:59.670 --> 05:01.980
진전이 있네요

05:01.980 --> 05:05.760
아직 이상한 점이 있어요 문제가 좀 있죠

05:06.000 --> 05:10.860
하지만 이게 진정한 가치이고 여기 어딘가에 있어야 한다고 생각했죠

05:11.160 --> 05:15.810
하지만 진전이 있다는 걸 알 수 있어요

05:16.200 --> 05:24.540
좀 더 고급 모델로 넘어가기 전에 이 세트의 마지막은 젠심 라이브러리를

05:24.540 --> 05:31.560
이용해 이 단어를 베크 모델에 소개하는 거예요

05:31.560 --> 05:37.050
그게 제가 NLP와 함께 있는 신경 네트워크를 처음 접한

05:37.410 --> 05:46.440
것 중 하나입니다. 벡터화 모델을 사용했고 더 복잡한 벡터 내장 모델을 사용했죠.

05:46.500 --> 05:54.950
겐심에서 2VC 함수 클래스를 사용하겠습니다 400D로

05:54.950 --> 05:57.980
벡터를 빌드하죠

05:58.250 --> 06:02.960
그래서 일꾼 8명을 쓰도록 설정했어요 제 상자를 망치질하는 셈이죠

06:02.960 --> 06:08.840
실행하는 데 몇 분 걸렸어요 이걸 다 기다리지 않게 미리 실행했죠

06:08.870 --> 06:19.610
그리고 이걸 실행했어요 이제 실행할 준비가 됐으면 좋겠네요

06:19.610 --> 06:24.410
바로 결과를 볼 수 있게요

06:25.160 --> 06:33.200
2bc 선형 회귀 프라이어를 통과시킬 수 있을 거예요 이 멋진 벡터화 모델이 선형 회귀로

06:33.200 --> 06:36.140
어떻게 작동하는지 보죠

06:37.070 --> 06:39.950
처음 두 줄까지는 괜찮아 보이는데 큰일이네요

06:39.980 --> 06:46.130
빨강도 있고 초록도 있고 빨강도 있고 초록도 있고 빨강이 많네요 250개의 데이터

06:46.130 --> 06:49.290
포인트를 지나 해도로 내려갔어요

06:50.100 --> 06:52.080
여기 있네요

06:52.110 --> 06:54.150
다시 괜찮아졌어요

06:54.180 --> 07:01.980
흥미롭게도 나쁜 소식은 단순한 단어 모델에 따르면 선형 회귀보다 머리카락이 더 나쁘다는

07:02.010 --> 07:03.540
거예요

07:03.540 --> 07:11.850
멋진 단어를 공개하죠 벡터 2개는 아직 도움이 안 됐어요

07:11.850 --> 07:17.850
여전히 이전과 같은 영역입니다. 아마도 2Vc 벡터에 있는

07:17.850 --> 07:23.730
추가 정보를 활용할 만큼 선형 회귀 모델이 강력하지 않기 때문일

07:23.730 --> 07:25.140
거예요.

07:25.410 --> 07:31.800
다음 시간에는 좀 더 복잡한 모델을 탐구해 보고 전통적인 머신

07:31.800 --> 07:34.890
러닝을 마무리할 거예요

07:34.890 --> 07:38.820
조금만 더 기다려 주세요 비트를 좀 더 짜내고 싶거든요

07:38.850 --> 07:43.920
베이스라인 모델로 좀 더 나아지길 원합니다 LMS가 비트 코일에서 쉽게 실행되는 걸 원치

07:43.920 --> 07:44.730
않으니까요

07:44.730 --> 07:46.350
Put it's go 우린 싸우고 싶어요

07:46.350 --> 07:48.330
다음에 봐요
