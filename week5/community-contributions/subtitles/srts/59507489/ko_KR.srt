WEBVTT

00:01.610 --> 00:06.140
하이퍼파라미터에서 훈련을 계속하죠

00:06.140 --> 00:11.660
다음 단계는 아주 중요한 학습률이에요

00:11.660 --> 00:16.220
여러분 중 데이터 과학자들이 이걸 너무 잘 알 거예요

00:16.460 --> 00:23.000
아주 잠깐, 이것에 덜 익숙한 분들을 위해 설명하자면 훈련의 목적은 여러분의 모델을

00:23.000 --> 00:26.690
트레이닝 데이터 포인트를 취하는 거죠

00:26.690 --> 00:32.150
전진 패스라는 걸 하는데요 모델을 살펴보고 다음

00:32.150 --> 00:38.780
토큰이 올지 예측하는 겁니다 예측된 다음 토큰을 주는 거죠

00:39.110 --> 00:44.330
아니면 가능한 모든 다음 패의 확률을 주죠

00:44.360 --> 00:49.610
그걸 사용하면 실제 있어야 할 다음 토큰이 생기죠

00:49.610 --> 00:54.380
이 두 가지를 예견하고 실제 손실을 계산해 보세요

00:54.500 --> 01:01.460
실제 비트를 예측하는 데 얼마나 형편없었는지를요 그런 다음 할 수 있는 건 손실을 받아들이고

01:01.460 --> 01:07.430
그것의 역전파를 할 수 있습니다 모델로 돌아가서 각 비트를 얼마나 민감하게 조정해야

01:07.460 --> 01:13.910
하는지 알아내는 거죠 다음에 좀 더 잘하기 위해 각 비트를 얼마나 조정할까요?

01:14.120 --> 01:17.570
그 방향으로 한 걸음씩 나아가야 해요

01:17.570 --> 01:23.060
역기를 옮겨야 해요 다음번엔 더 잘하도록 한 걸음씩 나아가는 거죠

01:23.060 --> 01:28.730
비트를 좋은 방향으로 많이 옮겨야 다음번에 더

01:28.760 --> 01:29.990
잘 걸리죠

01:29.990 --> 01:35.420
훈련 데이터 포인트를 마주했을 때 학습률이라고 하죠

01:35.570 --> 01:42.470
일반적으로 0 정도 돼요 0001이나 0요 00001년요

01:42.530 --> 01:45.350
살펴보면 몇 가지 예가 보일 거예요

01:45.440 --> 01:51.290
또 학습률 스케줄러라는 기능도 있습니다 여러

01:51.290 --> 01:57.470
개혁을 거치면서 학습률을 한 숫자로 시작해서 점점

01:57.470 --> 02:02.720
낮추는 거죠 모델이 훈련될수록 학습률이

02:02.720 --> 02:08.120
점점 낮아져서 네트워크에 아주 작은 변화만

02:08.120 --> 02:11.300
줄어드는 거예요

02:11.330 --> 02:15.050
거의 맞혔다고 확신하고 있잖아요

02:15.050 --> 02:17.540
이게 학습률이에요

02:17.570 --> 02:21.440
데이터 과학을 전공한 많은 사람들에겐 익숙한 일이겠죠

02:21.440 --> 02:23.450
다른 사람에겐 생소할 수 있죠

02:24.050 --> 02:27.920
그러데이션 축적도 방법이에요

02:27.950 --> 02:35.210
훈련 속도를 높이는 기술이에요 이렇게 말할 수 있죠 우리가

02:35.210 --> 02:40.760
뭘 할 거냐면 보통은 앞으로 패스해요

02:40.970 --> 02:46.340
방금 말한 것처럼 get 손실이 발생하죠

02:46.370 --> 02:52.400
그리고 뒤로 가는 그러데이션을 계산해서 올바른 방향으로 한 걸음씩 나아가는 거죠

02:52.400 --> 02:58.790
그러한 그러데이션 누적 현상을 반복해 보면 이렇게 말할 수 있죠 앞으로 패스해서

02:58.790 --> 03:03.800
얻으면 어떨까? 한 걸음씩 가지 않고 두 번째 패스해서 얻으면 어떨까?

03:03.800 --> 03:07.040
그 과정을 몇 번 더 반복하는 거죠

03:07.040 --> 03:13.790
이런 그러데이션을 계속 축적하고 한 단계씩 나아가 네트워크를 최적화하세요

03:14.060 --> 03:19.170
이 단계를 비정상적으로 수행하면 비트 코드가 더 빨라지죠

03:19.350 --> 03:21.900
비트 크기와 비슷할 거예요

03:21.900 --> 03:27.120
개념적 유사점이 있어요 왜냐하면 함께 그룹을 이루고

03:27.120 --> 03:30.060
약간 더 큰 단계를 밟으니까요

03:30.330 --> 03:35.070
제가 설정한 hyperpaameter에는 그러데이션 축적을 사용하지 않아요

03:35.070 --> 03:36.540
1로 설정했어요

03:36.690 --> 03:39.480
하지만 전에 해봤는데 속도가 빨라지는 걸 봤어요

03:39.480 --> 03:44.220
그러니 이 방법으로 실험해 보고 어떻게 되는지 보세요

03:44.220 --> 03:46.710
이게 그러데이션 축적이에요

03:47.100 --> 03:50.700
마지막은 최적화 장치예요

03:50.730 --> 03:57.030
최적화 프로그램은 적절한 때에 사용하는 공식이에요 그러데이션이 되면 학습률이

03:57.030 --> 03:57.780
올라가죠

03:57.780 --> 04:05.730
이제 신경망을 업데이트할 시간이에요 모든 걸 좋은 방향으로 약간씩 바꾸면

04:05.730 --> 04:11.730
다음에는 올바른 토큰을 예측할 가능성이 커지죠 비트

04:11.730 --> 04:14.550
그 과정을 최적화 장치라고 하죠

04:14.550 --> 04:21.090
그 방법에 관한 잘 알려진 공식이 아주 많은데 각각 장단점이 있죠 그중 하나를

04:21.090 --> 04:22.860
선택할게요

04:22.860 --> 04:27.180
비트는 성능 면에서 조금 더 비싸죠

04:27.180 --> 04:31.050
비트 박스는 좀 힘들지만 좋은 결과를 낳아요

04:31.050 --> 04:33.840
이 제품부터 시작하시길 추천해요

04:33.990 --> 04:40.170
그리고 메모리 문제가 발생하면 메모리 소모량을 줄여주는

04:40.170 --> 04:42.150
대안이 있어요

04:42.300 --> 04:44.820
하지만 그 과정을 최적화라고 하죠

04:44.820 --> 04:49.410
그 작업을 위해 선택한 알고리즘을 최적화 장치라고 해요

04:49.410 --> 04:54.210
또 다른 hyperperameter죠 다양한 걸 시도해보고 어떤지 보세요

04:54.540 --> 04:57.390
말이 너무 많다는 걸 깨달았어요

04:57.390 --> 05:04.860
hyperperameter에 대한 대화도 비트로 사용했어요 훈련 프로세스에 대해 설명하려고요

05:04.950 --> 05:10.590
지금까지 배운 배경이 도움이 됐길 바랍니다 앞으로 일어날

05:10.590 --> 05:17.340
일에 대비해서요 구글 Colab으로 돌아가 SFT 트레이너를

05:17.340 --> 05:24.120
시작할 겁니다 우리만의 LM을 세밀하게 조정하기 위해서요
