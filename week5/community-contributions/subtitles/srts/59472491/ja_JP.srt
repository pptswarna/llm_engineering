WEBVTT

00:00.620 --> 00:01.790
お帰りなさい｡ 

00:01.790 --> 00:07.370
もしあなたがJupyterLabで私についてきているのであれば､

00:07.370 --> 00:16.550
コーヒー休憩のためにJupyterLabを立ち去る必要があるだろう｡

00:16.550 --> 00:17.750
それがこれだ｡ 

00:17.780 --> 00:23.810
自動車関連は最大で､ 90万を超えるデータポイントがあり､ オフィスもある｡ 

00:23.870 --> 00:27.830
あ､ すみません､ エレクトロニクスは40万人以上です｡ 

00:27.830 --> 00:29.600
だから合計すると

00:29.600 --> 00:30.980
何があるのか見てみよう｡ 

00:30.980 --> 00:36.410
合計で2人強だ｡  800万データポイント｡ 

00:36.410 --> 00:38.120
たくさんのデータだ｡ 

00:38.120 --> 00:39.680
データが多すぎるんだ｡ 

00:39.680 --> 00:43.580
私たちがやろうとしているようなトレーニングには､ そのような数字は必要ない｡ 

00:43.640 --> 00:54.020
つまり､ このデータセットに磨きをかけ､ 我々にとって最も価値があり､ 最もシグナルを与えてくれるデータポイントを選択する機会があるということだ｡

00:54.020 --> 00:59.480
ではまず､ トークンの数の分布をもう一度見てみよう｡ 

00:59.510 --> 01:05.310
これは､ 次回オープンソースのラマ・モデルをうまく微調整できるようにするため､

01:05.310 --> 01:10.440
またフロンティア・モデルを扱うときにコストを低く抑えるために､

01:10.440 --> 01:19.350
特に達成しようとしたことです｡

01:19.890 --> 01:22.050
もう一度価格を見てみよう｡ 

01:22.290 --> 01:33.690
これは200万ドルほどの価格分布で､ 999ドル以下という制約があることがわかるだろう｡

01:33.780 --> 01:39.510
つまり､ 1〜999の間です｡ これは､ トレーニングのすべてを歪めてしまうような異常値がなく､

01:39.510 --> 01:45.060
管理しやすいデータセットにするために入れた制約です｡

01:45.450 --> 01:52.200
しかし､ データセットが小さい数字に非常に偏っているという同じ問題があることがわかるだろう｡

01:52.230 --> 01:55.320
そして､ 非常に細いトレイルがある｡ 

01:55.320 --> 01:57.660
ええと､ これは300までです｡ 

01:57.720 --> 02:05.460
だから､ 1000まで行って､ データセットの終わりまで行ってみよう｡ 

02:05.490 --> 02:06.030
そうだ｡ 

02:06.060 --> 02:06.540
あれを見ろ｡ 

02:06.540 --> 02:07.110
これがそうだ｡ 

02:07.140 --> 02:14.850
909に達するデータもある｡  49だが､ ほとんど見えない｡ 

02:14.850 --> 02:18.840
軸にはほとんど触れていない｡ 

02:18.960 --> 02:30.030
ええと､ 80万人ほどのデータセットが低価格帯のものばかりだからです｡ 

02:30.480 --> 02:34.170
ええと､ もうひとつ､ カテゴリーをざっと見てみましょう｡ 

02:34.170 --> 02:40.920
この小さな棒グラフは､ 各カテゴリーのデータ数を示している｡

02:40.920 --> 02:44.970
つまり､ ここでも自動車が90万人と圧倒している｡ 

02:44.970 --> 02:51.510
続いて､ 工具とホームセンターが40万件､ エレクトロニクスが40万件と続く｡ 

02:51.510 --> 02:59.580
というのも､ モデルが特定のカテゴリーの特定の価格についてより深く学ぶことに偏ってしまったり､

02:59.580 --> 03:09.670
歪んでしまったりするのは避けたいからだ｡

03:09.790 --> 03:17.380
それが世界の現実なのだから｡

03:17.530 --> 03:23.110
しかし､ トレーニングの進歩を歪めたり､ 妨げたりするようなことはしたくない｡ 

03:23.350 --> 03:31.090
それで､ これからやることは､ このデータセットのサンプルから､ 価格とカテゴリーをよりよく表現したより小さなデータセットを得るために､

03:31.090 --> 03:37.960
いくつかの選択をすることだ｡

03:37.960 --> 03:42.910
データセットのサイズは約40万点だ｡ 

03:42.940 --> 03:48.100
それに､ 微調整のための大規模なデータセットであっても､ それほど大きくする必要はない｡ 

03:48.130 --> 03:50.290
しかし､ 私は大きなジューシーなデータセットを持ちたかった｡ 

03:50.290 --> 03:52.420
だから､ 40万ドルを目指した｡ 

03:52.510 --> 03:54.910
その方法については､ また今度話そう｡ 

03:55.000 --> 04:00.580
そこでまず､ スロットという辞書を作った｡ 

04:00.580 --> 04:01.810
そして､ これが何なのかを教えよう｡ 

04:01.840 --> 04:04.060
そうすれば､ 私がなぜこのようなことをしたのか､ よく理解してもらえるだろう｡ 

04:04.090 --> 04:12.980
スロットは辞書であり､ 辞書のキーは商品の全ドル価格である｡ 

04:12.980 --> 04:17.720
つまり､ 1ドルから9ドルだ｡  99の1､ 2､ 3から999まで｡ 

04:17.720 --> 04:21.830
この辞書には999個のキーがある｡ 

04:21.830 --> 04:29.570
そしてその値は､ その価格を持つすべての商品､ すべてのアイテムのリストになる｡ 

04:29.570 --> 04:36.590
つまり､ スロット辞書の2番スロットには､ 2ドルの商品がすべてリストアップされていることになる｡ 

04:36.620 --> 04:39.860
そうして､ すべてをこれらのスロットに整理している｡ 

04:39.860 --> 04:43.100
基本的にデータセットをバケット化しているんだ｡ 

04:43.370 --> 04:46.010
うーん......それで納得できればいいんだけど

04:46.010 --> 04:47.750
もちろん､ そうでなければこのコードを持ち出す｡ 

04:47.750 --> 04:48.380
それを踏み越える｡ 

04:48.380 --> 04:54.710
Defaultdictは､ 基本的に辞書で､ 辞書にないものがあれば､

04:54.710 --> 05:01.940
自動的に初期化してくれる｡

05:01.970 --> 05:06.230
コードにifテストのようなものを入れる必要がなくなる｡ 

05:06.230 --> 05:08.900
だから､ エレガントで素敵なコードになる｡ 

05:08.900 --> 05:14.430
さて､ ここでちょっと肉厚な機能を説明しよう｡ 

05:14.460 --> 05:14.850
アミティ

05:14.880 --> 05:15.360
アミティ

05:15.420 --> 05:16.170
Jupyterノートブック｡ 

05:16.170 --> 05:16.740
細胞だ｡ 

05:17.010 --> 05:21.390
ええと､ この枠をひとつひとつ見ていきます｡ 

05:21.420 --> 05:23.760
999の各スロット｡ 

05:23.760 --> 05:33.090
そして､ これらのスロットからデータのサブセットをサンプリングし､ トレーニングに使用する代表的なサンプルとするつもりだ｡

05:33.240 --> 05:41.640
この後のヒストグラムに慣れるまで､ 任意で微調整した部分もある｡

05:41.640 --> 05:45.390
だから､ 特別な理由があるわけではない｡ 

05:45.390 --> 05:51.330
試行錯誤を繰り返し､ バランスの取れたデータセットに満足できるようになることだ｡

05:51.330 --> 05:59.040
そしてもちろん､ それをトレーニングに回し､ こうすることでより質の高い結果が得られると自分自身を納得させる｡

05:59.400 --> 06:05.370
それで､ 僕がやっているのは､ 各スロットを順番に見ていって､ 240ドル以上の価値があるものは､

06:05.370 --> 06:09.930
単純にそのスロット全部を取ることにしているんだ｡

06:09.960 --> 06:12.870
私はそれらの点をすべてサンプルに加える｡ 

06:13.320 --> 06:13.950
うーん｡ 

06:14.400 --> 06:16.710
それ以下の金額で｡ 

06:16.710 --> 06:24.930
私は基本的に､ そのスロットから1200のアイテムをサンプリングするコードをここに持っている｡ 

06:24.930 --> 06:29.790
その枠に数千の選手が入るかもしれない｡ 

06:29.820 --> 06:39.090
私はそのスロットから1200を選ぶだけで､ choiceというnumpyのメソッドを使って､ スロットから特定の数字を選ぶことができる｡

06:39.090 --> 06:48.870
そして､ チョイスのいいところのひとつは､ ウェイトと呼ばれるものを渡すことができることだ｡

06:48.870 --> 06:53.970
そして......､ これがウェイトにとって驚きのない形でまとまることを願っている｡ 

06:53.970 --> 07:02.160
私が言いたいのは､ 自動車に関係するものには1の重みを､ それ以外のものには5の重みを与えようということだ｡

07:02.310 --> 07:08.160
繰り返しになるけど､ これは､ 出てくる数字に納得がいくまで､ いろいろな数字を使って遊んでみただけなんだ｡

07:08.160 --> 07:15.930
そして､ 現実の世界で我々が持っているようなデータに忠実でありたいので､ あまりやり過ぎたくなかった｡

07:15.930 --> 07:19.700
しかし､ 我々はデータセットの不均衡を補正したかった｡ 

07:19.820 --> 07:23.570
だから､ 一行ずつ説明するつもりはない｡ 

07:23.570 --> 07:32.900
そして､ 私の言うとおりになっていること､ そしてその結果が気に入っていることを確認してほしい｡

07:32.900 --> 07:37.550
そしてもちろん､ データセットをもう少し違った形で作りたいのであれば､ これはチャンスである｡ 

07:37.610 --> 07:43.880
また､ 私のモデルのパフォーマンスという点では､ あなたが私の結果を打ち負かすことも十分にあり得るし､

07:43.880 --> 07:52.010
カテゴリの重み付けを変えたり､ スロットから別のものを選んだりした方がいいと思うかもしれない｡

07:52.010 --> 07:57.170
だから､ 絶対に実験してみるべきだよ｡ 

07:57.170 --> 07:59.270
でも､ 今はこれを実行した｡ 

07:59.270 --> 08:07.460
現在､ サンプルリストが作成され､ そのサンプルには408,000のデータポイントがある｡ 

08:07.460 --> 08:10.040
だから､ 僕らが目指していたのはそのくらいのサイズなんだ｡ 

08:10.460 --> 08:14.360
では､ 価格の分布を見てみよう｡ 

08:14.360 --> 08:18.230
そして､ 価格の分布という点では､ その方がずっと合理的に見える｡ 

08:18.230 --> 08:24.710
まだ安いものもたくさんあるが､ 安い方ではどの価格帯でも一貫した数字だ｡

08:24.740 --> 08:32.510
そして､ より高価格になるにつれて､ 高価格に伴うデータポイントの完全なまともなセットが存在する｡

08:32.540 --> 08:37.160
いろいろなポイントがあることに気づくだろう｡ 

08:37.160 --> 08:46.100
予想通り､ 399ドル､ 499ドルという値段のものは､ データ・ポイントの数という点で少し急増する｡

08:46.130 --> 08:48.530
それが現実の世界を反映しているのだから｡ 

08:48.530 --> 08:51.140
だから､ データセットの中にそれがあるのはいいことだ｡ 

08:51.140 --> 08:53.840
それを潰したくはない｡ 

08:54.230 --> 09:07.220
この価格のヒストグラムを､ 先ほどの価格のヒストグラムと比較すると､ データの価格分布が改善されていることがすぐにおわかりいただけると思います｡

09:07.250 --> 09:12.080
これは明らかにもっと歪んでいるし､ 現実の世界も歪んでいる｡ 

09:12.170 --> 09:16.070
うーん､ でも､ 高価格帯の商品の方がよく表現されている｡ 

09:16.070 --> 09:23.220
そして､ 質の高い学習ができるようになり､ サンプルの検証も進むということだ｡

09:23.250 --> 09:26.700
それで満足できないなら､ ぜひいくつかのデータセットを作ってほしい｡ 

09:26.730 --> 09:33.120
そしてトレーニングに入ったら､ 両方を試して､ バランスの取れたデータセットがもたらす影響を見ることができる｡

09:33.900 --> 09:36.810
カテゴリーももう一度見てみよう｡ 

09:36.930 --> 09:38.640
これがカテゴリーだ｡ 

09:38.640 --> 09:40.500
だから､ 実は大した違いはないんだ｡ 

09:40.500 --> 09:42.030
少しずれている｡ 

09:42.210 --> 09:44.760
少しバランスが良くなったよ｡ 

09:44.820 --> 09:51.990
うーん､ これ以上訂正したくなかったのは､ 結局のところ､ これが現実の世界を多少なりとも反映していると感じたからだ｡

09:51.990 --> 09:54.360
だから､ 過度に歪めたくはない｡ 

09:54.360 --> 10:00.630
販売されている自動車関連製品の数は､ 他よりも多い｡ 

10:00.630 --> 10:04.950
それで､ これで十分だと思われるが､ アンバランスな部分が少し修正された｡ 

10:05.130 --> 10:08.370
おそらく､ 別の見方として円グラフを見ることができるだろう｡ 

10:08.370 --> 10:13.410
一般的に言って､ 円グラフはデータサイエンティストに不人気であることが多い｡ なぜなら､

10:13.410 --> 10:18.210
棒グラフの方が量を並べて見たり､ 非常に定量的に見たりするのに適しているからだ｡

10:18.420 --> 10:23.400
しかし､ 円グラフは時として有用なビジュアルである｡ 

10:23.490 --> 10:33.180
これはカテゴリー別の円グラフで､ これらの単語のいくつかを分離するために､ 私は明らかに少し仕事をしなければならないが､ あなたはアイデアを得るだろう｡

10:33.390 --> 10:40.530
つまり､ 自動車が最大のシェアを占めてはいるのだが､ それが大量に支配しているわけではない､

10:40.530 --> 10:42.150
ということだ｡

10:42.150 --> 10:45.600
そして､ この2､ 3台が一緒になれば､ 明らかに自動車以上のものになる｡ 

10:45.660 --> 10:47.340
だから､ まったく理にかなっている｡ 

10:47.340 --> 10:50.460
そして､ このチビは家電製品だ｡ 

10:50.460 --> 10:57.510
私たちが昨日始めたのは､ パイの中で1％､ 最も小さいものだった｡ 

10:57.510 --> 10:59.400
この場合は文字通りだ｡ 

11:00.000 --> 11:04.020
これが､ 私たちのデータセットだ｡ 

11:04.020 --> 11:07.170
うーん､ ちょっと大変だったね｡ 

11:07.170 --> 11:13.200
そして､ サンプリングのような､ いくつかの､ ええと､ もっと難しい部分については､ 私は目をつぶっていた｡ 

11:13.350 --> 11:20.190
そして､ また戻ってきて､ それに目を通し､ 自分で評価し､ より良いデータセットを作る可能性があることを強く勧める｡

11:20.370 --> 11:25.260
ええと､ ハブにアップロードする前に､ ようやく最後の分析ができるんだ｡ 

11:25.260 --> 11:27.630
それはまた次のビデオで｡ 
