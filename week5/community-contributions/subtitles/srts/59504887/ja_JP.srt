WEBVTT

00:00.560 --> 00:02.960
さて､ グーグルコラボに再びやってきた｡ 

00:02.960 --> 00:06.230
ここに来るのは久しぶりだが､ おかえりなさい｡ 

00:06.290 --> 00:10.790
あー､ 今週はここで過ごすことになるんだけど､ すごいことになりそうだよ｡ 

00:10.790 --> 00:12.680
実際､ これまでで最高の週になりそうだ｡ 

00:12.680 --> 00:16.610
何度も言うようだが､ 今回は本当に､ ピークがピークになりそうだ｡ 

00:16.640 --> 00:21.740
ここでやっていることより良くなりそうなのは､

00:21.740 --> 00:24.590
来週の第8週だ｡

00:24.590 --> 00:27.860
だが､ 集中を切らさずに7週目を続けてくれ｡ 

00:27.890 --> 00:29.870
ここには通過しなければならないことがたくさんある｡ 

00:29.870 --> 00:32.240
だからまず､ そうしよう｡ 

00:32.270 --> 00:35.660
この週7日目のコラボをセットアップした｡ 

00:35.660 --> 00:37.730
まずはインストールから始めよう｡ 

00:37.850 --> 00:42.380
PeftはParameter Efficient

00:42.380 --> 00:52.460
Fine Tuningの略で､ ローラを含むライブラリの名前だ｡

00:52.610 --> 00:54.080
この図書館の中にあるんだ｡ 

00:54.110 --> 00:55.220
プッ､ プッ｡ 

00:55.220 --> 00:56.420
舌を巻く｡ 

00:56.480 --> 00:57.830
ええと､ それで終わりです｡ 

00:57.830 --> 01:08.180
私は､ GPUボックスの中で最も低いT4ボックスを使用しており､ 15ギガバイトのGPUラムを搭載しているだけだ｡ 

01:08.210 --> 01:12.890
見てわかるように､ ほんの数セルを入れただけで､ そのほとんどをすでに使い切っている｡ 

01:12.890 --> 01:15.170
とにかく､ 僕らはピップをインストールするんだ｡ 

01:15.170 --> 01:20.000
ここではたくさんのインポートを行い､ いくつかの定数を設定する｡ 

01:20.000 --> 01:24.800
ベースモデルはラマ3だ｡  180億ドル

01:24.800 --> 01:27.440
そして､ 私はここで微調整モデルも設定している｡ 

01:27.440 --> 01:29.690
まだ微調整はしていない｡ 

01:29.690 --> 01:30.110
私は｡ 

01:30.140 --> 01:31.670
これは未来から来たものだ｡ 

01:31.790 --> 01:42.980
ラウラ・マトリックスがターゲット・モジュールに適用されるファインチューニング・モデルがどのようなものかをお見せするために持ってきたんだ｡

01:43.160 --> 01:52.310
そして､ ここに3つのハイパーパラメーターがある｡ 

01:52.580 --> 01:56.690
8から始めて16､ 32と言ったのを覚えているだろう｡ 

01:56.720 --> 01:58.130
32歳まで来たよ｡ 

01:58.130 --> 02:00.680
結局､ そこに行き着いたんだ｡ 

02:00.680 --> 02:03.650
それで､ ここで32をやっているんだ｡ 

02:03.770 --> 02:06.980
ええと､ アルファは経験則で､ ダブルRだ｡ 

02:07.010 --> 02:08.630
それで64点だ｡ 

02:08.630 --> 02:10.610
そしてターゲット・モジュール｡ 

02:10.610 --> 02:14.780
これが､ 私たちがターゲットにしている4つのレイヤーの名前だ｡ 

02:14.780 --> 02:17.420
その理由はすぐにわかるだろう｡ 

02:17.420 --> 02:22.880
そして､ これがラマ・モデルの最も一般的なセットアップだ｡ 

02:22.910 --> 02:27.590
他のモデルではレイヤーの名前が違うかもしれないが､ そう､ ターゲットモジュールに割り当てるこのリストに､

02:27.590 --> 02:32.120
ターゲットとするレイヤーの名前を与えるのだ｡

02:32.870 --> 02:33.530
オーケー｡ 

02:33.530 --> 02:38.990
次に､ これはハギング・フェイスにログインするために何度かやったことのある標準的なものだ｡ 

02:39.200 --> 02:43.760
ええと､ もしあなたがハグする顔のアカウントを持っていないなら､ もちろんあなたは今頃ハグする顔のアカウントを持っているでしょう､

02:43.760 --> 02:45.290
といういつもの宣伝文句があります｡

02:45.440 --> 02:51.680
でも､ そこでログインして､ 無料だし､ トークンももらえるし､ それからColabのこのセクションに行って､

02:51.710 --> 02:59.570
キーというのを使って､ トークンを入れるんだ｡

02:59.570 --> 03:03.500
そうすれば､ このセルを走らせれば､ ハギング・フェイスにログインできる｡ 

03:03.710 --> 03:06.260
トークンを入力することもできる｡ 

03:06.260 --> 03:09.440
ノートブックへのアクセスに問題がある場合｡ 

03:09.680 --> 03:18.480
さて､ それでは早速､ ベースモデルを量子化せずに読み込んでみよう｡ 

03:18.480 --> 03:19.440
おかしなことはしない｡ 

03:19.470 --> 03:26.940
ラマ3世を全部読むだけだ｡  180億円のベースモデルは､ リャマ・シリーズの最小モデルであることを忘れてはならない｡

03:27.090 --> 03:33.120
ええと､ デバイス・マップ・イコール自動というのは､ GPUがあればそれを使うということです｡ 

03:33.150 --> 03:36.810
そして､ 私は今これを実行するつもりはない｡ 

03:36.930 --> 03:40.830
ええと､ GPUに全部は入りきらないという警告が出たんだ｡ 

03:40.830 --> 03:42.600
だから､ その一部がCPUに使われたんだ｡ 

03:42.600 --> 03:47.850
右側のリソースを見ると､ GPUは15ギガのうち11ギガが埋まっていて､

03:47.850 --> 03:53.880
ラムも13ギガ近く埋まっている｡

03:53.880 --> 04:01.710
だから､ ここでの急上昇の理由は､ 一度やって､ 再開してまたやったからだ｡

04:01.710 --> 04:07.620
ええと､ これを実行すると､ 明らかに1がトップに上がるのが見えるだけです｡ 

04:07.710 --> 04:08.310
オーケー｡ 

04:08.340 --> 04:14.460
では､ このベースモデルのメモリ使用量をプリントしてみよう｡ 

04:14.460 --> 04:18.990
そしてまた､ もしこれをトレーニングしようと思ったら､ 何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も何度も｡ 

04:18.990 --> 04:25.440
これは､ ベースモデルがどれだけのメモリーを使用しているかということであり､ そのメモリーのフットプリントは32GBのすぐ北にあり､

04:25.440 --> 04:29.790
32GBのメモリーが使用されている｡

04:29.790 --> 04:31.890
以前､ 私たちがそう言ったことを覚えているだろう｡ 

04:31.890 --> 04:38.310
基本的には､ 80億の各パラメーターに対して32ビットの浮動小数点数だ｡ 

04:39.120 --> 04:39.690
オーケー｡ 

04:39.690 --> 04:41.610
だから､ それは大きいんだ｡ 

04:41.850 --> 04:43.770
ええと､ ちょっと見てみましょう｡ 

04:43.770 --> 04:47.340
ベースモデルそのものをプリントして見ることができる｡ 

04:47.340 --> 04:54.870
そして､ これは今､ どんな風に見えるかというと､ うーん､ 前に簡単に書いたけど､ ちょっと小休止｡ 

04:54.870 --> 05:02.310
繰り返しになるが､ これは深く理論的な授業になるつもりはないので､ できることを言う以外にはあまり説明するつもりはない｡

05:02.340 --> 05:14.070
このニューラルネットワークのアーキテクチャーを見れば明らかなように､

05:14.070 --> 05:18.840
まず埋め込み層がある｡

05:18.840 --> 05:23.700
つまりこれは､ 前に話したLMSをエンコードするエンコーダーのようなものだ｡ 

05:23.730 --> 05:32.270
最初の層は､ トークンumをベクトルumに埋め込む｡ 

05:32.300 --> 05:36.740
実際､ これが可能なトークンの数の次元なんだ｡ 

05:36.740 --> 05:40.130
そして､ これが埋め込まれたベクトルの次元数である｡ 

05:40.550 --> 05:46.820
そのため､ ラマ・デコーダー・レイヤーと呼ばれる32のレイヤーが存在する｡ 

05:46.850 --> 05:48.860
32セットだ｡ 

05:48.860 --> 05:53.510
そして､ その32人はそれぞれこのように見える｡ 

05:53.960 --> 05:55.490
それを正しく伝えよう｡ 

05:55.520 --> 06:08.180
このレイヤーは､ q､ proj､ k proj､ v､ o projと呼ばれるアテンション・レイヤーのセットで構成されています｡

06:08.180 --> 06:15.410
そしてこれらのレイヤーは､ ターゲット・モジュールでターゲットにしたレイヤーである｡

06:15.440 --> 06:19.190
他の方法も試すことができるが､ これが最も一般的な方法だ｡ 

06:19.190 --> 06:26.120
そして､ これらのレイヤーのいくつかは､ 4000の奇妙な寸法が出たり入ったりしているのがわかるだろう｡ 

06:26.330 --> 06:31.530
ええと､ これとこれと､ いくつかは4000人入って1000人出ている｡ 

06:32.700 --> 06:36.030
だから､ 彼らには次元の違うものがある｡ 

06:36.030 --> 06:39.690
そしてそれは､ ローラAとローラBを見るときに多少関係してくる｡ 

06:39.720 --> 06:47.430
しかし､ 私はこのことについてあまり深入りするつもりはない｡

06:47.670 --> 06:54.450
そして多層パーセプトロン層があり､ 例えば､ アップは次元数を爆発的に増やし､

06:54.450 --> 07:00.090
ダウンは次元数を減らす｡

07:00.090 --> 07:03.150
そしてその後に活性化関数が続く｡ 

07:03.150 --> 07:06.360
繰り返しになるが､ こういうことに詳しい人たちのために｡ 

07:06.390 --> 07:12.540
Lamaに使われている活性化関数はSeluで､ PyTorchのドキュメントで見ることができる｡ 

07:12.540 --> 07:19.020
それを見て､ それが何なのか､ なぜ使われるのか､ もっと詳しく知ることができる｡ 

07:19.410 --> 07:25.830
ええと､ だから､ その後にレイヤー・ノルム・レイヤーが続く｡ 

07:26.280 --> 07:33.110
そして一番最後にLMヘッドという直線的な層がある｡ 

07:33.110 --> 07:34.910
そして､ これは時に標的にされる｡ 

07:34.910 --> 07:36.950
これはターゲットモジュールに追加されることもある｡ 

07:36.950 --> 07:41.930
前にも述べたように､ 何かを生成したい場合､ 学習させたいことの一部が結果を生成することである場合､

07:41.930 --> 07:47.120
それは何らかの別の形式を取ることになる｡

07:47.150 --> 07:52.700
JSONの特定の構造が必要かもしれないし､ まったく別のものが必要かもしれない｡ たとえば､

07:52.730 --> 07:58.490
別の言語を話させたいとか､ 非常にユニークな方法で物事を構造化させたいとか｡

07:58.520 --> 08:02.210
それなら､ ターゲットとするモジュールでも､ これをターゲットにするかもしれない｡ 

08:02.840 --> 08:05.540
うーん､ でもこれで建築の雰囲気はわかるよね｡ 

08:05.540 --> 08:12.170
そしてすぐに､ ローラのアダプターを見れば､ なぜ私がこのことに時間を割いたのかがわかるだろう｡

08:12.470 --> 08:13.250
分かった｡ 

08:13.250 --> 08:15.020
それで32GBを使い切った｡ 

08:15.050 --> 08:23.120
次に必要なのは､ このセッションを再起動することだ｡ ランタイムからセッションの再起動を実行し､ メモリーをクリアにして続行できるようにする｡

08:23.480 --> 08:27.110
キャッシュをクリアするトーチコマンドはいくつかあるが､ 実際には十分な攻撃性はない｡ 

08:27.110 --> 08:31.070
私たちが多くのものを消費してきたせいで､ まだ多くのものを抱えている｡ 

08:31.100 --> 08:33.950
今はセッションを再開するしかない｡ 

08:33.950 --> 08:35.420
だから､ そうするつもりだ｡ 

08:35.540 --> 08:40.430
また次のビデオでお会いしましょう｡ 
