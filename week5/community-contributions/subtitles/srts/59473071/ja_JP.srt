WEBVTT

00:01.340 --> 00:02.180
やあ､ みんな｡ 

00:02.210 --> 00:04.250
何を考えているかは分かる｡ 

00:04.250 --> 00:07.130
今週はトレーニングウィークのはずだった｡ 

00:07.160 --> 00:11.900
フロンティア・モデルを微調整するために､ 私はすべてを設定した｡ 

00:11.900 --> 00:13.250
私たちは何をしなかったのか？

00:13.250 --> 00:15.110
私たちはフロンティアモデルを微調整していない｡ 

00:15.110 --> 00:16.490
だが､ いいニュースがある｡ 

00:16.490 --> 00:18.140
今日がその日だ｡ 

00:18.170 --> 00:19.460
今日がその日だ｡ 

00:19.460 --> 00:25.550
しかし､ 今日もまた､ 私たちと同じように､ ある意味で失望させられるかもしれないことを覚悟しておかなければならない｡ 

00:25.580 --> 00:26.720
いずれ分かるだろう｡ 

00:26.900 --> 00:35.720
うーん､ でも､ 勇敢な新世界に乗り出すのだから､ その覚悟はしておいてほしい｡ 

00:35.720 --> 00:51.140
簡単にまとめると､ すでにフロンティア・モデルを使ってテキストやコードを生成したり､ APIを使ったり､ パイプラインと下位レベルのトランスフォーマーAPIの両方を通してオープンソースのモデルを抱き合わせで使ったりすることができる｡

00:51.140 --> 00:59.390
モデルを直接使用するのと同様に､ ランゲ・チェインを使用して高度なラグ・パイプラインを作成することも､ ランゲ・チェインを使用しないこともできます｡

00:59.390 --> 01:09.110
そして最も重要なことは､ 問題解決のための5つのステップ戦略に従うことができるようになったことだ｡

01:09.140 --> 01:13.910
その結果､ 多くの時間をデータのキュレーションに費やし､ ベースラインモデルを作成することが判明した｡ 

01:13.910 --> 01:16.760
トレーニングも行ったが､ それはベースラインモデルのトレーニングだった｡ 

01:16.790 --> 01:22.970
伝統的なMLは､ 私たちがまだ行っていない､ 訓練された､ 微調整された､ フロンティアモデルである｡ 

01:22.970 --> 01:24.500
そして､ それが今日私たちがやろうとしていることだ｡ 

01:24.530 --> 01:30.170
ファイン・チューニングのプロセスをモデルの前で理解し､ そのためのデータセットを作成し､ ファイン・チューニングを実行し､

01:30.200 --> 01:35.360
そしてファイン・チューニングされた新しいモデルをテストする｡

01:35.930 --> 01:42.290
この種のモデルの文脈では､

01:42.290 --> 01:54.020
微調整はトレーニングと同義なのです｡

01:54.020 --> 02:00.680
つまり私たちは常に､ 訓練済みの既存のモデル（事前訓練済みモデル）を使って､ 転移学習を利用しながら訓練を重ねているのです｡

02:00.680 --> 02:12.030
転移学習とは､ 事前訓練済みの既存のモデルを使い､ もう少し訓練を重ねれば､ 新しいタスクに対してより優れた訓練ができるようになるという理論です｡

02:12.120 --> 02:14.760
そしてそれは微調整とも呼ばれる｡ 

02:15.450 --> 02:24.030
それでは､ OpenAIで微調整を行うための3つのステップについて説明しよう｡ 

02:24.180 --> 02:33.390
GPT4ゼロ､ あるいはGPT4ミニを獲得し､ 微調整するためには､ 3つのことを守る必要がある｡

02:33.420 --> 02:39.270
最初のステップは､ トレーニングに使用するトレーニングデータを準備することだ｡ 

02:39.270 --> 02:44.820
従来のモデル､ 線形回帰などの文脈では､ 明らかにトレーニングデータを使用する｡ 

02:44.820 --> 02:49.170
我々はいくつかのトレーニング例を得て､ それを線形回帰モデルにかけた｡ 

02:49.170 --> 02:50.790
だから､ トレーニングデータを作らなければならない｡ 

02:50.820 --> 02:54.720
そしてそれをOpenAIにアップロードしなければならない｡ 

02:54.720 --> 03:07.160
そして､ JSON L（JSON行の略）と呼ばれる特定のフォーマットでトレーニング・データを受け取る｡

03:07.640 --> 03:13.880
これからトレーニング､ 微調整を行い､ これらのチャートはすべて下向きになる｡ 

03:13.880 --> 03:14.900
あなたを悩ませるかもしれない｡ 

03:14.930 --> 03:20.270
物事がうまくいっていないように見えるが､ とんでもない｡ 

03:20.300 --> 03:21.590
トレーニングの損失｡ 

03:21.710 --> 03:23.960
そしてもちろん､ あなたは損失が減ることを望んでいる｡ 

03:23.960 --> 03:25.880
つまり､ 状況は良くなっているということだ｡ 

03:25.970 --> 03:32.480
だから私たちはタカのようにチャートを注視し､ 損失が減少していることを確認しようとする｡

03:33.200 --> 03:39.560
そして最も重要なのは､ バッチの過程でトレーニングのロスを調べ､

03:39.590 --> 03:44.090
さらに検証のロスを調べることだ｡

03:44.090 --> 03:45.740
ええと､ それも降りてくるんですか？

03:45.770 --> 03:49.940
トレーニングの損失を見るだけでは､ トレーニングデータにオーバーフィットする可能性があるからだ｡ 

03:49.970 --> 03:56.990
なぜなら､ トレーニング・データを1エポック実行するだけだからだ｡

03:56.990 --> 03:58.850
そしてエポックとは､ あなたがそう呼ぶものだ｡ 

03:58.940 --> 04:07.370
そして､ 同じデータで2回目を行う｡

04:07.400 --> 04:10.070
これはトレーニングの第2のエポックと呼ばれるものだ｡ 

04:10.490 --> 04:16.130
なぜなら､ 我々はトレーニングデータをたくさん持っているからだ｡ 

04:16.130 --> 04:19.580
もう少し多くのトレーニングデータを使って､ 1エポックやったほうがいいかもしれない｡ 

04:19.580 --> 04:27.230
そして､ すべてのデータは常に新しいデータであるため､ トレーニング損失は検証損失と同様に我々にとって有用である｡

04:28.100 --> 04:31.760
そして最後に結果を評価する｡ 

04:31.760 --> 04:38.240
そして､ その結果に基づいて微調整をし､ 繰り返し､ 続けていく｡ 

04:38.390 --> 04:40.310
これがその段階だ｡ 

04:41.060 --> 04:44.870
その第一は､ データを準備することだ｡ 

04:45.050 --> 04:53.750
だからOpenAIは､ JSON Lと呼ばれるフォーマット､ つまりJSONデータの一連の行でそれを期待する｡

04:53.900 --> 04:56.030
JSONデータと同じじゃないか､ と思うかもしれない｡ 

04:56.030 --> 04:56.870
それは違う｡ 

04:56.870 --> 04:59.000
コレクションの中には入っていない｡ 

04:59.000 --> 04:59.900
だからリストには入っていない｡ 

04:59.900 --> 05:02.770
カンマ付きの角括弧では始まらない｡ 

05:02.770 --> 05:11.410
このファイルの各行は､ 中かっこで始まり､ 中かっこで終わる別々のJSONオブジェクトである｡

05:11.410 --> 05:16.270
これは微妙な違いだが､ JSONオブジェクトを記述していないことを想定していない場合､

05:16.270 --> 05:19.390
リストに囲まれてしまうため､ 注意を引くことができる｡

05:19.390 --> 05:27.550
あなたはこのファイルにJSONの行を書き､ それぞれの行は私たちにとって非常に馴染みのあるものになる｡

05:27.580 --> 05:30.730
メッセージという属性を持つ｡ 

05:30.730 --> 05:40.750
そしてそこに入るのは､ 私たちがよく知っている辞書のリストであり､ 各辞書には役割と内容がある｡

05:40.750 --> 05:42.100
それは会話だ｡ 

05:42.100 --> 05:45.460
つまり､ これが各列に入るものだ｡ 

05:45.610 --> 05:51.040
おわかりのように､ この特殊なデータセットをアップロードするために細工をする｡ 

05:52.240 --> 05:53.260
分かった｡ 

05:53.680 --> 05:57.130
雑談はこれくらいにしておこう｡ 

05:57.160 --> 05:58.900
Jupyter Labに行ってみよう｡ 

05:58.900 --> 06:01.000
実際に走らせてみよう｡ 

06:01.000 --> 06:05.230
そして初めてフロンティアモデルをトレーニングする｡ 

06:05.260 --> 06:06.370
そうしよう｡ 
