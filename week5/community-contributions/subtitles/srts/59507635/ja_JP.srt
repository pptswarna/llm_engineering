WEBVTT

00:00.950 --> 00:02.870
興奮してるかな？

00:02.870 --> 00:04.160
そうあるべきだ｡ 

00:04.190 --> 00:09.110
コースの8割を消化し､ この瞬間まですべてが積み重なってきた｡ 

00:09.140 --> 00:15.920
今日､ あなたは楽しみながら､ そして利益を得るために､ 独自のLLMをトレーニングすることになる｡ 

00:16.100 --> 00:17.780
すべてはここから始まる｡ 

00:17.780 --> 00:20.360
では､ 今日は実際に何があるのか？

00:20.360 --> 00:23.600
まずは､ あまりスリリングではないものから始めよう｡ 

00:23.600 --> 00:25.700
ハイパーパラメーターについて､ もう1度話そう｡ 

00:25.700 --> 00:29.090
必要不可欠なハイパーパラメーターをいくつか用意した｡ 

00:29.090 --> 00:35.060
これがとても重要な理由は､ ハイパーパラメーターの最適化を自分で行うことになるからだ｡

00:35.060 --> 00:37.490
試行錯誤を意味する洒落た言葉だ｡ 

00:37.490 --> 00:41.270
そして､ 自分がプレーしていることの背景を理解する必要がある｡ 

00:41.270 --> 00:46.430
そしてこれは､ 他のモデルに勝てるものを作るチャンスでもある｡ 

00:46.460 --> 00:51.200
どのようなレバーを使って実験するのかを理解することだ｡ 

00:51.230 --> 00:56.630
それが､ トップモデルを作るための研究開発の核心である｡ 

00:56.630 --> 01:00.260
そこで､ ハイパーパラメーターについて説明しよう｡ 

01:00.480 --> 01:05.790
そして､ 監督付きの微調整トレーナー､ SFTをセットアップする｡ 

01:05.820 --> 01:11.040
トレーナーというのは､ このトレーニングの中核をなす存在だ｡ 

01:11.130 --> 01:16.440
ハギング・フェイスからTRLライブラリーの一部を見て､ それからキックオフだ｡ 

01:16.470 --> 01:20.760
独自のLMトレーニングプロセス

01:20.820 --> 01:22.350
素晴らしいものになるよ｡ 

01:22.920 --> 01:28.800
というわけで､ まずは素晴らしい話に入る前に､ このプロセスをコントロールする重要なハイパーパラメーターについて､

01:28.800 --> 01:36.810
Qローラから話をする必要がある｡

01:36.810 --> 01:41.970
そこで最初に挙げるハイパーパラメーターは､ ターゲット・モジュールを呼び出すためのものだ｡ 

01:41.970 --> 01:44.370
そして､ これが何なのか､ 正確に覚えていただけたと思う｡ 

01:44.700 --> 01:52.890
トランスフォーマーのアーキテクチャがあれば､ ラマ3のようなベースモデルがある｡  1､ この巨大で偉大な建築物を微調整しようとするには､

01:52.890 --> 01:55.320
あまりにも大きすぎる｡

01:55.320 --> 02:00.850
だから､ その代わりにアーキテクチャー内のいくつかのレイヤーを選び､ それをレイヤーと呼ぶ｡ 

02:00.850 --> 02:03.400
ターゲット・モジュールとは､ これからターゲットにするモジュールのことだ｡ 

02:03.430 --> 02:04.660
何でも冷凍するんだ｡ 

02:04.660 --> 02:06.520
ウエイトを最適化するつもりはない｡ 

02:06.520 --> 02:07.630
数が多すぎるんだ｡ 

02:07.630 --> 02:09.220
これらのターゲット・モジュールでもだ｡ 

02:09.220 --> 02:10.660
これをトレーニングするつもりはない｡ 

02:10.660 --> 02:16.780
そうではなく､ 低次元の行列を片側に持ってきて､ この低次元の行列を訓練し､

02:16.780 --> 02:22.210
それを元のターゲット・モジュールに適用するのだ｡

02:22.210 --> 02:28.420
それを掛け合わせ､ ウェイトのデルタとして使うのだ｡

02:28.510 --> 02:38.230
そうして､ この小人を訓練して､ ターゲット・モジュールに適用するんだ｡ ターゲット・モジュールとは､ 大きなアーキテクチャーの中で選択されたレイヤーのことだ｡

02:38.230 --> 02:44.920
そして､ この大きな3Dゴーグルをロゴとして､ アイコンとして使っている｡ 

02:45.040 --> 02:51.190
Rは､ この低次元のアダプターマトリックスの次元数を表している｡ 

02:51.190 --> 02:56.350
ええと､ 言語学習の課題では､ 8から始めるのが一般的です｡ 

02:56.530 --> 03:07.020
このプロジェクトでは､ Rを32にしました｡ 学習データがたくさんあるので､ かなりの数のパラメータを使って学習できると考えたからです｡

03:07.200 --> 03:10.140
しかし､ もしそれでメモリが足りなくなるようなら､ 8枚にすることもできる｡ 

03:10.170 --> 03:16.890
実際には､ 8と16と32の差はごくわずかだったと言うべきだろう｡ 

03:16.890 --> 03:19.110
確かに改善されたが､ それほど大きな改善ではない｡ 

03:19.110 --> 03:22.470
記憶力に問題がある場合は､ Rを8にしてください｡ 

03:22.500 --> 03:25.320
もし､ あなたが小さい箱を使っているなら､ それで十分だ｡ 

03:25.440 --> 03:31.800
32ドルというのは少々高いが､ トレーニングデータの量を考えれば､ それだけの価値はある｡ 

03:32.550 --> 03:36.570
アルファはスケーリング・ファクターである｡ 

03:36.570 --> 03:42.990
このアダプターをターゲット・モジュールに適用する際に､ その重要性を倍増させるために使用される｡ 

03:42.990 --> 03:46.680
実は､ ラウラのマトリックスは2つあることを覚えているだろうか｡ 

03:46.680 --> 03:54.600
ひとつはローラA､ もうひとつはローラBと呼ばれ､ 計算式では重さの変化はアルファ値になる｡

03:54.630 --> 03:58.520
スケーリングファクターにA×Bをかけたもの｡ 

03:58.520 --> 04:02.410
それが､ このコースで習う数学の一番多いところだ｡ 

04:02.920 --> 04:05.230
そして､ それは行き過ぎではないと思う｡ 

04:05.230 --> 04:07.360
アルファとはそういうものだ｡ 

04:07.360 --> 04:08.530
スケーリングファクターだ｡ 

04:08.530 --> 04:12.130
そして経験則では､ アルファはダブルRであるべきだ｡ 

04:12.220 --> 04:13.630
それは誰もがやっていることだ｡ 

04:13.630 --> 04:16.480
ぜひ他のアルファ値を試してみてほしい｡ 

04:16.480 --> 04:20.650
しかし､ しかし､ アルファは2Rでやるのが普通だ｡ 

04:20.650 --> 04:25.120
そこで､ まずRを32､ アルファを64とする｡ 

04:26.230 --> 04:33.760
定量化とはもちろん､ ベースモデルの重みの精度を下げることを指す｡

04:33.760 --> 04:35.830
ベースモデルは32ビット｡ 

04:35.830 --> 04:36.490
その中にある｡ 

04:36.550 --> 04:41.980
私たちはそれを8ビット､ あるいは4ビットにまで減らす｡ 

04:42.070 --> 04:47.560
ベースモデルでそれをやってみたところ､ やはり結果が出た｡ 

04:47.650 --> 04:51.730
素晴らしい結果ではなかったが､ ベースモデル全体に言えることだと思う｡ 

04:51.730 --> 04:56.380
実際､ 8ビットモデルは4ビットモデルよりも成績が良かったが､

04:56.380 --> 04:58.420
どちらもかなり惨めだった｡

04:58.730 --> 05:02.900
そしてぜひ､ 8ビットモデルでのトレーニングも試してみてほしい｡ 

05:02.900 --> 05:07.640
しかし､ 4ビットのモデルでトレーニングすることにした｡ 

05:07.640 --> 05:12.650
でも､ 8ビットを試してみて､ 有意に異なる結果が得られるかどうか､

05:12.650 --> 05:13.910
興味があるね｡

05:14.630 --> 05:21.920
そして最後のハイパーパラメーターは､ コード・ドロップアウトでお見せする以外､ これまでお話ししたことのない新しいものです｡

05:21.920 --> 05:24.440
つまり､ ドロップアウトはタイプなのだ｡ 

05:24.440 --> 05:29.960
正則化テクニックとして知られているテクニックで､ いくつかありますが､ つまり､

05:29.960 --> 05:35.840
モデルがいわゆるオーバーフィッティングをしないように設計されたテクニックです｡

05:36.020 --> 05:43.340
オーバーフィッティングとは､ モデルがあまりにも多くの訓練データを取得し､ あまりにも多くの訓練を経て､

05:43.340 --> 05:52.580
訓練データセットに含まれるデータの構造を正確に予想し､ その通りの答えを返すようになることである｡

05:52.580 --> 05:59.330
そして､ もはや提案されていることの一般的な傾向を理解するのではなく､

05:59.330 --> 06:04.100
その言葉やその後に来る予測に集中するようになる｡

06:04.100 --> 06:10.520
その結果､ トレーニングデータセットで見たことのないような新しいポイントを与えると､ 一般的なテーマを学習しているのではなく､

06:10.550 --> 06:16.280
そのテーマに合わせて学習しているため､ 成績は非常に悪くなる｡

06:16.310 --> 06:21.980
このトレーニングデータセットの特殊性を学ぶのは大変だった｡ 

06:22.010 --> 06:24.530
また少し手探り状態になってしまったが､ ご理解いただけただろうか｡ 

06:24.560 --> 06:31.040
訓練データセットと結果に正確に準拠しすぎることをオーバーフィッティングと呼ぶ｡ 

06:31.040 --> 06:36.380
そして､ 何を予測しようとしているのかの一般的な風味を学んでいない｡ 

06:36.560 --> 06:38.000
あの､ あの味です｡ 

06:38.000 --> 06:39.770
そういうニュアンスなんだ｡ 

06:39.770 --> 06:42.020
それをモデルに教えようとしているんだ｡ 

06:42.260 --> 06:46.280
ええと､ つまり､ それが前文であり､ 何の説明なんですか？

06:46.310 --> 06:47.300
オーバーフィッティングとは何か？

06:47.300 --> 06:53.690
しかし今､ ドロップアウトが何をするのかを正確にお伝えすると､ 実に単純なことなのだが､

06:53.780 --> 07:03.870
ドロップアウトが実際にすることは､ ディープ・ニューラル・ネットワークからニューロンのランダムなサブセットを削除することだ｡

07:03.870 --> 07:06.840
変圧器からはランダムなパーセンテージを取る｡ 

07:06.960 --> 07:12.690
まず10％から始め､ ニューロンの10％を取り出し､ それらを消去して活性をゼロにし､

07:12.690 --> 07:16.800
前進パスにも後退パスにも関与しないようにする｡

07:16.800 --> 07:21.300
彼らは次のトークンの予測には関与しないし､ 最適化にも関与しない｡ 

07:21.300 --> 07:23.010
まるでそこにいないかのようだ｡ 

07:23.010 --> 07:35.490
その結果､ トレーニングのたびに､ モデルは異なるサブセット､ 異なる90％のニューラルネットワークを見ることになる｡

07:35.490 --> 07:44.310
そのため､ 重みは､ 1セットの入力トークンを正確に探しすぎることを抑制し､ その代わりに､

07:44.310 --> 07:50.700
異なるニューロンがトレーニング・プロセスに毎回参加することで､

07:50.700 --> 07:54.210
より多くのことを学び始める｡

07:54.240 --> 08:00.670
一般的なテーマは､ さまざまなトークンを期待する方法を具体的に学ぶことだ｡ 

08:00.670 --> 08:05.380
つまり､ 1つのニューロンが専門化しすぎるのを防いでいるのだ｡ 

08:05.380 --> 08:11.560
これは､ ニューロンの10％をプロセスから取り除くという非常に単純化された方法で､

08:11.560 --> 08:17.680
神経回路網におけるより一般的な理解という概念をサポートするものである｡

08:17.680 --> 08:18.910
それがドロップアウトだ｡ 

08:18.910 --> 08:20.230
とてもシンプルなことなんだ｡ 

08:20.260 --> 08:28.180
それに気づいたとき､ それは文字通り､ ニューロンと規範の束を取り除くことなのだ｡ 

08:28.570 --> 08:32.740
通常は5％から20％の範囲だ｡ 

08:32.860 --> 08:36.340
ええと､ 10％をドロップアウトとして使っています｡ 

08:36.340 --> 08:43.150
5％や20％で実験し､ より良い結果が得られるかどうかを確かめるべきだ｡ 

08:43.180 --> 08:47.320
これは非常に実験的なハイパーパラメーターである｡ 

08:47.830 --> 08:51.040
さて､ これがQの5つのハイパーパラメータだ｡ 

08:51.070 --> 08:57.130
ローラ､ 次回は全体的なトレーニングプロセスに関する5つのハイパーパラメータについて話そう｡ 
