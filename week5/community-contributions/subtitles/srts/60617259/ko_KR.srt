WEBVTT

00:00.080 --> 00:05.510
첫 번째 운동을 소개하게 돼서 기쁘고 어떻게 만드실지 정말 기대되네요

00:05.510 --> 00:11.270
PC나 터미널 창을 사용하신다면 아나콘다 프롬프트를 선택하세요

00:11.270 --> 00:15.110
Mac에 있다면 프로젝트 루트 디렉터리로 가야 해요

00:15.140 --> 00:24.710
LM 엔지니어링은 LMS 활성화 콘다 또는 그걸 사용한다면 가상env와 동등한 것으로 아나콘다를 활성화합니다 그런 다음 JupyterLab을

00:24.710 --> 00:29.180
입력해 Jupyter 랩을 불러와요

00:29.180 --> 00:33.590
왼쪽 파일 브라우저에 이런 주가 있을 거예요

00:33.620 --> 00:38.240
아니면 이미 첫 주 폴더에 있을 수도 있죠 그런 경우 이런 식으로 되죠

00:38.240 --> 00:45.500
이제 둘째 날 운동 공책을 꺼내세요 이렇게 나올 거예요

00:45.500 --> 00:47.840
계획은 이래요

00:47.840 --> 00:53.240
코드에서 llama를 호출하는 방법을 볼 거예요

00:53.240 --> 00:59.060
파이썬 코드를 이용해 llama 모델을 호출할 거예요 여러분 컴퓨터에서 실행되는 거죠

00:59.060 --> 01:02.320
그런 다음 할 일은∙∙∙ 첫 번째 설정부터요

01:02.320 --> 01:05.170
get get을 하면 결과를 볼 수 있어요

01:05.170 --> 01:12.760
여러분이 할 일은 어제 완료한 요약 프로젝트를 업데이트하는 겁니다 그리고 OpenAI

01:12.760 --> 01:18.370
호출 대신 Olama를 사용해 여러분의 로컬 모델을 사용하세요

01:18.370 --> 01:23.260
OpenAI API에 등록하지 않았다면 이번이 처음 할 기회예요

01:23.740 --> 01:30.550
우선 라마를 쓸 거라고 설명해 드릴게요

01:30.580 --> 01:34.300
라마를 사용하면 API 요금이 안 들어요

01:34.300 --> 01:35.110
오픈 소스 소스예요

01:35.110 --> 01:36.190
당신 박스에서 작동해요

01:36.190 --> 01:37.300
공짜예요

01:37.330 --> 01:41.080
또 다른 장점은 데이터가 상자 밖으로 나가지 않는다는 거죠

01:41.080 --> 01:46.030
여러분이 뭔가 작업하고 있다면 그게 클라우드로 절대 가면 안 되는

01:46.030 --> 01:54.280
기밀 데이터든 간에 이건 인터넷에서 데이터가 떠나지 않고 로컬에서 작업할 수 있는 기술을 제공하죠

01:54.490 --> 02:04.880
단점은 개척자 모델은 오픈 소스 모델보다 훨씬 크고 강력하다는

02:04.880 --> 02:06.950
거예요

02:06.950 --> 02:10.730
그러니 결과가 그렇게 강하지 않을 거라고 예상해야 해요

02:10.880 --> 02:15.950
하지만 한 통당 1센트도 안 되는 돈을 냈으니 당연한

02:15.980 --> 02:16.970
거죠

02:17.660 --> 02:22.400
먼저 올라마에 가서 올라마를 설치한 걸 복습해보죠

02:22.400 --> 02:27.260
기억하세요 다운로드 버튼만 누르면 바로 레이스가 시작되죠

02:27.260 --> 02:34.970
그걸 완료했다면 여기 localhost 11434 링크를 방문하면 이 올라마가 실행 중인 메시지를 볼 수 있습니다

02:34.970 --> 02:37.670
실행 중이라는 걸 알려주는 거죠

02:37.670 --> 02:45.800
안 나오면 터미널이나 PowerShell을 불러와 OlamaService를 입력하면 실행될

02:45.800 --> 02:46.910
거예요

02:46.910 --> 02:50.750
거기 가면 라마가 뛰는 걸 볼 수 있어요

02:50.750 --> 02:55.910
만약 비트가 작동하지 않는다면 디버깅과 조사를 해보시고 저에게 연락하세요

02:55.910 --> 02:58.240
제가 도와드릴게요

02:58.240 --> 03:00.550
몇 가지 일을 할 거예요

03:00.580 --> 03:02.620
이제 상수를 설정할게요

03:02.620 --> 03:14.860
이건 제 로컬 상자 이 포트의 URL이에요 슬래시 API 슬래시 채팅에서 실행되는 포트가 보이시죠

03:14.860 --> 03:19.180
모델이라 불리는 상수도 가질 거예요 그게 라마 3이 되겠죠 2번요

03:20.170 --> 03:27.280
여기 이 메시지들은 여러분이 이 구조를 알아보셨으면 합니다 메시지와

03:27.280 --> 03:29.800
같은 구조니까요

03:29.830 --> 03:31.420
비트를 좀 다르게 표현해 볼게요

03:31.420 --> 03:36.610
전에 얘기했던 메시지와 같은 거예요

03:36.640 --> 03:39.730
오픈라이에 쓰는 거예요

03:39.760 --> 03:43.750
메시지는 사전 목록이에요 사전들요

03:43.750 --> 03:50.470
각 사전은 역할의 키를 갖고 있고 그 값은 사용자나 시스템 그리고 콘텐츠의 키입니다 그 값은

03:50.470 --> 03:53.170
사용자 메시지나 시스템 메시지죠

03:53.170 --> 03:58.660
간단히 말하자면 사용자 프롬프트가 이런 말을 하는 거죠 인공지능이 재생되는 일부 비즈니스

03:58.660 --> 04:00.940
응용 프로그램을 묘사하라

04:01.180 --> 04:02.290
실행해 보죠

04:02.470 --> 04:09.160
이제 그걸 페이로드라는 JSON 객체에 넣을 거예요 모델과 메시지를 지정하는 거죠 결과를

04:09.160 --> 04:11.080
스트림하지 않고요

04:11.080 --> 04:12.910
Get it 결과만 받으면 돼요

04:13.150 --> 04:23.680
파이썬 패키지 요청을 이용해 JSON 내의 이 URL 패스 게시할 거예요

04:23.680 --> 04:31.750
그런 다음 JSON을 메시지 콘텐츠 필드에서 살펴보겠습니다 호출을 하면

04:31.750 --> 04:33.850
어떻게 되는지 보죠

04:33.850 --> 04:39.610
지금은 제 상자에서 제 상자까지 로컬로 웹 요청을 하고 있어요

04:39.880 --> 04:46.390
라마 3에 연결되고 있어요 라마가 제공하는 2가지 모델이죠

04:46.390 --> 04:48.070
이게 그 결과예요

04:48.070 --> 04:50.860
그 결과는 정말 훌륭하다고 장담해요

04:50.890 --> 04:56.720
상업적 응용 프로그램에 대해 알고자 하는 거니까 일부 응답을

04:56.720 --> 05:00.890
읽어보고 관심 있는 게 있는지 보세요

05:01.160 --> 05:05.330
이걸 보여드린 건 뒤에서 무슨 일이 벌어지는지 설명하고

05:05.330 --> 05:10.970
싶어서예요 이런 URL 즉, 로컬 상자에 대한 웹 요청을 만들고 있죠

05:11.270 --> 05:17.960
하지만 사실 알라마의 친절한 분들이 파이썬 패키지를 개발해 주셔서 작업이 더 간단해졌어요

05:17.960 --> 05:19.430
한 줄로 하면 돼요

05:19.430 --> 05:24.860
이걸로 시작할 수도 있지만 웹 요청을 만드는 단계를 보여드릴게요 실제 무슨 일이

05:24.860 --> 05:27.410
일어나는지 직관적으로 알 수 있게요

05:27.470 --> 05:33.890
Allama라는 멋진 패키지가 있어요 그냥 가져오면 되죠 Allama.챗이라고

05:33.890 --> 05:40.640
입력하고 모델 통과, 메시지 통과 그리고 응답 콘텐츠를 가져오면 돼요

05:40.640 --> 05:46.700
그걸 실행하면 기본적으로 같은 게 나오죠 get

05:46.760 --> 05:48.620
자, 보세요

05:48.650 --> 05:49.640
저기 있네요

05:50.060 --> 05:56.970
그리고 제 생각엔 네, 벌써 차이가 있다는 걸 알겠어요

05:57.000 --> 05:59.070
물론 매번 독특하죠

05:59.160 --> 06:01.650
이건 좀 긴 것 같네요

06:01.860 --> 06:05.310
티업은 다 했어요

06:05.310 --> 06:06.840
이제 당신에게 달렸어요

06:06.840 --> 06:13.860
첫날 이 솔루션을 구축한 걸 기억하실 겁니다 웹사이트를 요약하는 뭔가를 구축했죠

06:13.890 --> 06:18.390
그걸 달성하기 위해 OpenAI에 호출했어요

06:18.840 --> 06:23.550
사실 오픈AI를 호출한 건 여기죠

06:23.760 --> 06:32.430
여러분이 하실 일은 이 2일 차 운동 랩을 계속하면서 동일한 요약기 코드를 추가하는 겁니다

06:32.430 --> 06:40.710
그래야 Ulama 오픈 소스 모델인 llama 3을 사용하는 웹사이트를 만들

06:40.740 --> 06:45.000
수 있으니까요 요약하고 싶다면 다른 모델을 사용하세요

06:45.000 --> 06:46.650
그게 연습이죠

06:46.650 --> 06:50.220
솔루션은 솔루션 폴더에 있어요 필요하면 쓰세요

06:50.220 --> 06:55.290
하지만 이건 된 것 같네요 다음 영상에서 다시 뵙죠
