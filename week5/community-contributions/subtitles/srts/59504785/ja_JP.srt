WEBVTT

00:00.740 --> 00:04.160
そこでこの時点で､ ハイパーパラメーターについて話すことにする｡ 

00:04.160 --> 00:06.320
そのうちの3人を紹介しよう｡ 

00:06.320 --> 00:08.840
そこで､ ハイパーパラメータとは何かを思い出してほしい｡ 

00:08.840 --> 00:10.700
先週も少し話した｡ 

00:10.730 --> 00:13.910
ハイパーパラメータとは､ このレバーのひとつである｡ 

00:14.000 --> 00:19.250
それは､ 実験者であるあなたが､ どうしたいかを選ぶだけのものだ｡ 

00:19.250 --> 00:24.740
ハイパーパラメーターの最適化として知られるプロセスを使って､

00:24.740 --> 00:32.570
さまざまな値を試し､ 手元のタスクに最適なものを見極めることになる｡

00:32.570 --> 00:38.690
そして､ 実際に私たちがやっていることは､ 基本的に試行錯誤なのだ｡ 

00:38.690 --> 00:47.570
理論的な理由があるわけではないので､ 推測と実験を繰り返すことになる｡

00:47.570 --> 00:50.000
実践的な実験の問題だ｡ 

00:50.000 --> 01:01.250
よくあることだが､ うまく機能しているものがあって､ それをコントロールするパラメータがいくつかある｡

01:01.280 --> 01:03.570
それがどうあるべきかを語る理論はまだない｡ 

01:03.600 --> 01:05.490
私たちはこれをハイパーパラメーターと呼んでいる｡ 

01:05.610 --> 01:06.570
そう呼ばれているんだ｡ 

01:06.570 --> 01:11.940
そして､ 正しいセッティングを選ぶまで､ 試行錯誤と推測の世界に身を置くことになる｡ 

01:11.940 --> 01:13.590
それがあなたのモデルに最も適している｡ 

01:13.620 --> 01:17.550
もちろん､ 少し単純化しすぎてはいるが､ 大まかなイメージはつかんでいただけただろうか｡ 

01:17.550 --> 01:21.180
その中で最も重要なのは3つだ｡ 

01:21.180 --> 01:26.760
Qローラのファインチューニングの場合は､ 時々紹介したい｡ 

01:26.760 --> 01:29.640
私たちはこれから､ 彼らと一緒にプレーすることになる｡ 

01:29.640 --> 01:33.720
その最初のものはRと呼ばれ､ これはランクの略である｡ 

01:33.810 --> 01:49.800
つまり､ ラマ・アーキテクチャー内の下位ランクのマトリックスに何次元を使うかということだ｡

01:49.800 --> 01:55.020
低ランクの行列には､ もっと少ない次元数が必要だ｡ 

01:55.020 --> 01:56.640
それこそが､ 彼らのアイデアなんだ｡ 

01:56.850 --> 02:03.600
ええと､ だから､ さっきも言ったように､ この種の言語生成モデルで作業する場合､ タスクによって最初に求めるRの値が違うので､

02:03.600 --> 02:09.520
厳密なルールはないんだ｡

02:09.520 --> 02:18.910
私がいつも使っている経験則であり､ コミュニティで一般的に使われている経験則だと思う｡

02:19.210 --> 02:24.940
ええと､ つまり､ メモリの使用量が非常に少なく､ かなり高速に動作するということです｡ 

02:25.120 --> 02:30.940
そして､ より多くのメモリを消費し､ よりゆっくりと実行される16に倍増し､ より良い結果が得られるかどうかを確認し､

02:30.940 --> 02:37.450
収穫が減少するポイントに達するまで､ さらに倍増する可能性がある｡

02:37.450 --> 02:40.690
スピードは落ちているし､ 時間もかかっているが､ 改善は見られない｡ 

02:40.690 --> 02:47.170
そうなると､ Rが高くても意味がない｡ 今あるデータに必要なパワーはすでに持っている｡

02:47.350 --> 02:51.880
というわけで､ 次に話すのはアルファだ｡ 

02:51.880 --> 02:56.890
そしてアルファは､ 単純に掛け合わされるスケーリング係数である｡ 

02:56.890 --> 03:01.120
このローラ・A・ローラ・Bのマトリックスに適用される｡ 

03:01.120 --> 03:05.530
そして､ そのウェイトを使ってモデルのウェイトを変更する｡ 

03:05.530 --> 03:11.940
この計算式は､ モデル内のウェイトをターゲット・モジュール内で変更する量に相当する｡

03:11.970 --> 03:16.830
アルファ×A行列×B行列｡ 

03:16.830 --> 03:18.630
それらはすべて掛け算になる｡ 

03:18.630 --> 03:21.330
つまり､ アルファ値が大きいほど効果が大きいということだ｡ 

03:21.330 --> 03:32.880
そして､ 実際に使用される経験則は､ ほとんどどこにでもあるものだと思う｡

03:32.910 --> 03:36.540
つまり､ Rが8でスタートした場合､ アルファは16となる｡ 

03:36.570 --> 03:42.540
そしてRが16になるとアルファは32になり､ 32になると64になる｡ 

03:42.540 --> 03:44.940
これが経験則だ｡ 

03:44.940 --> 03:53.040
しかしもちろん､ 異なるアルファを試してみて､ 精度が変わるかどうかを確認する価値は常にある｡

03:54.240 --> 04:08.640
そして3つ目､ 3つの重要なハイパーパラメータの最後が､ アーキテクチャに適応させるターゲットモジュールは何かということです｡

04:08.640 --> 04:19.540
どのレイヤーに注目するかというと......一般的には､ アテンション・レイヤーに注目するのが最も一般的な選択だ｡

04:19.540 --> 04:20.830
よくあることだよ｡ 

04:20.830 --> 04:23.920
コードを見てもらえればわかると思う｡ 

04:23.980 --> 04:28.990
他のモジュールをターゲットにしたい場合もあるだろう｡ 

04:29.080 --> 04:43.780
例えば､ まったく違う言語で出力したい場合などは､ 最終レイヤーのいくつかをターゲットにするといいかもしれない｡

04:43.780 --> 04:49.210
だから､ いくつか､ いくつか､ いくつか､ その仕組みについてもう少し詳しく説明しよう｡ 

04:49.210 --> 04:54.940
しかし､ 一般的に言って､ 最も一般的なのは､ ヘッドレイヤーに注目することである｡ 

04:54.940 --> 04:56.440
それが私たちがやることだ｡ 

04:56.470 --> 04:59.470
どのようにセットアップされるかは､ すぐにわかるだろう｡ 

05:00.670 --> 05:06.760
そして､ これからGoogle Colabに向かい､ これを見て､ いくつかのモデルを見て､ ローラについて話し､

05:06.760 --> 05:12.760
Qとローラについて話し､ そしてこれら3つのハイパーパラメータが実際に使われているのを見ることにする｡

05:13.180 --> 05:14.260
そうしよう｡ 
