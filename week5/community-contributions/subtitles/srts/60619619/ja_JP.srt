WEBVTT

00:00.200 --> 00:03.650
さて､ 4日目は情報密度の濃い一日だった｡ 

00:03.650 --> 00:09.500
また､ トークンやコンテクスト・ウィンドウのようなものにすでに多少慣れている人たちも､

00:09.530 --> 00:17.420
何かひとつかふたつは得て､ より自信を持って実践できるようになっていることを願う｡

00:17.420 --> 00:23.150
確かに､ これは基礎的なものであり､ 来週､ 再来週と､ これをベースに商業的な問題に応用していく中で､

00:23.150 --> 00:26.870
何度も何度も使っていくことになるだろう｡

00:26.870 --> 00:32.900
つまり､ OpenAIとラマを呼び出すコードを書いて､ それを使って私たちが取り組んだユースケースを要約することで､

00:32.900 --> 00:39.710
主要な6つのフロンティアモデルを対比することができます｡

00:39.710 --> 00:44.510
実際､ ゼロワン・プレビューやGPTフォーゼロ､ クロードのアーティファクトなどにも触れているので､

00:44.540 --> 00:47.750
それよりも少し多い｡

00:48.020 --> 00:56.690
特に､ その文の中にAはいくつあるかという質問に､ ほとんど全員が答えられないことがわかっている｡

00:56.690 --> 00:59.570
もちろん､ 彼らが苦労した理由も指摘する価値がある｡ 

00:59.570 --> 01:07.880
このテキストはモデルに送られる時点でトークン化されており､ モデルが知っているのはトークンだけだからだ｡

01:07.880 --> 01:13.730
なぜなら､ すでに組み合わされているトークンしか見ていないし､

01:13.730 --> 01:18.440
トークンには文字の意味はないからだ｡

01:18.440 --> 01:23.640
だから､ LLMにとっては非常に難しい問題なのですが､ ゼロワン・プレビューのように段階を追って考えることができ､

01:23.670 --> 01:30.660
理性的で､ 物事のスペルがどうあるべきかを理解している人ならできるのです｡

01:30.720 --> 01:33.390
うーん､ それから当惑もできたよね｡ 

01:33.390 --> 01:38.430
そして､ それは知識のリソースでそれを調べることができたからだろう｡ 

01:39.000 --> 01:46.890
トランスフォーマーの歴史や､ 現在に至るまでの経緯について理解するために､ このようなことを積み重ねてきたわけですね｡

01:46.920 --> 01:52.470
トークンと､ コンテキスト・ウィンドウをトークン化することの意味､ そしてそれが単なる入力ではないということ｡ 

01:52.470 --> 01:54.540
これまでの会話のすべてだ｡ 

01:54.540 --> 02:04.260
そして､ APIのコストと､ APIのコストと大きなモデルに関連するコンテキスト・ウィンドウを調べるにはどこに行けばいいのかがわかっただろう｡

02:05.160 --> 02:06.240
オーケー｡ 

02:06.270 --> 02:08.790
次の講義はエキサイティングなものになりそうだ｡ 

02:08.790 --> 02:10.560
今回はコーディングすることになる｡ 

02:10.560 --> 02:14.970
OpenAI APIに対するコーディングに自信をつけることになるだろう｡ 

02:15.000 --> 02:23.400
私たちはさまざまなテクニックを駆使し､ よりホールセール的なビジネス・ソリューションを導入することになる｡

02:23.400 --> 02:28.860
LMSに何度か電話して､ 数分で終わらせるつもりだ｡

02:28.860 --> 02:32.610
素晴らしい研究室だし､ 最後には運動もできる｡ 

02:32.610 --> 02:39.960
それではまた明日､ ビッグ・ウィーク第1弾の企画でお会いしましょう｡
