WEBVTT

00:00.200 --> 00:03.650
넷째 날은 정보가 많은 날이었어요

00:03.650 --> 00:09.500
여러분이 여기서 유용한 걸 배우셨길 바랍니다 이미 토큰이나 컨텍스트 같은 것에

00:09.530 --> 00:15.440
익숙한 분들도 Windows가 픽업한 게 한두 개 있다면 그걸 좀 더 자신 있게

00:15.440 --> 00:17.420
실행할 수 있길 바라요

00:17.420 --> 00:23.150
이건 기본 기술이고 앞으로 몇 주 동안 계속 사용할 겁니다 이걸

00:23.150 --> 00:26.870
토대로 상업적 문제에 적용할 거예요

00:26.870 --> 00:32.900
이제 오픈AI와 llama를 호출하는 코드를 작성해서 지금까지 다룬

00:32.900 --> 00:39.710
요약 사용 사례를 요약해 보겠습니다 이제 여섯 가지 프런티어 모델을 비교해 보죠

00:39.710 --> 00:44.510
사실 그 이상이죠 왜냐하면 01 미리보기와 GPT 40에 노출됐거든요

00:44.540 --> 00:47.750
클로드 아티팩트 같은 것도요

00:48.020 --> 00:53.270
특히 이 질문에 답할 수 있는 학생이 거의 없다는 걸 알아요 그

00:53.300 --> 00:56.690
문장에 A가 몇 개나 들어가는지요

00:56.690 --> 00:59.570
왜 그들이 힘들어했는지도 짚고 넘어갈 필요가 있어요

00:59.570 --> 01:05.240
보시면 아시겠지만 모델로 전송될 때마다 텍스트가 토큰화되기 때문입니다

01:05.270 --> 01:07.880
모델은 토큰에 대해서만 알고 있죠

01:07.880 --> 01:13.730
그래서 그런 관점에서 보면 글자를 세는 건 아무 의미가 없어요 이미 합쳐진

01:13.730 --> 01:18.440
토큰들만 보이니까요 글자의 의미가 없는 토큰들이죠

01:18.440 --> 01:23.640
그래서 LLM에 대해 아주 어려운 질문인 겁니다 하지만 01 프리뷰처럼

01:23.670 --> 01:30.660
단계별로 이성과 생각을 할 수 있고 스펠트가 어떻게 필요한지 이해하는 건 가능하죠

01:30.720 --> 01:33.390
당혹스러움도 그 역할을 할 수 있었죠

01:33.390 --> 01:38.430
그건 아마 지식을 통해 찾아볼 수 있었기 때문일 거예요

01:39.000 --> 01:45.750
그 위에 또 다른 건물을 지었잖아요 트랜스포머의 역사와 현재의 모습을 이해하기

01:45.750 --> 01:46.890
위해서요

01:46.920 --> 01:52.470
토큰과 컨텍스트 윈도우를 토큰화하는 것의 의미 그리고 어떻게 입력만 하는 것이 아닌지를요

01:52.470 --> 01:54.540
지금까지의 대화 전부요

01:54.540 --> 02:01.350
이제 API 비용도 알고 API 비용을 어디서 찾을지도 알죠 큰 모델과

02:01.380 --> 02:04.260
관련된 컨텍스트 창도요

02:05.160 --> 02:06.240
네

02:06.270 --> 02:08.790
다음 강의는 정말 재미있을 거예요

02:08.790 --> 02:10.560
이번엔 당신이 코딩을 해요

02:10.560 --> 02:14.970
OpenAI API 코딩에 대한 자신감이 생기게 되죠

02:15.000 --> 02:20.430
다양한 테크닉을 사용할 겁니다 여러분은 비즈니스 솔루션을 구현할 겁니다

02:20.430 --> 02:23.400
도매에 가까운 비즈니스 솔루션이죠

02:23.400 --> 02:28.290
LMS에 대한 몇 가지 다른 호출이 포함될 겁니다 몇 분 안에 get get을 완료하도록

02:28.290 --> 02:28.860
하죠

02:28.860 --> 02:32.610
훌륭한 실험실이고 운동으로 끝날 거예요

02:32.610 --> 02:39.960
그럼 지체 없이 오늘은 이만하고 내일 첫 주 프로젝트로 만나요
