WEBVTT

00:01.430 --> 00:06.980
そして今回は､ このコースの大きなプロジェクトに対してコーディングする初めての機会だ｡ 

00:06.980 --> 00:08.930
Jupyter Labへようこそ｡ 

00:08.930 --> 00:14.390
ビッグプロジェクトに乗り出す第6週のフォルダへようこそ｡ 

00:14.390 --> 00:21.710
というわけで､ 私たちのプロジェクトは､ 商品の説明に基づいて､ その商品の値段を推定できるモデルを構築することである｡

00:21.710 --> 00:26.120
そして今日は､ データ・キュレーションの最初のステップを行う｡ 

00:26.120 --> 00:34.220
まずは､ 家電製品､ 洗濯機などのデータの一部を見てみよう｡ 

00:34.220 --> 00:36.710
それではまず､ データセットそのものをお見せしよう｡ 

00:36.710 --> 00:39.260
データセットはこちらのリンクにある｡ 

00:39.260 --> 00:46.190
これはHugging Face hubのHugging Face datasetsセクションにあるデータセットです｡ 

00:46.580 --> 00:55.520
アマゾンのレビューをスクレイピングしたもので､ 過去にさかのぼります｡ 

00:55.520 --> 00:59.810
しかし､ 20日に行われたこの最新のスクレイプは､ 2023年後半に行われたものだった｡ 

01:00.020 --> 01:07.450
ええと､ 膨大な数のレビューが含まれていますが､ ほぼ5000万件の商品も含まれています｡ 

01:07.450 --> 01:15.040
だから､ いろいろな製品があり､ それぞれのカテゴリーに分かれている｡ 

01:15.250 --> 01:17.410
これらすべてを扱うつもりはない｡ 

01:17.410 --> 01:24.070
この練習のために､ この中から最も興味のあるカテゴリーのサブセットを抜き出す｡

01:24.490 --> 01:28.930
そうでないと､ トレーニングにものすごく時間がかかるし､ 面白くない｡ 

01:28.930 --> 01:34.000
これで､ 私たちが扱っているデータの種類をご理解いただけると思います｡ 

01:34.330 --> 01:41.560
データセットがメタデータと呼んでいる､ 商品､

01:41.590 --> 01:48.400
説明､ 価格そのもののデータだ｡

01:48.430 --> 01:49.270
これだ｡ 

01:49.300 --> 01:59.320
電子機器などのデータセットを見れば､ 5ギガバイト強のサイズであることがよくわかるだろう｡

01:59.320 --> 02:05.110
つまり､ これらは大きなデータセットであり､ 有益な情報をたくさん持っていることになる｡ 

02:05.110 --> 02:08.160
アップロードされたのは7カ月前だ｡ 

02:08.160 --> 02:10.290
だから､ これはすべてごく最近のことなんだ｡ 

02:11.100 --> 02:14.010
さあ､ 始めよう｡ 

02:14.010 --> 02:16.410
まずは輸入品から｡ 

02:16.440 --> 02:19.440
特に複雑なことはない｡ 

02:19.470 --> 02:20.700
今のところはまだだ｡ 

02:20.730 --> 02:22.380
まだまだ続くだろう｡ 

02:22.680 --> 02:24.660
これから環境を整えるんだ｡ 

02:24.690 --> 02:26.550
今日使うわけではない｡ 

02:26.550 --> 02:28.260
ハグする顔を使うだけだ｡ 

02:28.290 --> 02:29.970
ハグ顔にログイン｡ 

02:30.720 --> 02:37.230
そして､ matplotlibがJupyterノートブックにチャートを表示できるようにする｡ 

02:37.380 --> 02:41.070
そこで最初にすることは､ データセットをロードすることだ｡ 

02:41.070 --> 02:45.660
これから行うのは､ データセットの名前､ アマゾンのレビューを指定することだ｡ 

02:45.660 --> 02:49.980
まずは家電製品のカテゴリーから｡ 

02:50.010 --> 02:56.970
冷蔵庫や洗濯機などのような家電製品は､ 最初にHugging faces

02:57.000 --> 03:03.480
load data set機能を使って読み込みます｡

03:03.780 --> 03:13.370
最初にこれを実行すると､ Huggingfaceのハブからダウンロードされる｡

03:13.400 --> 03:20.870
これはすでに完成しており､ どれだけの電化製品があるか見てみることになる｡ 

03:20.990 --> 03:26.510
そこには94,000台の家電製品がある｡ 

03:26.510 --> 03:28.520
では､ そのうちの一人を見てみよう｡ 

03:28.520 --> 03:34.550
データ・ポイントとデータ・セットを見てみよう｡ 

03:35.030 --> 03:36.290
まず1つ目｡ 

03:36.290 --> 03:37.100
なぜだ｡ 

03:37.370 --> 03:38.360
見てみよう｡ 

03:42.590 --> 03:44.210
というわけで､ こんな感じだ｡ 

03:44.210 --> 03:45.950
たくさんの情報がある｡ 

03:45.950 --> 03:49.370
しかし､ 特に特徴というものがあるのがわかるだろう｡ 

03:49.370 --> 03:54.350
タイトルもついているし､ 他にもいくつか役に立つことが書いてある｡ 

03:54.530 --> 04:01.910
そして特に､ タイトル､ 説明､ 特徴､ 詳細､ 価格を持っている｡ 

04:01.940 --> 04:04.970
それぞれをプリントアウトして､ ざっと見てみよう｡ 

04:05.030 --> 04:06.770
これがタイトルだ｡ 

04:06.770 --> 04:09.860
これは製氷機のカウンタートップ｡ 

04:09.890 --> 04:13.280
これは空虚な説明である｡ 

04:13.820 --> 04:16.430
これがその詳細だ｡ 

04:16.460 --> 04:24.830
そのため､ 多くの機能､ ここでの詳細､ そしてすぐに問題が見えてくる価格が特徴だ｡ 

04:24.830 --> 04:26.870
この場合､ 値段は関係ない｡ 

04:26.870 --> 04:30.080
だから､ すべての商品に値段がついているわけではないのは明らかだ｡ 

04:30.560 --> 04:35.780
そして､ その説明はリストの形で表示されることに気づくだろう｡ 

04:35.780 --> 04:41.090
つまり､ Aには特徴があるのに対して､ 詳細は辞書の形で提供される｡ 

04:41.090 --> 04:43.790
しかし､ これは辞書ではない｡ 

04:43.790 --> 04:47.180
JSONを含む文字列である｡ 

04:47.180 --> 04:58.940
つまり､ これはテキストであり､ これを読み込むには､ それをロードしてAを使い､ JSONロード文字列ロードsを使って辞書に変換する必要がある｡

05:00.020 --> 05:01.310
うーん､ わかった｡ 

05:01.310 --> 05:05.060
次は別のデータを見てみよう｡ 

05:05.060 --> 05:12.740
冷蔵庫のエッグホルダーのようだが､ 卵は10個まで入る｡ 

05:13.250 --> 05:15.390
これも値段はない｡ 

05:15.420 --> 05:17.880
そしてこの3本目も値段がない｡ 

05:17.880 --> 05:20.940
真新しい乾燥機のドラムスライドだ｡ 

05:20.940 --> 05:24.720
だからこの時点で､ 私たちは初めて心配することになるかもしれない｡ 

05:24.750 --> 05:27.300
94,000台の家電製品がある｡ 

05:27.300 --> 05:30.000
最初の3つは値段がない｡ 

05:30.000 --> 05:32.340
では､ いくつに値段がついているのか見てみよう｡ 

05:32.340 --> 05:37.800
そのための簡単な方法は､ データ・セットのすべてのデータ・ポイントを繰り返し処理することだ｡ 

05:37.800 --> 05:40.410
そして､ その対価を得る｡ 

05:40.650 --> 05:44.040
それをトライブロックに入れる｡ 

05:44.190 --> 05:50.010
なぜなら､ もしそれがなければ､ 失敗してそのデータポイントをスキップしてしまうからだ｡ 

05:50.010 --> 05:53.220
だから､ 価格がゼロのものも無視する｡ 

05:53.220 --> 05:57.120
だから私たちは､ 価格が数字になっているものだけを見ることにしている｡ 

05:57.120 --> 06:01.140
そして､ その価格がゼロでないことはゼロ以上である｡ 

06:01.170 --> 06:05.490
マイナス価格があるとは思わないが､ もしあったとしてもカウントされないだろう｡ 

06:06.360 --> 06:11.310
だから､ これはこれから調べて､ それを見つけ出そうとしているんだ｡ 

06:11.310 --> 06:12.540
さあ､ 行こう｡ 

06:12.540 --> 06:19.560
つまり､ 726人で､ ほぼ50％ということになる｡ 

06:19.560 --> 06:20.580
だからひどくはない｡ 

06:20.580 --> 06:22.290
それでいい､ それでいい｡ 

06:22.290 --> 06:29.700
一瞬､ 収穫が少ないのではと心配になるかもしれないが､ 少なくとも電化製品に関してはそうではない｡

06:29.970 --> 06:35.010
ええと､ ええと､ データセットで､ 半分が価格を持っている｡ 

06:35.400 --> 06:36.960
小さなサイドポイントだ｡ 

06:36.960 --> 06:39.270
私がいつから数字を印刷しているのか､ お気づきになっただろうか｡ 

06:39.270 --> 06:47.370
一般的に､ コンマで数千を区切っている｡

06:47.370 --> 06:53.340
その方法は､ Pythonのf文字列を使う場合､ 次のようにコロン・コンマと言う｡ 

06:53.460 --> 06:59.850
そうすれば､ このスタイルで数字が表示されます｡ 

07:00.000 --> 07:02.610
ちょっとホットな情報を｡ 

07:02.700 --> 07:06.510
ええと､ すでにご存じかもしれないが､ そうでなければ､ 知っておくと便利なことだ｡ 

07:07.500 --> 07:08.490
オーケー｡ 

07:08.970 --> 07:14.490
だから､ これからやることは､ 値段がついているものをすべて取り上げるということだ｡ 

07:14.850 --> 07:23.370
そして､ タイトル､ 説明､ 特徴､ 詳細の文字数を調べます｡

07:23.370 --> 07:29.130
文字数を合計して､ 長さのリストにするんだ｡ 

07:29.130 --> 07:36.150
だから､ 私たちが今持っているのは価格と長さのリストであり､ 私たちがどれだけの詳細な文字数を持っているかを把握し､

07:36.180 --> 07:42.390
それが均一なものなのか､ それとも何らかの形で偏っているものなのかを確認することができる｡

07:42.450 --> 07:53.250
では､ これからよく使うmatplotlibを使って､ 長さをヒストグラムの形でプロットしてみよう｡

07:53.250 --> 08:06.930
ヒストグラムは基本的に､ すべてのものをバケツに入れて各ビンにいくつ入っているかを示すものだ｡

08:06.960 --> 08:09.060
それがどんなものかをお見せするのは簡単だ｡ 

08:09.060 --> 08:10.170
こんな感じだ｡ 

08:10.170 --> 08:18.330
つまり､ X軸に沿って､ さまざまな家電製品､ さまざまな洗濯機などの長さを､

08:18.330 --> 08:22.830
その説明文の文字数で表している｡

08:22.830 --> 08:29.240
そしてこれは､ それだけのキャラクターを持つ電化製品がいくつあるかという数である｡ 

08:29.240 --> 08:38.960
そして､ このあたりにピークがあるのがわかるだろう｡

08:38.960 --> 08:48.650
というのも､ 最終的には自分たちのオープンソースモデルを使ってトレーニングしたいからだ｡

08:48.650 --> 08:55.730
そして､ 理解することが非常に重要な制約の1つは､ 各ポイントでモデルに渡す可能性のある最大文字数､

08:55.730 --> 09:01.490
つまり最大トークン数です｡

09:01.490 --> 09:06.770
そして､ 各トレーニングポイントに渡す必要があるトークンの数が多ければ多いほど､ トレーニングに必要なメモリが増え､

09:06.770 --> 09:09.950
達成するのが難しくなる｡

09:09.980 --> 09:14.990
もう一つのポイントは､ フロンティア・モデルを使う場合でも､

09:14.990 --> 09:24.710
そのような問題はないものの､ 別の問題があるということです｡

09:24.830 --> 09:31.490
しかし､ これを怒りにまかせて多数の製品で行おうとすれば､ 数字がどんどん増えていくことになる｡

09:31.520 --> 09:37.430
だから理想的なのは､ カットオフを決めて､ その時点でデータを制約することだ｡ 

09:37.550 --> 09:40.820
それは後で考えることにしよう｡ 

09:41.120 --> 09:45.200
もうひとつ注目すべきは､ 価格の分布だ｡ 

09:45.200 --> 09:47.270
では､ いくらかかるのか？

09:47.300 --> 09:53.150
家電製品は冷蔵庫や洗濯機などでいっぱいになると思っていたのに､ 私たちが見たものはむしろ小さかったということは､

09:53.150 --> 09:59.900
先の分析でお分かりいただけたかもしれない｡

09:59.900 --> 10:05.810
エッグホルダーや製氷機などである｡ 考えてみれば､ データにはおそらく､

10:05.810 --> 10:12.860
高価格帯の商品を押しのけるような安価なものが非常に多く含まれるはずで､

10:13.010 --> 10:16.730
それほど驚くことではない｡

10:16.730 --> 10:17.660
では､ それを見てみよう｡ 

10:17.690 --> 10:19.100
どう見えるか見てみよう｡ 

10:20.360 --> 10:22.850
まあ､ それは事実のようだ｡ 

10:23.120 --> 10:29.280
つまり､ このデータセットの平均価格は6ドルということになる｡ 

10:29.310 --> 10:33.360
最高額は21,000ドル｡ 

10:33.390 --> 10:43.140
このリストの中には21,000ドルの家電製品もあるが､ それ以下の価格のものも非常に多いことがわかるだろう｡

10:43.470 --> 10:48.150
そして､ 平均値､ 中央値､ 最頻値の違いを覚えている人のために｡ 

10:48.180 --> 10:54.930
学校の統計学で習ったことだが､ これは平均値が高価なものによって引き上げられ､

10:54.930 --> 11:01.710
最頻値や中央値よりも明らかに大きくなるといういい例だ｡

11:02.730 --> 11:12.690
そうですね､ 確かに､ 安い商品が非常に多いという偏った分布になっていますね｡

11:12.690 --> 11:22.140
トレーニングの間､ トレーニングデータが低価格のアイテムに埋もれてしまうからだ｡

11:22.440 --> 11:26.880
この超高価なものが何なのか､ ちょっと見てみよう｡ 

11:26.880 --> 11:30.630
この2万1,000ドルの品だ｡ 

11:30.630 --> 11:35.300
データセットを調べて､ 21,000ドル以上するものは何でも抜き出す｡ 

11:35.300 --> 11:38.000
どうやらターボチェフ弾のようだ｡ 

11:38.000 --> 11:41.300
ラピッド・クック電気電子レンジ・コンベクション・オーブン｡ 

11:41.330 --> 11:46.940
今､ もし誰かが私にそのような説明をしたとしたら､ それが21,000ドルもするとは思わなかっただろう｡

11:47.300 --> 11:53.870
同じものではないが､ おそらく今アマゾンで最新版だと思われるものを見つけた｡

11:53.960 --> 12:00.590
これもターボシェフ製だ｡ 

12:00.590 --> 12:05.420
21,000ドルどころか､ わずか18,000ドルのバーゲンプライスだ｡ 

12:05.900 --> 12:07.640
でも､ 私はあなたのことを知らない｡ 

12:07.640 --> 12:12.440
電子レンジがこんなに高いとは知らなかったが､ 明らかにプロ仕様の電子レンジで､

12:12.470 --> 12:17.090
非常に高級な電子レンジだ｡

12:17.090 --> 12:27.110
その2万1,000ドルバージョンは､ 私たちのデータでは､ こちらのどこかにある｡ 

12:28.340 --> 12:35.120
次のビデオでは､ それをやってみよう｡ 
