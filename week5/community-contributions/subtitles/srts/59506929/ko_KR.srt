WEBVTT

00:00.890 --> 00:04.820
포옹 페이스 오픈 LLM 리더보드로 돌아가죠

00:04.820 --> 00:09.650
훈련용 기본 모델을 고를 때 가장 먼저 가는 곳이죠

00:09.650 --> 00:17.690
가장 먼저 모델에 초점을 맞추겠습니다 매개 변수 크기, 최대 9라고

00:17.690 --> 00:18.740
해보죠

00:18.740 --> 00:26.690
비트만 조금 낮추고 기본 모델만 빼고 전부 걸러내죠

00:26.810 --> 00:30.950
매개 변수의 개수도 보여 주세요

00:30.950 --> 00:34.190
이 아래 테이블로 가죠

00:34.280 --> 00:39.890
다양한 지표에 따라 가장 강력한 모델의 결과가 나왔어요

00:39.950 --> 00:48.890
제가 여러 번 언급했던 파워하우스 클랜이 보일 겁니다 이 2를 포함해서요 52살요 5는 새로운 거예요

00:48.890 --> 00:51.710
70억 개 변종요

00:51.710 --> 00:56.600
이 로드의 최고 득점자였어요

00:56.840 --> 01:02.760
보시다시피 제마는 90억 개의 변수 제마예요

01:02.790 --> 01:07.650
미스트랄강 바로 위예요

01:07.950 --> 01:12.780
우리가 자주 얘기하는 그 사람도요

01:12.810 --> 01:17.010
마이크로소프트의 phi2도 있었죠

01:17.010 --> 01:20.700
라마 3요 1번 비트는 좀 더 아래예요

01:20.850 --> 01:26.340
먼저 언급할 점은 상위권의 숫자는 꽤 근소한 차이라는 겁니다 약간의

01:26.340 --> 01:28.380
차이가 있긴 하지만요

01:28.500 --> 01:33.030
라마 3에 대해 말씀드릴 게 있어요 여러분이 고르게 될 1은 우리가 선택하게

01:33.030 --> 01:36.510
될 겁니다 코드에서 더 아래쪽에 있는 걸 보셨으니까요

01:36.660 --> 01:39.570
하지만 거기엔 이유가 있어요

01:39.630 --> 01:45.960
이런 다양한 스코어를 볼 때 훈련된 버전도 반드시

01:45.960 --> 01:49.170
가져와야 해요

01:49.260 --> 01:56.850
다중 분석이라고 부르는 건 같은 모델이지만 좀 더 훈련된 형태로 다양한 강화

01:56.920 --> 02:03.880
학습 기술을 이용해 특정 채팅 분석 스타일에 대응하는 거예요

02:03.880 --> 02:10.210
그런 프레임워크가 갖춰지면 다양한 테스트에 더 잘 대처할 수

02:10.210 --> 02:16.840
있어요 주어진 지시에 반응할 테니까요 다른 작업에 적응하도록 훈련받기를

02:16.840 --> 02:19.870
기대하기보다요

02:20.050 --> 02:26.080
이 기능을 좀 더 현실적으로 볼 수 있습니다 기본 모델이라도 벤치마크를

02:26.080 --> 02:32.170
가지고 어떻게 작동하는지 보고 변형된 부분을 보면 이해가 되죠

02:32.170 --> 02:38.590
그렇게 하면 라마다 3이 나오죠 제일 위에 있는 게 180억 개예요

02:38.620 --> 02:42.880
그 위에는 피3도 있어요

02:42.880 --> 02:43.870
재머요

02:43.870 --> 02:48.040
메타 라마 3도 있어요 1번요

02:48.040 --> 02:52.030
그러니까 instructant varian을 보면 아주 잘 되고 있어요

02:52.060 --> 02:58.160
좀 삐딱하게 들리겠지만 실제로 지시 기능을 쓰자는 건 아니에요

02:58.160 --> 03:03.320
기본 형태를 유지하자는 거예요 그건 원하지 않으니까요

03:03.350 --> 03:08.600
사고 과정이나 훈련 능력을 모두 소모하는 건 원치 않아요 시스템 프롬프트나

03:08.600 --> 03:12.620
사용자 프롬프트 같은 걸 배우는 것도요

03:12.620 --> 03:16.940
일단 그 훈련을 하고 나면 모든 점수가 잘 나온다는 걸 알

03:16.970 --> 03:17.900
수 있어요

03:17.900 --> 03:24.290
이런 걸 보면 기본 모델이 이런 다양한 벤치마크를 해결하도록 잘 적응했다는 느낌이

03:24.290 --> 03:25.160
들죠

03:25.160 --> 03:29.570
순위표 결과를 해석하는 미묘한 방법이네요

03:29.570 --> 03:34.580
변형 기능을 보고 어떻게 작동하는지 볼 수 있어요 베이스 모델이

03:34.580 --> 03:37.400
어떻게 작동할지 알 수 있죠

03:38.030 --> 03:46.010
라마를 고른 데는 또 다른 미묘한 이유가 있어요 물론 167cm인

03:46.010 --> 03:54.740
제마나 그웬이 여러 면에서 더 높아 보일 수도 있지만요

03:54.740 --> 03:57.350
라마는 편리해요

03:57.350 --> 03:57.980
정말 그래요

03:58.010 --> 04:03.350
모든 것에 약간의 차이는 주지만 코드를 좀 더 간단하게 만들고

04:03.350 --> 04:10.040
라마를 위한 작업을 좀 더 쉽게 해줍니다 라마를 위한 토큰라이저를 보면

04:10.040 --> 04:19.760
0부터 999까지의 모든 숫자, 모든 세 자리 숫자가 하나의 토큰에 매핑되어 있는 것을 볼 수 있죠

04:19.790 --> 04:25.280
3, 4편이나 콴의 경우는 달라요

04:25.460 --> 04:30.950
이 세 모델 모두 기본적으로∙∙∙ 숫자당 토큰 같은 걸 갖고 있다고 생각하면

04:30.980 --> 04:31.670
돼요

04:31.670 --> 04:36.110
999는 결국 세 개의 패가 되죠

04:36.140 --> 04:40.940
이런 질문을 하실지도 모르겠네요. 그게 무슨 차이가 있죠? 전혀 차이가 없어야 하는 걸까요?

04:41.210 --> 04:50.150
그래서 우리가 트레이닝을 할 때 토큰을 생성하기 위해 모델을 사용합니다 회귀

04:50.150 --> 04:54.990
모델의 관점에서 생각하도록 하는 거죠

04:54.990 --> 05:00.660
문제를 해결하고 다음 토큰을 더 잘 예측할 수 있도록 하는 거죠 가격에도

05:00.660 --> 05:02.610
적용되도록요

05:02.610 --> 05:04.830
문제를 단순화하죠

05:04.830 --> 05:10.740
모델이 생성해야 하는 토큰 하나에 가격이 정확히 반영된다면요

05:10.740 --> 05:16.980
이 특정한 상황에서 우리가 해결하려는 특정한 문제에

05:16.980 --> 05:25.950
대해 람다 3의 토큰화 전략이에요 1은 아주 잘 작동합니다. 생성되는 단일 다음 토큰이 가격에

05:25.950 --> 05:28.950
대한 모든 것을 반영하기 때문이죠

05:29.130 --> 05:35.460
다음 토큰은 9가 되어야 한다고 예측하는 경우는 없어요 9달러,

05:35.460 --> 05:39.660
999달러 혹은 999달러일 수도 있죠

05:39.660 --> 05:42.000
토큰을 써야만 효과가 나타나죠

05:42.000 --> 05:42.690
그다음에요

05:42.720 --> 05:49.710
아뇨, 다음 토큰으로 제시되는 하나의 토큰이 하나의 토큰에 제품의

05:49.720 --> 05:53.890
전체 가격을 반영하는 경우죠

05:54.340 --> 06:02.290
그런 미묘한 차이 때문에 llama 3을 선택하는 거예요 이 경우엔 1명이죠

06:02.650 --> 06:07.690
하지만 다른 모델을 선택해서 성능을 확인할 수 있어요

06:07.690 --> 06:13.510
라마는 비트 마이너스가 편리해서 좀 더 유리해요

06:13.900 --> 06:19.360
이로써 여러분은 모델을 선택하는 과정과 leaderboard를

06:19.360 --> 06:23.770
보고, 더 깊이 살펴볼 수 있습니다. 기본 모델의 매개 변수

06:23.800 --> 06:29.920
크기와 비교해 다양성을 만드는 과정과 토큰화의 작동 방식에 대한 뉘앙스를 볼

06:29.920 --> 06:31.420
수 있죠.

06:31.420 --> 06:38.810
그 모든 걸 종합한 결과 이런 결정을 내릴 수 있었습니다 llama 3을 선택하기로요 180억 달러를 기본

06:38.810 --> 06:42.190
모델로 삼았어요

06:42.280 --> 06:48.640
이제 Colab으로 가서 기본 모델을 시도해 보죠
