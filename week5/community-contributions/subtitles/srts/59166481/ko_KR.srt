WEBVTT

00:00.860 --> 00:05.330
우리가 좋아하는 장소에 다시 모였네요 주피터 연구소

00:05.330 --> 00:07.310
몇 주 준비됐죠

00:07.340 --> 00:09.620
둘째 주에는 운동하고요

00:09.620 --> 00:14.930
2주 차 폴더로 가서 2주 차 첫날을 열어보죠

00:15.230 --> 00:18.230
자, 시작할게요

00:18.230 --> 00:26.990
첫째 주에 다중 프런티어 LMS를 사용했죠 채팅방 사용자 인터페이스를 통해서요 웹을 통한

00:26.990 --> 00:32.990
사용법이죠 API를 통해 OpenAI API에 연결했어요

00:33.020 --> 00:39.890
그래서 오늘은 안트로픽과 구글의 API를 통합해 오픈AI 사용 기술에 추가할

00:39.890 --> 00:41.090
거예요

00:41.960 --> 00:47.630
다시 한번 말씀드리지만 계속 그 얘기 하면 절 죽이실 거잖아요

00:47.630 --> 00:50.300
여기에 열쇠를 꽂아두죠

00:50.300 --> 00:55.850
오픈라이 키를 설정할 수 있죠 아마 지난주에 이미 했겠죠 인류애

00:55.850 --> 00:58.460
키와 구글 제미니 키를요

00:58.490 --> 01:05.330
하지만 구글 키를 설정하는 데 더 많은 모험이 있다는 걸 명심하세요

01:05.390 --> 01:09.410
설정이 끝나면 창조하는 거죠

01:09.470 --> 01:11.330
파일을 생성했어야 해요

01:11.480 --> 01:15.170
그 형태로 열쇠가 있는지 확인하세요

01:15.560 --> 01:21.500
그렇게 하는 대신 이 셀에서 키를 입력하면 돼요

01:21.500 --> 01:24.020
그렇게 할 수 있어요

01:24.020 --> 01:26.270
보안상 권장할 수 없는 일이죠

01:26.270 --> 01:30.350
언젠가 이걸 공개해서 다른 사람들이 열쇠를 보게 되면요

01:30.380 --> 01:32.300
서론은 그만하죠

01:32.330 --> 01:33.800
수입품 검사를 해 보죠

01:33.800 --> 01:37.400
환경 변수를 설정하는 코드 블록을 실행해보죠

01:37.400 --> 01:38.900
잘 아시네요

01:38.900 --> 01:46.280
이 셀에서 오픈AI API 연결을 설정하기 위해 OpenAI에

01:46.280 --> 01:49.400
같은 전화를 걸었어요

01:49.400 --> 01:55.790
클로드 비트도 비슷하고 제미니 비트는 구글에서 약간 다르게

01:55.790 --> 01:56.840
만들었죠

01:56.960 --> 02:04.220
이런 식으로 반이나 어느 정도 비슷한 명령을 이 세 가지에 사용하고 있어요

02:04.730 --> 02:05.510
네

02:05.510 --> 02:11.420
Lms가 잘하는 것들을 많이 보았고 몇 가지 실행되는 것들을 보았습니다 하지만 대부분은 Lms가

02:11.420 --> 02:13.160
잘하는 것들이죠

02:13.190 --> 02:17.600
한 가지 잘 안 되는 건 농담을 하는 거예요

02:17.600 --> 02:24.080
아주 딱 맞는 문맥을 주면 그 안에서 농담을 구성해야 해요

02:24.260 --> 02:28.610
이건 상업적인 예는 아니지만 재미를 위한 방법이고

02:28.610 --> 02:30.980
API로 경험을 쌓는 거죠

02:31.040 --> 02:34.850
API 상에서 농담을 해 줄 llms를 모실 거예요

02:35.120 --> 02:36.770
어떤 정보를 제공하죠?

02:36.770 --> 02:37.550
API 하나를 보내죠

02:37.580 --> 02:41.750
일반적으로 사용하고 싶은 모델의 이름을 항상 지정해요

02:41.750 --> 02:45.380
시스템 메시지와 사용자 메시지를 주로 제공하죠

02:45.380 --> 02:48.950
전반적인 컨텍스트를 제공하는 시스템 메시지에 아주 익숙하죠

02:48.950 --> 02:52.340
사용자 메시지가 실제 프롬프트죠

02:52.550 --> 02:54.410
다른 특징도 있어요

02:54.410 --> 02:55.700
다른 방법도 있어요

02:55.730 --> 03:00.890
온도라는 걸 통과시킬 수 있어요 0에서 1 사이죠 보통 좀 더 무작위적인

03:00.890 --> 03:09.960
창의적 출력 출력을 뜻합니다 0은 가장 가능성이 낮은 집중된 결정론적 반복 가능 설정이고요

03:10.320 --> 03:14.250
여러분이 제공할 수 있는 또 다른 매개 변수죠

03:14.280 --> 03:19.470
이 경우에 시스템 메시지를 설정하겠습니다 농담을 잘 하는 비서라고 하는

03:19.470 --> 03:20.010
거죠

03:20.010 --> 03:26.670
데이터 과학자들을 위해 가벼운 농담을 할 거예요

03:26.670 --> 03:30.000
당신과 내가 되겠죠

03:30.660 --> 03:35.850
여기 이 구조는 여러분에게 아주 익숙하길 바라요

03:35.850 --> 03:43.410
여기서 프롬프트들을 목록에 넣습니다. 요소들, 시스템, 사용자를 역할로 for each each each each each요.

03:43.410 --> 03:44.910
이 두 요소에서요.

03:44.940 --> 03:49.860
이 리스트를 살펴보면, 설명하지 않아도 될 것 같네요. 이제 익숙해졌으니까요.

03:50.040 --> 03:55.080
말씀드렸듯이 이 값은 여기 이 역할이 시스템 또는 유저가 될 수 있어요

03:55.080 --> 03:56.070
곧 알게 될 거예요

03:56.070 --> 03:57.570
조수라고도 할 수 있죠

03:57.570 --> 03:59.760
시스템 사용자나 보조가 될 수 있죠

03:59.760 --> 04:04.150
그리고 이번 주 후반에는 다른 것도 넣을 수 있을 거예요

04:04.240 --> 04:04.990
그래서요?

04:05.020 --> 04:09.790
하지만 지금은 시스템과 사용자를 우리가 사용할 두 역할로 기억해야 해요

04:09.790 --> 04:12.610
그래서 그걸 Put 프롬프트 목록에 넣었어요

04:13.480 --> 04:16.570
그 전에 감옥을 처리해야 하고요

04:16.570 --> 04:18.790
그 전에 이 감방을 실행했어요

04:18.790 --> 04:20.350
네, 괜찮았어요

04:20.350 --> 04:20.770
시작할게요

04:20.800 --> 04:21.790
다시 해 보죠

04:21.790 --> 04:22.840
저 감방을 처형해요

04:22.840 --> 04:23.860
이 감방을 실행해요

04:23.890 --> 04:25.720
좋아요

04:25.750 --> 04:33.280
오래된 GPT 모델부터 살펴보죠 GPT 3 5 터보 엔진인데 최근에 나온 최고의 개척

04:33.280 --> 04:34.390
모델이죠

04:34.390 --> 04:35.830
하지만 이미 지난 일이에요

04:35.830 --> 04:37.330
하지만 이걸 쓸 거예요

04:37.330 --> 04:44.680
오픈AI에서 이제 익숙해진 API는 OpenAI.챗, .완성 .Creetions,

04:44.680 --> 04:53.500
.Create 완성 이 API의 이름이죠 기존 프롬프트 모음을 가져다가 대화를 완성하기

04:53.500 --> 04:59.530
위해 텍스트 생성을 시도하는 거예요

04:59.800 --> 05:07.980
생성을 통해 모델과 메시지를 전달합니다 여러분이 익숙한 형식으로 전달하죠

05:08.010 --> 05:09.750
어디 보죠

05:09.780 --> 05:15.870
응답을 받았을 때 우리가 하는 건 완료 .선택입니다 가능한

05:15.870 --> 05:18.030
선택 목록이죠

05:18.030 --> 05:19.980
하지만 한 가지 요소만 넣을 거예요

05:19.980 --> 05:23.790
여러 개의 선택을 반환하도록 지정할 수 있는 방법이 있어요

05:23.790 --> 05:28.740
하지만 아직 안 했으니 get get을 하면 당연히 0위치에 있죠

05:28.740 --> 05:35.550
완료 .선택 .0.Message는 메시지를 반환하고 콘텐츠는 문자열로 반환하죠

05:35.760 --> 05:37.770
그걸 get 해 프린트하는 거죠

05:37.770 --> 05:39.360
어떤 장난인지 볼까요?

05:39.360 --> 05:42.690
데이터 과학자인 GPT 3을 위해서요 5 터보로 할 수 있어요

05:42.720 --> 05:43.680
시작할게요

05:44.010 --> 05:48.000
왜 데이터 과학자들이 컴퓨터와 분리했을까요?

05:48.000 --> 05:52.020
둘의 복잡한 관계를 감당하지 못했죠

05:52.830 --> 05:53.970
알았어요

05:54.000 --> 05:56.250
Get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it.

05:56.280 --> 05:58.770
세상에서 가장 웃긴 농담은 아니지만 끔찍하지도 않아요

05:58.800 --> 06:03.540
데이터 과학자들은 사물의 관계를 모델로 삼지만 그 복잡한 관계를 다룰

06:03.540 --> 06:04.200
수 없어요

06:04.200 --> 06:04.800
좋아요

06:04.800 --> 06:13.140
GPT 3에서 나온 농담치고는 아주 괜찮은데요 5 터보요

06:13.200 --> 06:17.010
GPT 4 미니는 더 잘할 수 있을까요?

06:17.160 --> 06:21.450
이번엔 API 사용을 살짝 확장할게요

06:21.600 --> 06:26.340
온도도 포함해서 0에서 1 사이의 숫자를 입력할 수 있어요

06:26.340 --> 06:29.220
창의성은 1점, 최저점은 0점이죠

06:29.490 --> 06:34.980
이 중엔 완료 선택지와 메시지 콘텐츠도 없어요

06:34.980 --> 06:36.720
이것도 아주 익숙할 거예요

06:36.750 --> 06:38.970
잘 달리는지 보죠

06:39.570 --> 06:42.060
데이터 과학자가 왜 통계학자랑 헤어졌죠?

06:42.060 --> 06:44.670
너무 못됐다고 생각했거든요

06:44.700 --> 06:46.230
꽤 괜찮은 농담이네요

06:46.230 --> 06:47.490
괜찮은 것 같아요

06:47.490 --> 06:49.950
그 정도는 괜찮은 농담이죠

06:49.980 --> 06:54.990
llm은 이런 농담 잘 못한다고 한 게 너무 심했나 봐요 그 정도면 괜찮은 농담인데요

06:55.170 --> 07:02.610
GPT 4에 작은 박수를 보내 줘야 할 것 같네요

07:03.030 --> 07:09.160
GPT 4 미니와 더 큰 사촌인 GPT 4를 써 보죠

07:09.190 --> 07:12.130
GPT 4의 맥시 버전이네요

07:12.160 --> 07:14.260
덩치 큰 친구요

07:14.260 --> 07:16.000
우리가 물어볼 거예요

07:16.030 --> 07:19.210
온도는 똑같이 유지해야 해요 그래야 실수가 없죠

07:19.240 --> 07:21.160
농담 삼아 물어보죠

07:21.190 --> 07:23.230
둘, 어떻게 되나 보죠

07:24.250 --> 07:27.130
데이터 과학자가 왜 파산했죠?

07:27.130 --> 07:30.850
어레이에서 캐시를 못 찾았거든요

07:32.410 --> 07:35.560
Get up이 아니었다면 더 나았을 거예요

07:35.560 --> 07:38.650
캐시는 못 찾았어요

07:38.650 --> 07:39.910
괜찮을 거예요

07:40.810 --> 07:42.280
내가 뭘 놓쳤나 봐요

07:42.310 --> 07:45.280
Get me get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get it, I'm get'm it, I'm it'm it'm get'm it, I'm it'm it'm get'm it'm it'm it'm it're

07:45.550 --> 07:47.380
다른 걸 해 보죠

07:47.560 --> 07:52.480
아까 했던 대로 비트 온도를 낮춰서 어떻게 되는지 보죠 get it

07:52.990 --> 07:56.560
왜 과학자들은 물류 회귀 모델과 헤어졌을까요?

07:56.590 --> 07:58.390
맞는 걸 못 찾았거든요

07:58.600 --> 08:00.130
괜찮은 생각이네요

08:00.130 --> 08:00.970
그건 괜찮아요

08:00.970 --> 08:06.160
미니와 맥시 중에 뭐가 더 좋은지 모르겠지만 꽤

08:06.160 --> 08:08.860
튼튼한 개그 소재예요

08:08.860 --> 08:12.640
이건 확실히 통과라고 할 수 있겠네요

08:13.810 --> 08:14.800
좋아요

08:14.830 --> 08:17.050
3번 조항으로 넘어가죠 5분

08:17.080 --> 08:17.680
소네트요

08:17.950 --> 08:21.430
API가 눈에 띄게 비슷하죠

08:21.430 --> 08:22.270
좋은 소식이죠

08:22.270 --> 08:25.030
기본적으로 아주 비슷해요

08:25.060 --> 08:26.530
차이점이 몇 가지 있어요

08:26.530 --> 08:31.510
시스템 메시지를 개별 특성으로 전달해야 해요

08:31.510 --> 08:36.430
메시지는 다시 이 데크 목록이에요

08:36.430 --> 08:42.550
물론 시스템 메시지의 첫 항목은 없어요 이미 별도로 넘겼으니까요

08:42.910 --> 08:45.310
그건 미묘한 차이죠

08:45.340 --> 08:52.360
최대 토큰은 오픈AI API 선택 사항으로 최대 토큰의 수를 지정하는 데 사용되죠

08:52.360 --> 08:55.180
클로드도 그래야 할 거예요

08:55.180 --> 08:56.860
그래서 여기 들어있군요

08:56.860 --> 08:59.200
하지만 그 외에는 전부 비슷해 보여야 해요

08:59.230 --> 09:03.250
API 자체는 외우기가 좀 더 쉬워요 비트

09:03.250 --> 09:05.740
클로드 점 메시지 점 만들기예요

09:05.740 --> 09:11.470
약간 짧지만 오픈AI 챗GPT 완성본과 상당히 비슷해요

09:11.710 --> 09:13.150
저기 있네요

09:13.180 --> 09:17.830
응답이 오면 메시지 콘텐츠 get이 0이 되죠

09:17.860 --> 09:22.630
첫 번째 것만 요청하는데 하나만 나올 거예요 왜냐하면 .text만

09:22.630 --> 09:28.750
요청했거든요 OpenAI의 .content와 같은 거죠

09:28.780 --> 09:30.100
어디 보죠

09:30.100 --> 09:35.020
클로드의 API 프레임워크에 유용하면 좋겠네요

09:35.020 --> 09:38.080
클로드가 농담을 어떻게 하는지 보죠

09:39.910 --> 09:40.630
네

09:40.660 --> 09:43.540
데이터 과학자들을 위한 가벼운 농담이 있어요

09:43.570 --> 09:46.210
데이터 과학자들은 왜 사랑하는 사람과 헤어질까요?

09:46.240 --> 09:51.310
관계에 너무 많은 변화가 있었고 그걸 정상화할 좋은 방법을 찾지 못했어요

09:51.970 --> 09:53.530
네, 괜찮아요

09:53.530 --> 09:59.110
좀 더 너디스러운 것 같아요 데이터 과학에 가깝죠

09:59.110 --> 10:03.640
비트만 조금 less지만 나쁘지 않아요

10:03.640 --> 10:07.570
글쎄요, GPT 4보다 그게 더 좋은지는 취향의 문제겠죠

10:07.900 --> 10:10.100
완벽한 농담이죠

10:10.220 --> 10:14.210
폭발할 만큼 웃기진 않지만 아주 튼튼해요

10:14.210 --> 10:15.440
나쁘지 않아요

10:15.950 --> 10:16.550
네

10:16.610 --> 10:22.220
어쨌든 핵심은 API와 농담에 관한 겁니다 늘 재미있긴 하지만요

10:22.250 --> 10:24.800
지금부터 보여드릴 건 스트리밍에 관한 거예요

10:24.890 --> 10:29.090
스트리밍 예시를 보기 전에 스트리밍에 대해 잠깐 얘기했었죠?

10:29.090 --> 10:33.140
전에 했던 건 좀 복잡해 보였어요 비트코인 가격 인하를 다시

10:33.140 --> 10:36.470
해야 하고 그 가격 인하에 대처해야 했으니까요

10:36.470 --> 10:40.280
마크다운 비트를 다루는 게 아니라서 더 간단해 보이죠

10:40.280 --> 10:46.730
같은 모델인 클라우드 3에 질문할게요 다시 5가 나왔네요 이번엔 결과를 스트리밍할게요

10:46.730 --> 10:53.090
오픈AI에 스트림하라고 요청했을 때 다른 특성 스트림이 true인 것을 추가한 것을

10:53.090 --> 10:54.470
기억하시나요?

10:54.470 --> 10:56.570
그건 스트리밍 모드였다는 뜻이죠

10:56.570 --> 10:58.490
클로드는 조금 다르죠

10:58.490 --> 11:00.380
추가 속성은 없어요

11:00.380 --> 11:06.440
.Stream 메서드를 호출해요 .Create 메서드 대신에요

11:06.440 --> 11:09.020
접근법이 약간 달라요

11:09.020 --> 11:13.790
인도적인 것과 오픈AI의 스트리밍은 차이가 있어요

11:13.790 --> 11:16.430
클로드 메시지의 흐름이라고 부르기로 했어요

11:16.460 --> 11:17.840
그 외에는 똑같아요

11:17.840 --> 11:22.430
돌아온 결과로는 스트림으로서의 결과를 가진 컨텍스트 관리자를 사용하죠

11:22.610 --> 11:26.960
스트림 텍스트 스트림의 텍스트죠

11:26.960 --> 11:31.550
오픈아이는 그에 대한 답장이었죠

11:31.550 --> 11:35.990
오픈AI는 비트 백 결과를 읽는 방식이 조금 달랐어요

11:35.990 --> 11:37.040
하지만 저기 있네요

11:37.040 --> 11:41.420
각각의 덩어리를 get 해 프린트하죠

11:41.540 --> 11:46.460
이렇게 하는 이유는 한 줄에 한 덩어리가 찍히지 않도록 하기 위해서죠

11:46.670 --> 11:48.170
안 그러면 읽기 힘들었을 거예요

11:48.170 --> 11:49.490
이게 더 보기 좋을 거예요

11:49.490 --> 11:56.510
클로드 3을 보죠 농담과 함께 소네트 5개가 유피터랩에서 스트리밍될 거예요

11:57.200 --> 11:57.800
됐어요

11:57.800 --> 11:58.040
봤죠?

11:58.040 --> 11:59.060
스트리밍이에요

11:59.330 --> 12:01.580
데이터 사이언스에게는 가벼운 농담이 있죠

12:01.610 --> 12:03.110
왜 똑같은 농담을 해요?

12:03.110 --> 12:08.690
똑같은 농담 같지만 브람스 드럼에 추가된 거죠

12:08.840 --> 12:12.000
마지막에 폭발하는 게 좋았어요

12:12.000 --> 12:14.670
왜 전보다 패를 더 많이 달라고 했을까요?

12:14.700 --> 12:15.180
어디 보죠

12:15.210 --> 12:15.630
아뇨

12:15.630 --> 12:16.350
똑같아요

12:16.650 --> 12:17.730
네

12:17.760 --> 12:19.020
설명이 나오죠

12:19.020 --> 12:22.170
이 농담은 데이터 과학에서 흔한 통계 개념을 이용한 거예요

12:22.260 --> 12:27.060
좀 따분하지만 데이터에 밝은 관객들은 웃을 거예요 Get it

12:27.060 --> 12:32.070
데이터에 밝은 분들이니 판단해 주실 수 있겠죠

12:32.100 --> 12:34.440
Get it, Get it, Get it, Get it, Get, Get, Get, Get, Get 웃었어요?

12:35.220 --> 12:36.540
넘어가죠

12:36.570 --> 12:39.120
제미니는 구조가 달라요

12:39.120 --> 12:41.370
사실 비트는 좀 달라요

12:41.400 --> 12:48.780
구글 크레딧은 토큰을 설정하는 기능이 훨씬 복잡하지만 API 설정은

12:48.780 --> 12:50.580
좀 더 간단해요

12:50.670 --> 12:56.850
여기 보이는 것처럼 모델 객체를 생성해서 모델 이름을 입력해요 제미니 1호를

12:56.850 --> 12:59.550
쓸 거예요 5번 섬광이에요

12:59.580 --> 13:03.510
제미니 1호의 경우 얼마나 큰 문맥 창문이 필요했는지 기억하시죠? 5번 섬광이에요

13:03.540 --> 13:04.680
기억할 수 있겠어요?

13:04.710 --> 13:07.050
전에 있던 테이블 위였나요?

13:07.050 --> 13:10.380
놀랍게도 백만 토큰이었죠

13:10.410 --> 13:11.450
백만 토큰요

13:11.480 --> 13:13.310
750,000단어요

13:13.340 --> 13:15.500
제미니 1호예요 5번 섬광이에요

13:15.950 --> 13:23.270
이 객체를 만들 때 시스템 지시를 전달해요 제미니 닷이라고 부르죠

13:23.270 --> 13:26.420
사용자 프롬프트에서 콘텐츠를 생성하세요

13:26.420 --> 13:28.520
응답 .text예요

13:28.520 --> 13:35.090
요청과 응답 둘 다에서 좀 덜 빈둥거리죠 API가 좀 더 간단해요 하지만

13:35.120 --> 13:37.520
비트의 질을 보죠

13:37.670 --> 13:42.200
중요한 건, 데이터 과학자들이 왜 통계 전문가와 헤어졌느냐죠

13:42.200 --> 13:45.590
P 값에 대한 견해가 달랐기 때문이죠

13:47.420 --> 13:48.020
네

13:48.800 --> 13:52.310
전 데이터 과학 쪽을 봐요

13:52.310 --> 13:53.810
Get you get. 잘 모르겠어요

13:53.900 --> 13:55.070
네

13:55.370 --> 13:57.380
Get it, get it, get it! 이해하실지도 모르겠네요

13:57.380 --> 13:59.540
전 졸린 것 같아요

13:59.540 --> 14:01.310
그런 경우라면 어떻게든 절 가리키겠죠

14:01.310 --> 14:05.450
근데 그 농담의 재밌는 면을 잘 모르겠어요 Get it

14:05.450 --> 14:13.440
그래서 제 생각에는 제미니 1호가 확실히 뒤처졌다고 봐요 유머 면에서 말이죠

14:14.220 --> 14:15.060
좋아요

14:15.090 --> 14:18.960
아무튼 본격적으로 GPT 4로 돌아가 보죠

14:19.170 --> 14:20.910
다들 같은 질문을 했죠

14:20.910 --> 14:22.410
정말 도움이 되는 조수네요

14:22.440 --> 14:25.950
사업상의 문제가 LLM 해결책에 적합한지 어떻게 판단하죠?

14:25.950 --> 14:29.790
채팅 인터페이스를 통해 가장 먼저 받은 질문이었죠

14:29.970 --> 14:32.970
이제 다시 합칠 수 있어요

14:32.970 --> 14:34.260
이런 거 익숙하죠?

14:34.290 --> 14:37.320
가격 인하 결과를 스트리밍으로 보여드릴게요

14:37.320 --> 14:40.770
OpenAI 채팅 .완료 .Create죠

14:40.770 --> 14:41.880
모델을 통과시키죠

14:41.880 --> 14:43.350
큰 녀석을 노릴 거예요

14:43.530 --> 14:44.820
프롬프트도 사용해요

14:44.820 --> 14:45.840
온도를 설정했어요

14:45.840 --> 14:47.250
스트리밍은 true라고 하죠

14:47.250 --> 14:49.680
오픈아이는 이렇게 요리해요

14:49.830 --> 14:54.750
그리고 이런 식으로 결과를 다시 스트리밍하죠

14:54.750 --> 14:57.720
비트 박스를 줄이는 중이라 좀 더 복잡해요

14:57.720 --> 15:03.390
그래서 우린 여기서 특별한 작업을 해야 합니다 각 반복에서 마크다운을 새로

15:03.390 --> 15:04.950
고침하기 위해서요

15:04.980 --> 15:08.850
이렇게 해야 하는지 확신이 안 들면 저걸 빼고 다르게 해 보세요

15:08.850 --> 15:11.190
그럼 바로 어떻게 되는지 보일 거예요

15:11.220 --> 15:13.200
보기 안 좋을 거예요

15:13.440 --> 15:15.720
그걸 실행해 보죠

15:15.720 --> 15:21.810
get get 결과가 나왔네요. 아주 좋아 보이죠.

15:22.500 --> 15:28.260
일부만 뚫렸을 때 튕기는 게 보이죠

15:28.260 --> 15:33.600
즉 부제목을 나타내는 여러 개의 해시를 해석하는 것이죠

15:33.600 --> 15:37.050
해시 하나만 받았는데 큰 게 오는 줄 아나 봐요

15:37.110 --> 15:41.430
마크다운이 일어나면서 깜빡이는 현상이 잠깐 있었던

15:41.430 --> 15:42.660
것 같아요

15:42.660 --> 15:50.730
하지만 마지막에 우린 아주 잘 구성된 응답을 얻습니다 잘 구조화되어 마크다운

15:50.730 --> 15:55.020
스트림 백에서 완벽한 포맷이죠

15:55.740 --> 15:56.460
좋아요

15:56.460 --> 16:04.140
다양한 API에 대한 감각을 얻었고 재미있는 질문도 비트 박스를 좀 어지럽혔죠

16:04.170 --> 16:12.150
다음 비디오에서 할 것은 몇 개의 llms가 서로 대화하는 겁니다 재미있을

16:12.150 --> 16:13.200
거예요

16:13.200 --> 16:14.340
그때 봐요
