WEBVTT

00:00.800 --> 00:09.230
다시 오신 걸 환영합니다 얼굴 포옹 트랜스포머 라이브러리 모델 수업에 잘 오셨어요

00:09.260 --> 00:14.960
라마 모델의 구조를 살펴보고 있었어요 단순히 작성했을 때 얻게 되는 거죠 그리고 다른

00:14.960 --> 00:17.690
것들을 위해 그걸 살펴보라고 했어요

00:17.690 --> 00:22.010
또 하나 지적할 점은 입체감을 봐야 한다는 거예요

00:22.010 --> 00:28.490
간단히 말씀드렸듯이 단어 사용을 나타내는 입력 크기 수를

00:28.490 --> 00:32.120
보세요 아래의 출력과 일치하죠

00:32.300 --> 00:39.080
get-ture 구조를 통해 크기를 따라가면 어떤 상황인지 알 수 있죠

00:40.010 --> 00:41.060
좋아요

00:41.060 --> 00:48.170
이제 이 모든 걸 끝냈고 어떻게 되는지에 대해 얘기했고 입력을 빌드했어요

00:48.200 --> 00:51.320
이제 일할 시간이에요

00:51.320 --> 00:53.990
이건 메서드 모델 .생성이에요

00:53.990 --> 00:58.220
이걸 위해 GPU 위에 있는 입력값을 가져가죠

00:58.460 --> 01:02.100
새 토큰을 80개까지 원한다고 할 수 있어요

01:02.340 --> 01:06.150
우리가 부탁한 건 농담이란 걸 잊으셨나 봐요

01:06.180 --> 01:08.970
데이터 과학자들에겐 농담거리였죠

01:09.000 --> 01:11.460
우리가 좋아하는 실험이죠

01:11.700 --> 01:16.110
그런 다음 출력부를 찍어요

01:16.110 --> 01:20.490
첫 번째 출력물을 가져가면 하나밖에 안 남아요

01:20.760 --> 01:28.770
그런 다음 토큰라이저 .디코드를 호출해 토큰에서 다시 글자로 텍스트로 바꾸죠

01:28.770 --> 01:30.690
결과를 프린트하죠

01:30.720 --> 01:32.340
시작하죠

01:32.520 --> 01:34.560
달리기 시작하네요

01:34.710 --> 01:41.070
아래를 보면서 진행 상황을 보고 싶어요 앞으로 패스하는 걸 보는 거죠

01:41.070 --> 01:45.810
명랑한 농담에 대한 답이 나왔네요

01:45.810 --> 01:49.590
왜 회귀 모형이 신경망과 분리된 거죠?

01:49.620 --> 01:54.900
안 맞았거든요 신경망이 감정에 너무 부합했어요

01:55.230 --> 01:56.610
뭐, 괜찮아요

01:56.610 --> 01:57.830
나쁘지 않아요

01:57.830 --> 01:58.880
네

01:58.910 --> 02:02.990
네, 완벽하게 그럴듯한 농담이죠

02:02.990 --> 02:05.870
그렇게 웃기진 않지만 뭐, 그렇죠

02:05.900 --> 02:07.370
나쁘지 않네요

02:09.290 --> 02:12.350
왜 물리적 회귀 모델이 상담을 받으러 갔죠?

02:12.350 --> 02:15.410
감정을 분류하는 게 어려웠거든요

02:15.410 --> 02:17.120
정말 좋은 것 같아요

02:17.120 --> 02:18.560
좋은 것 같아요

02:19.520 --> 02:23.810
데이터 과학을 좋아하는 사람들에겐 더 간단하고 딱 맞죠

02:23.810 --> 02:30.410
프론티어 모델에서 만든 것보다 더 괜찮은 개그 같아요

02:30.830 --> 02:33.320
정말 잘했어요

02:33.320 --> 02:34.520
라마 3요 1번요

02:34.730 --> 02:38.630
다시 한 번 명심해야 할 것은 우리가 다루는 건 80억

02:38.630 --> 02:45.320
개의 라마 3 매개 변수 버전이란 거예요 첫째, 가장 작은 버전이죠 4개로 퀀타이즈한 다음 두

02:45.320 --> 02:46.730
배로 퀀타이즈했어요

02:46.910 --> 02:54.740
모델의 아주 날씬한 버전인데 아주 괜찮은 농담을 하더군요

02:55.710 --> 02:57.840
재밌게 보셨길 바라요

02:57.990 --> 03:01.500
다음으로 할 일은 청소해서 메모리를 확보하는 거예요

03:01.500 --> 03:06.570
그렇지 않으면 계속 실행하면 다른 모델들이 GPU 소진으로 빠르게 소진되죠

03:06.660 --> 03:13.320
이런 경우에 여러분은 언제든 세션을 다시 시작할 수 있습니다 런타임 리스타트

03:13.320 --> 03:19.110
세션으로요 그런 다음 가져오기 다음에 중단한 곳에서 계속하죠

03:19.620 --> 03:25.770
다음으로 할 일은 우리가 한 모든 걸 패키지화해 작은 함수로

03:25.770 --> 03:27.450
만드는 거예요

03:27.450 --> 03:34.770
이 함수는 모델의 이름과 일반적인 사전 목록의 메시지를 취하죠

03:34.770 --> 03:38.970
한 줄씩 살펴보면서 방금 한 걸 다시 검토해보죠

03:39.000 --> 03:48.060
오토 토큰라이저 클래스를 이용해 우리가 작업하는 모델에 기반을 둔 새 토큰라이저를 만드는 것으로 시작하죠

03:48.990 --> 03:54.860
이 선은 안쪽 토큰을 문장 토큰과 동일하게 설정하는 부분이죠

03:54.890 --> 03:57.710
이건 표준 절차예요

03:57.950 --> 03:58.970
그리고 이거요

03:58.970 --> 04:00.890
잘 알고 있어요

04:01.130 --> 04:08.900
여기서 토큰라이저에 적합한 채팅 템플릿을 리스트에 적용해요

04:08.900 --> 04:13.220
그럼 일련의 패를 반환하죠

04:13.220 --> 04:20.240
그런 다음 GPU에 넣고 입력을 할당하죠

04:20.510 --> 04:21.590
새롭네요

04:21.590 --> 04:29.120
추가할 또 다른 기술로 결과를 스트리밍으로 보여드리죠

04:29.240 --> 04:33.320
허깅페이스 라이브러리도 그걸 지원하죠

04:33.320 --> 04:36.290
텍스트 스트리머라는 걸 만들어요

04:36.320 --> 04:41.360
토큰라이저를 줘야 합니다 토큰을 스트리밍하면 토큰을 텍스트로 다시

04:41.360 --> 04:43.190
변환해야 하니까요

04:43.220 --> 04:46.130
어떤 토큰라이저를 쓰는지 알아야 해요

04:46.130 --> 04:50.240
그걸 제공하고 액션을 하는 거죠

04:50.450 --> 04:52.400
먼저 모형을 get 해요

04:52.400 --> 04:55.200
인과관계 lm 오토 모델이에요

04:55.200 --> 04:58.770
오토 토큰마이저와 같은 거예요

04:58.770 --> 05:04.350
하지만 미리 훈련된 모델을 로드하려면 모델 이름을 말해야 해요

05:04.470 --> 05:07.590
장치 맵은 자동입니다 사용자 GPU 말이에요

05:07.590 --> 05:16.260
하나를 갖고 계시면 저 위에 설치한 퀀트 구성을 넘겨요 4비트 이중

05:16.260 --> 05:22.650
퀀트화된 NF로 4비트 숫자 유형을 뜻하죠

05:22.650 --> 05:28.710
bfloat16은 데이터 타입을 계산한 거고요

05:29.010 --> 05:34.170
이제 비즈니스 모델 생성 차례죠

05:34.170 --> 05:35.460
그게 큰 방법이죠

05:35.460 --> 05:42.540
새 토큰 80개까지 생성할 입력을 전달하고 스트리머를 주는 거죠

05:42.570 --> 05:49.020
이게 결과물을 스트림으로 내보낸다는 뜻입니다 그런 다음 정리를 하죠

05:49.530 --> 05:55.550
이 함수는 우리가 전에 했던 모든 걸 래핑해주고 스트리밍도 추가해줘요

05:55.550 --> 06:03.350
이제 간단하게 메시지를 파이 3으로 불러볼게요 방금 만든 함수를 사용해서요

06:03.350 --> 06:06.380
피 3은 다시 로드할게요

06:06.410 --> 06:12.920
이건 시간이 좀 걸릴 거예요 처음으로 비트를 싣는 거니까요

06:12.980 --> 06:16.370
이미 로드했어요 디스크에 캐시돼 있죠

06:16.460 --> 06:20.660
얼굴 껴안는 걸 다시 할 필요는 없어요

06:20.690 --> 06:21.380
허브요

06:21.710 --> 06:25.730
비트를 싣는 동안 할 일이 좀 더 남았어요

06:25.730 --> 06:27.140
그럴까요?

06:27.170 --> 06:32.180
리소스를 살펴보자고 하려고 했는데 너무 빨리 끝날 것 같네요 스트리밍하는

06:32.180 --> 06:34.580
걸 보여드리고 싶어요

06:34.610 --> 06:39.050
실망하실지도 모른다는 걸 미리 알려드려야 할 것 같아서요

06:39.560 --> 06:49.010
그래서 적어도 제가 갖고 있는 프롬프트만 사용해봐도 파이 3이 농담을 하게 만들 수 없다는 걸 알게 됐죠.

06:49.040 --> 06:50.400
Get it.

06:50.400 --> 06:57.540
데이터 과학자들이 장황하게 늘어놓는 일반적인 정보를

06:57.540 --> 06:59.010
제공하죠

06:59.040 --> 07:04.320
비트 부분을 53번에게 좀 더 단호하게 바꿀 수 있을지

07:04.320 --> 07:08.700
53번이 원하지 않을지는 모르겠어요

07:08.970 --> 07:14.040
53 소방대는 많은 일을 훌륭히 해내겠지만 이 작업은 아니에요

07:14.040 --> 07:19.680
다른 모델과 마찬가지로 여러분도 연습해 보세요

07:19.680 --> 07:27.000
또한 수정 헌정으로 53이 농담을 하게 할 수 있는지 혹은 53이 농담이 안 웃기면 get이 잘하는

07:27.000 --> 07:28.860
걸 찾을 수 있는지도요

07:28.860 --> 07:34.200
우리가 llms에 대해 했던 질문에 대한 답변이 될 겁니다. Lllms의 사용에

07:34.200 --> 07:35.130
대해서요.

07:35.700 --> 07:41.070
이게 53번 결과예요

07:41.070 --> 07:43.410
제마는 어떤지 보죠

07:43.410 --> 07:51.290
같은 접근법으로 인포 구글의 젬마 2 모델에 유틸리티 함수를 쓸 수 있어요 젬마는 시스템 프롬프트를

07:51.320 --> 07:57.650
지원하지 않죠 그래서 이렇게 사용자 프롬프트를 입력해야 해요 시스템 프롬프트는

07:57.650 --> 08:02.270
특별한 걸 말하지 않으니 괜찮아요

08:02.540 --> 08:06.500
제마를 한번 보죠

08:06.500 --> 08:08.780
물론 20억 개 모델이죠

08:08.780 --> 08:10.130
아주 작은 모델이죠

08:10.130 --> 08:15.980
아주 작은 모델일 뿐 아니라 4비트로 수량화하고 다시 수량화하고

08:15.980 --> 08:16.850
있어요

08:16.850 --> 08:25.520
이 시점에서 우린 아주 슬림 모델을 다루고 있어요 메모리를 많이 쓰지 않고 잘

08:25.520 --> 08:29.270
로드되고 빨리 농담도 할 수 있죠

08:32.330 --> 08:34.130
농담도 나오네요

08:34.130 --> 08:37.400
데이터 과학자들은 왜 통계 전문가와 헤어졌을까요?

08:37.400 --> 08:41.240
p 값에 대한 의견이 너무 많이 충돌했거든요

08:41.270 --> 08:44.540
p 값에 관한 또 다른 샌님 농담이죠

08:44.570 --> 08:46.130
Get it, get it, get it! 이해가 안 돼요

08:46.140 --> 08:49.470
하지만 제가 놓치고 있는 게 있을지도 몰라요

08:49.620 --> 08:50.790
어서 오세요

08:50.790 --> 08:53.040
누가 좀 알려줘요

08:53.370 --> 08:53.760
네

08:53.760 --> 08:56.820
그래도 착하고 친근해서 좋아요

08:56.820 --> 08:57.540
또 있어요

08:57.540 --> 08:59.490
다른 농담 듣고 싶으면 말해요

08:59.640 --> 09:02.970
Get it로 하면 더 재밌는 농담이 나올지도 모르겠네요

09:03.120 --> 09:07.890
하지만 확실히 즐거운 어조예요

09:07.890 --> 09:14.340
2번 제마는 칭찬받을 만한 일을 했어요 데이터 과학과 관련이

09:14.340 --> 09:15.180
있죠

09:15.480 --> 09:21.780
특히 이건 작은 모델이라 수량화하고 있다는 걸 기억하면요

09:21.780 --> 09:28.440
2번도 잘 보여 줬지만 제가 사용한 양자 원리를 사용하면 우수한

09:28.440 --> 09:36.150
결과가 나올 거예요 3번 파이에서도 더 나은 결과가 나올 수 있겠죠

09:36.150 --> 09:41.310
혼합 트라이얼 모델이든 좀 더 날씬한 비트 모델이든 좋은 결과가

09:41.340 --> 09:44.370
나올 거예요 get it get it

09:44.510 --> 09:49.580
수학 문제를 내도 돼요 애들이 어려워하는 문제요

09:49.610 --> 09:52.640
어려운 수학 문제라면요

09:52.790 --> 09:59.360
하지만 라마 3에 꽤 어려운 질문을 했어요 당신도 같은 경험을 할 수 있는지

09:59.360 --> 10:02.840
보는 데 전혀 어려움이 없었죠

10:03.200 --> 10:09.050
어쨌든 이 모델들을 사용해 보고 다양한 것을 시도해 볼 기회예요

10:09.080 --> 10:11.450
오픈 소스 모델로 작업하고 있죠

10:11.540 --> 10:13.490
API 비용은 없어요

10:13.520 --> 10:16.610
모든 걸 소진할 때만 대가를 치르죠

10:16.610 --> 10:23.840
프리 콜랍을 안 쓰는데 구글 콜랍 비용으로 단위를 쓴다면

10:23.840 --> 10:25.850
말이죠

10:26.030 --> 10:32.240
1을 쓰고 있어요 시간당 76유닛요

10:32.240 --> 10:40.250
이와 함께 작업할 시간은 충분합니다 포옹하는 트랜스포머 라이브러리를 이용해

10:40.250 --> 10:43.790
오픈 소스 모델 추론을 즐길 수 있죠
