WEBVTT

00:00.260 --> 00:02.930
さて､ トークンについて少し話をしよう｡ 

00:02.960 --> 00:07.700
トークンとは､ モデルに渡される個々のユニットのことである｡ 

00:07.880 --> 00:16.790
ニューラルネットワークを構築した初期には､ 一文字ずつ学習させたニューラルネットワークをよく見かけたものだ｡

00:16.790 --> 00:22.400
つまり､ 一連の個々の登場人物をモデル化し､ 前に登場した人物から､

00:22.400 --> 00:28.250
次に登場する可能性の高い人物を予測するように訓練するのだ｡

00:28.280 --> 00:33.020
あれは特殊なテクニックで､ ある意味､ 多くの利点があった｡ 

00:33.020 --> 00:39.950
つまり､ 入力可能な数は限られており､ アルファベットといくつかの記号の数だけだった｡

00:39.950 --> 00:43.760
つまり､ 非常に扱いやすいボキャブラリーサイズだったということだ｡ 

00:43.760 --> 00:49.910
そして､ そのウェイトが必要なのだから､ インプットに多くの異なる可能性を心配する必要はない｡

00:49.940 --> 00:57.200
しかし､ 一連の異なる文字がどのようにして単語になるのかを理解するという点で､

00:57.200 --> 01:15.550
モデルには多くのことが要求され､ 単語の背後にある意味に関連するすべての知性をモデルの重みの中に取り込まなければならない｡

01:15.550 --> 01:25.270
そこで私たちは､ 可能性のある単語をひとつひとつ訓練してニューラルネットワークを構築した｡

01:25.270 --> 01:31.840
それは辞書のようなもので､ ありとあらゆる単語の索引のようなものだ｡

01:31.840 --> 01:37.420
そして､ トークンが持つ可能性のある単語は､ これらの可能性のある単語のいずれかである可能性がある｡ 

01:37.420 --> 01:49.600
つまり､ 一連の文字がどのような意味を持つかを理解するのではなく､ 個々の単語がそれぞれ異なる意味を持つことをモデル自体が理解し始めたのだ｡

01:49.600 --> 01:51.040
だから､ それはいいことだった｡ 

01:51.040 --> 01:55.210
しかし､ その結果､ 膨大なボキャブラリーを抱えることになった｡ 

01:55.240 --> 01:58.740
可能な限りの単語をボキャブラリーとして持っておく必要があった｡ 

01:58.740 --> 02:04.140
もちろん､ 地名や人名もあるので､ 可能性のある単語はたくさんある｡ 

02:04.140 --> 02:07.800
だから､ 未知の言葉には特別なトークンが必要だった｡ 

02:07.800 --> 02:10.650
そして､ そのためにいくつかの制限が生じた｡ 

02:10.830 --> 02:13.860
珍しい言葉は省略され､ 特別な場所は省略されなければならなかった｡ 

02:13.860 --> 02:17.370
それで､ いくつか奇妙なことが起きた｡ 

02:17.550 --> 02:25.860
そしてGPTの頃､ ある発見がなされた｡ 個々の文字に基づいてモデルを訓練し､ それらを組み合わせて単語を形成する方法を学習させるのではなく､

02:25.860 --> 02:36.360
この2つの両極端の中間に､ ある種の幸福な媒体があるという画期的な発見がなされたのだ｡

02:36.360 --> 02:43.350
各単語が異なるトークンであると言うのではなく､ 文字のかたまり､ あるときは完全な単語を形成し､

02:43.350 --> 02:48.600
あるときは単語の一部を形成するかたまりをトークンと呼び､ 一連のトークンを受け取り､

02:48.600 --> 02:57.740
渡されたトークンに基づいてトークンを出力するようにモデルを訓練することができる｡

02:57.740 --> 03:01.250
そして､ これには多くの興味深い利点があった｡ 

03:01.250 --> 03:07.880
そのひとつは､ 物事をトークンに分解することで､ 地名や固有名詞のようなものも扱えるようになることだ｡

03:07.880 --> 03:10.040
トークンの断片が増えるだけだ｡ 

03:10.040 --> 03:15.800
それは､ 単語の語幹､ つまり同じ語頭と複数の語尾の可能性があるものを1つのトークンにエンコードし､

03:15.950 --> 03:27.950
その後に2つ目のトークンをいくつか並べるという処理に長けているということだ｡

03:27.950 --> 03:40.400
つまり､ トークンが同じような構造を持っているので､ 言おうとしていることの根本的な意味をモデルの中で簡単に表現することができた｡

03:40.400 --> 03:42.380
もう少し現実的な話をしよう｡ 

03:43.100 --> 03:51.530
GPT､ OpenAIは実際にプラットフォームopenaiであるツールを提供しています｡  comのスラッシュ・トークナイザーで､ テキストを入力して､

03:51.530 --> 03:58.240
そのテキストがトークンにどのように変換されるかを視覚的に見ることができる｡

03:58.390 --> 04:05.080
そこで私はある文章を取り上げた｡ 私のクラスのAIエンジニアにとって重要な文章だ｡ 

04:05.350 --> 04:08.650
GPTがそれをトークン化しているのがわかるだろう｡ 

04:08.650 --> 04:12.700
これは､ 言葉をトークンに変えるときに使う動詞だ｡ 

04:12.700 --> 04:17.080
そして､ それをどのようにトークンに変えたかを色分けして強調している｡ 

04:17.080 --> 04:26.320
そしてこの場合､ これらはすべて一般的な単語であるため､ これらの単語のひとつひとつがひとつのトークンに正確に対応する｡

04:26.590 --> 04:28.480
だから､ これは明確な例だ｡ 

04:28.480 --> 04:29.860
色を見ればわかるだろう｡ 

04:29.860 --> 04:34.060
もうひとつ､ 少し興味深い点がある｡ 

04:34.060 --> 04:40.030
色つきのボックスの中には､ 例えば "for "の前にスペースが入っているものがある｡ 

04:40.060 --> 04:41.200
まるで宇宙のようだ｡ 

04:41.200 --> 04:43.570
そして､ それが形骸化したものだ｡ 

04:43.660 --> 04:49.330
トークン化の際､ 単語と単語の間の区切りにも意味があるからだ｡ 

04:49.330 --> 04:55.440
このトークンはforという単語を単独で表し､ 単語の頭に続くforという文字が1つのトークン､

04:55.530 --> 05:05.010
トークンの単語の頭にマッピングされる｡

05:05.010 --> 05:05.250
でもね｡ 

05:05.250 --> 05:11.970
しかし､ 単語と単語の間のギャップがトークンの一部として含まれることは注目に値する｡ 

05:12.780 --> 05:15.120
では､ 別の例を挙げてみよう｡ 

05:15.150 --> 05:26.880
この例では､ もう少し面白い文章､ つまりLMの魔術師である私のマスタリングのために絶妙に手作りされた小ネタを考えている｡

05:26.910 --> 05:30.060
Musterersは造語だと思う｡ 

05:30.060 --> 05:34.950
ご覧のように､ 赤い四角いアンダーラインは､ それが本当の言葉ではないことを示している｡ 

05:34.950 --> 05:38.010
トークン化がどのように行われたかを見てみよう｡ 

05:38.010 --> 05:48.330
だから､ この4はまだ1つの単語としてここにある｡

05:48.330 --> 05:54.920
しかし､ 冒頭のアンもそうだが､ 絶妙に複数のトークンに分割されている｡ 

05:54.950 --> 05:56.690
絶妙だ｡ 

05:57.380 --> 06:04.130
そしてこれは､ 珍しい単語がある場合､ その単語がボキャブラリーの中に1単語として含まれていないことを示している｡ 

06:04.130 --> 06:08.630
そのため､ 複数のトークンに分割しなければならなかったが､ それでも通過させることができた｡ 

06:08.750 --> 06:11.570
そして今､ ハンドクラフトという言葉を見てほしい｡ 

06:11.570 --> 06:19.280
このボキャブラリーも単一のトークンとして持っているわけではなく､ それを手で分解して作ることができるのだ｡

06:19.280 --> 06:25.640
そしてそれは､ ある意味､ 意味を反映している｡ 

06:25.670 --> 06:28.280
この2つの手を組み合わせて作られる｡ 

06:28.280 --> 06:33.320
そして､ 細工されたトークンには文頭が含まれていないこともわかるだろう｡ 

06:33.320 --> 06:38.810
つまり､ 真ん中に細工が施された単語を表すトークンなのだ｡ 

06:38.810 --> 06:41.000
それがこのトークンに反映されている｡ 

06:41.090 --> 06:43.430
その口癖がまったくないことがわかるだろう｡ 

06:43.430 --> 06:48.700
侵入されて､ それで......マスタリーを見ることになる｡ 

06:48.700 --> 06:52.300
そしてこれは､ 私が語幹について言ったことの良い例である｡ 

06:52.300 --> 06:58.900
Masteriesはmaster､ つまりマスターする人という動詞と､

06:58.900 --> 07:04.720
その延長線上にあるerrsに分かれる｡

07:05.020 --> 07:15.820
実際の単語ではないにもかかわらず､ 2つのトークンに分割することで､ 私たちが言おうとしていることの意味を反映することができるのです｡

07:16.390 --> 07:22.600
また､ 魔術が魔女と工芸に分かれたのも興味深い｡ 

07:23.170 --> 07:25.540
それで､ そう､ 手作りなんだ｡ 

07:25.540 --> 07:29.740
そしてマスターは､ 私が言ったように､ トークンによってそこにどのように意味が反映されているかがわかるだろう｡ 

07:29.740 --> 07:35.980
そして､ 何かをトークンに分解するということがどういうことなのか､ これが本当の意味での洞察になることを願っている｡ 

07:37.150 --> 07:42.250
そこで興味深いのは､ もう少し洗練された例をお見せすることだ｡ 

07:42.280 --> 07:48.130
ええと､ 私の好きな番号､ 6534589793238462643383らしい｡ 

07:48.160 --> 07:49.030
ブラブラブラ｡ 

07:49.180 --> 08:01.750
円周率のような長い数字は､ もちろん1つのトークンには対応しない｡

08:01.930 --> 08:11.020
そして実際､ ここで起こっているのは､ 3桁の数字が1つのトークンにマッピングされているということだ｡

08:11.260 --> 08:13.030
これは興味深い性質だ｡ 

08:13.030 --> 08:19.000
実はこれはGPTの2つのトークナイザーの特性なのだが､ 他の多くのケースはそれほど多くない｡ 

08:19.000 --> 08:23.170
複数のトークンにマッピングされていることがわかるだろう｡ 

08:24.220 --> 08:33.670
一般的に言って､ トークンを見るときに覚えておくと便利な経験則があります｡

08:33.670 --> 08:41.560
経験則では､ 平均して1トークンは約4文字に対応する｡ 

08:41.860 --> 08:50.060
つまり､ 通常の英文では､ トークンは単語の4分の3程度ということになる｡

08:50.060 --> 08:53.210
1トークンは約0に相当する｡  75語｡ 

08:53.210 --> 08:54.950
そして､ それをより簡単に考える方法もある｡ 

08:54.950 --> 09:00.260
つまり､ 1000トークンは約750ワードということだ｡ 

09:00.260 --> 09:01.880
だから､ これがあなたの頭の中にあるマッピングなんだ｡ 

09:01.910 --> 09:04.820
1000トークンは750ワードである｡ 

09:04.820 --> 09:11.300
ということは､ 例えばシェイクスピア全集で､ 実際の例を挙げれば､

09:11.300 --> 09:16.460
約90万語ということになる｡

09:16.460 --> 09:23.510
だから1. 200万トークンといえば､ シェークスピア全集のサイズだ｡ 

09:23.540 --> 09:26.270
さて､ これは英語の話だ｡ 

09:26.270 --> 09:33.920
数学の公式や科学用語､ コードなどを見ている場合､ トークン数はもっと多くなる｡

09:33.920 --> 09:43.990
数字で見たように､ 右の記号などを組み込むにはもっと多くのトークンに分割する必要があるからだ｡

09:44.290 --> 09:48.790
そして､ もうひとつのポイントは､ これはGPTトークナイザーを示しているということだ｡ 

09:48.820 --> 09:52.930
トーケナイザーがどのように機能すべきかについて､ 確固たるルールはない｡ 

09:52.960 --> 09:57.340
実際､ 1分前に見たように､ 初期の頃はトークナイザーがあり､

09:57.340 --> 10:02.770
すべての文字が1つのトークンにマッピングされていた｡

10:02.770 --> 10:08.440
オープン・ソースを後で見るときには､ いろいろなトークナイザーに触れることになる｡

10:08.470 --> 10:12.280
そして､ Llamaのトークナイザーの興味深い特性も探求するつもりだ｡ 

10:12.310 --> 10:15.280
だから､ トークナイザーによって動作が異なることがある｡ 

10:15.280 --> 10:20.170
トークンの数が少なかったり､ 多かったりすることには賛否両論がある｡ 

10:20.380 --> 10:27.640
モデルのパラメーターの数や学習方法などによって､ 答えは一つではない｡

10:27.820 --> 10:34.930
言葉や文字からトークンの世界に入るということがどういうことなのか､

10:34.930 --> 10:41.230
少しはわかってもらえただろうか｡
