WEBVTT

00:01.160 --> 00:06.590
제가 이런 말 하는 게 지겹겠지만 진실의 순간이 왔어요

00:06.590 --> 00:07.970
진실의 순간이 왔어요

00:07.970 --> 00:12.440
보시다시피 전 정말 흥분돼요 여러분도 그렇길 바라요

00:12.500 --> 00:17.270
이건 저희에게 아주 중요한 순간이에요

00:17.270 --> 00:24.680
콜랍 비트를 7일간 사용하고 있는데요 솔직히 말하면 좀 야한

00:24.710 --> 00:37.370
걸 준비했어요 아주 우람한 A100 GPU 박스를 골랐죠 콜랍 박스에서 가장 비싼 제품이에요

00:37.370 --> 00:46.790
11점을 기록하네요 시간당 계산 단위 77개 현재 시세는 제가 잘못

00:46.790 --> 00:52.160
안 게 아니라면 약 100개 단위 약 10달러네요

00:52.160 --> 00:56.000
시간당 1달러 정도를 쓰는 거예요

00:56.210 --> 01:00.050
지금은 가격이 때때로 바뀌어요

01:00.050 --> 01:05.820
네, 이건 확실히 평소보다 싸지 않아요

01:05.820 --> 01:07.260
이럴 필요 없어요

01:07.260 --> 01:13.050
꼭 필요한 건 어디 보죠

01:13.080 --> 01:14.820
런타임 타입을 변경하세요

01:14.820 --> 01:16.560
t4 GPU를 사용할 수도 있어요

01:16.590 --> 01:21.030
어떤 매개 변수를 변경할지 알려드릴게요, 괜찮을 거예요

01:21.090 --> 01:25.560
하지만 몇 달러만 써도 블래스트를 경험하고 하이퍼 매개변수를

01:25.560 --> 01:31.320
바꾸고 빠르게 훈련하고 싶다면 A100 박스를 사용하면 됩니다 GPU

01:31.320 --> 01:36.540
40GB가 있는 박스의 힘을 경험해보세요

01:36.540 --> 01:42.450
구글에 돈을 좀 쥐여줘도 괜찮아요 이렇게 즙이 많고

01:42.450 --> 01:46.080
강력한 비트 박스가 있잖아요

01:46.080 --> 01:49.260
어쨌든 파이프 설치부터 시작할게요

01:49.260 --> 01:56.850
TRL 트랜스포머 강화 학습 프로그램인 HINGPace 라이브러리도 포함됩니다

01:56.850 --> 02:03.730
SFT 트레이너가 감독한 파인 튜닝 트레이너로 오늘 저희가 할 트레이너죠

02:04.090 --> 02:10.990
잠시 드릴 말씀이 있는데요 지난 비디오에서 훈련에 관해

02:11.170 --> 02:17.140
조금 다뤘습니다 비트를 통해 하이퍼파라미터도

02:17.140 --> 02:23.440
언급했고 전후 패스 훈련 과정도 조금 다뤘죠

02:23.440 --> 02:26.830
어떤 사람들은 그런 게 익숙할 거예요

02:26.830 --> 02:32.140
어떤 사람들은 당신의 머리 위에 있는 것을 요구할 수도 있습니다. HDTT를

02:32.170 --> 02:36.490
더 잘 설명하기 위해 시간을 더 들이지 않을 수 있나요?

02:36.490 --> 02:43.600
한 가지 말씀드리고 싶은 건 포옹하는 얼굴 덕분에 이 과정이 아주 쉬워졌다는 거예요

02:43.660 --> 02:49.150
훈련에 있어서 진입 장벽이 너무 낮아서 직관력을 발휘하는

02:49.180 --> 02:55.360
게 도움이 되긴 해요 최적화하고 올바른 방향으로 나아가는

02:55.360 --> 02:57.220
직관력요

02:57.220 --> 03:00.880
이론의 세부 사항을 알 필요는 없어요

03:00.940 --> 03:04.390
하이퍼파라미터 조정할 만큼만 알면 돼요

03:04.390 --> 03:04.390
매개 변수요

03:04.390 --> 03:07.960
얼굴을 끌어안으면 훈련이 아주 쉬워져요

03:07.960 --> 03:13.690
만약 어떤 것들이 여러분 머리 위를 지나갔다고 생각하신다면 걱정하지 마세요. 중요하지 않아요. HET

03:13.690 --> 03:15.760
코드가 아주 명확할 거예요

03:15.760 --> 03:20.710
이미 이 모든 걸 알고 최적화에 익숙하다면 아주 좋아요

03:21.130 --> 03:22.750
그럼 훨씬 더 좋겠죠

03:22.750 --> 03:24.220
그럼 더 쉽겠죠

03:24.430 --> 03:27.760
수입을 좀 해 보죠

03:28.120 --> 03:31.180
이제 매개 변수가 많이 생겼죠

03:31.180 --> 03:36.160
기본 모델은 당연히 라마 3이고요 180억 달러요

03:36.160 --> 03:38.680
프로젝트 이름은 잘 알죠

03:38.680 --> 03:42.820
무게와 편향에 사용될 프로젝트 이름이에요

03:42.820 --> 03:47.980
Get을 하면 비교할 수 있는 결과를 보여줄 거예요

03:47.980 --> 03:51.010
H깅 페이스 허브에 업로드 할 때도 사용할 거예요

03:51.010 --> 03:55.990
이 프로젝트에선 프라이서란 이름을 사용하고 있어요 가격과 관련된 뭔가처럼요

03:55.990 --> 04:00.820
기억하실지 모르겠지만 GPT를 훈련할 때 프라이서 GPT라고 불렀어요

04:01.360 --> 04:07.520
결과가 다를까 봐 프로젝트를 분리해 뒀어요 다른 양을 측정할 거라서 같은

04:07.520 --> 04:10.640
프로젝트에 두면 헷갈릴 수 있거든요

04:10.910 --> 04:14.600
그래서 프라이서라고 이름 지었어요

04:15.170 --> 04:19.580
아, 안는 얼굴 사용자요 여기에 안는 얼굴 이름을 넣으세요

04:19.670 --> 04:27.020
정밀 튜닝 모델을 허브에 post 하고 싶을 테니까요. 왜냐하면 여러분이 그것들을 소중히 여기고 미래에

04:27.020 --> 04:29.180
사용하게 될 테니까요.

04:29.180 --> 04:30.860
비밀로 간직할 수 있어요

04:30.860 --> 04:33.440
그냥 여러분이 드실 거예요

04:34.430 --> 04:38.000
데이터에 관해서는 데이터 세트를 로드해야 해요

04:38.000 --> 04:41.810
여기선 선택할 수 있어요

04:41.840 --> 04:47.990
몇 주 전에 데이터 큐레이션을 할 때 데이터 세트를 포옹 페이스 허브에 업로드했죠

04:47.990 --> 04:53.960
이 선을 여기 둘 수 있어요 포옹 페이스 사용자 이름과 프라이서 데이터죠

04:53.960 --> 04:59.210
우리가 부르는 이름이에요 로드할 수 있어요

04:59.330 --> 05:05.330
아니면 그냥 이걸 보면서 데이터셋 작업하는 걸 보든지요 허깅페이스에

05:05.330 --> 05:08.460
업로드 안 한 거요

05:08.460 --> 05:09.030
안타깝네요

05:09.030 --> 05:11.460
그래도 많은 일이 있었다는 건 알아요

05:11.490 --> 05:15.660
하드코딩한 이 라인을 주석 처리 해제할 수 있어요

05:15.660 --> 05:17.010
F는 필요 없어요

05:17.220 --> 05:24.150
하드 코드화 했어요 허브에 데이터 가격을요 공개될 거예요

05:24.150 --> 05:26.460
그냥 다운로드 하세요

05:26.460 --> 05:26.730
좋아요

05:26.760 --> 05:27.360
좋아요

05:27.360 --> 05:29.730
직접 업로드 안 해도 돼요

05:30.090 --> 05:36.180
최대 시퀀스 길이는 데이터가 항상 조각되어서 179개의 토큰을

05:36.180 --> 05:43.080
넘지 않게 됩니다 문장 시작에 토큰 몇 개를 더하고 끝에 검프한 것을 추가하면

05:43.080 --> 05:46.620
최대 시퀀스 길이는 182개가 되죠

05:46.620 --> 05:50.610
이건 아주 중요해요 모든 훈련 데이터 포인트가 여기에 맞물리거든요

05:50.610 --> 05:55.080
GPU 메모리에 들어갈 만큼 작은 숫자여야 하죠

05:55.260 --> 05:59.640
그래서 이 정도 양에 맞게 설명을 잘라낸 거예요

06:00.450 --> 06:07.110
몇 가지 관리적인 것들을 살펴봅시다 각 실행의 실행 이름이라는 것을 생각해 냈습니다

06:07.110 --> 06:11.350
간단하게 현재 날짜를 나타내는 것이죠

06:11.350 --> 06:12.010
한 달요

06:12.010 --> 06:13.720
낮과 시간요

06:13.720 --> 06:13.990
1분

06:13.990 --> 06:14.650
두 번째요

06:14.800 --> 06:18.550
잠시 후에 그 이유를 알게 될 거예요

06:18.580 --> 06:23.830
프로젝트 Run name이라고 입력할게요 Pricer요

06:23.830 --> 06:32.200
그다음 하이픈, 날짜 허브 모델 이름을 입력하죠 모델을 저장하려는 건 사용자 이름이 될 거예요

06:32.200 --> 06:34.480
그리고 이거요

06:34.810 --> 06:35.290
네

06:35.290 --> 06:36.670
왜 이걸 하냐고요?

06:36.670 --> 06:40.000
그래서 사람들은 모델을 한 명만 쓰기도 해요

06:40.000 --> 06:46.720
실행하면 같은 모델 리포지토리에 저장한 다른 버전을 업로드만 하면 되죠

06:46.720 --> 06:49.810
허그페이스에서 하는 건 전부 깃 회수예요

06:49.810 --> 06:56.560
그러니까 여러분은 여러분의 모델의 새 버전을 계속 푸시할 수 있는 겁니다 그것의

06:56.560 --> 07:03.700
새 버전이 되는 거죠 코드의 새 버전을 확인하거나 버전의 코드를 푸시하는 것처럼요

07:03.700 --> 07:07.600
모델 이름은 같지만 모델의 다른 버전일 수도 있죠

07:07.600 --> 07:13.240
하지만 저는 다른 실행을 분리해서 다른 모델로 두는 걸 좋아합니다 그 안에 다른

07:13.240 --> 07:17.230
버전이 있으니까요 다른 시대로서도 가능하고요

07:17.230 --> 07:21.070
저는 분리하는 걸 좋아해요 다른 hyperperameter로 훈련했으니까요

07:21.070 --> 07:22.450
그걸 기록하고 싶어요

07:22.900 --> 07:25.300
전 이렇게 하는 게 좋아요

07:25.390 --> 07:28.420
꼭 필요한 건 아니지만 도움이 될 거예요

07:28.510 --> 07:33.340
그에 대한 느낌을 드리기 위해 run name을 선택해보죠 그것부터

07:33.340 --> 07:34.150
시작하죠

07:34.480 --> 07:37.450
지금 실행 이름을 보여드릴게요

07:37.960 --> 07:46.150
여기 보이는 실행 이름은 현재의 날짜입니다 22일과 현재 시간이죠

07:46.330 --> 07:50.080
우주 시간으로 이루어진 협회의 시간이죠

07:50.080 --> 07:52.690
사실 지금은 12시 4분이 아니에요

07:53.020 --> 07:55.060
그게 실행명이에요

07:55.060 --> 07:57.220
또 뭐가 있었죠?

07:57.220 --> 08:00.730
프로젝트 실행 이름과 허브 모델 이름이 있네요

08:00.730 --> 08:07.480
프로젝트 실행 이름은 Pricer로 하고요

08:07.480 --> 08:12.860
그리고 업로드 될 허브 모델 이름이죠

08:14.720 --> 08:16.190
그래요?

08:16.190 --> 08:20.360
실행된 후에 그 이름으로 모델을 생성할 거예요

08:20.360 --> 08:26.780
모델 디렉터리를 보면 이런 게 잔뜩 보여요 너무 많이 실행했거든요 인정하는

08:26.780 --> 08:29.150
것보다 더 많이요

08:29.240 --> 08:31.100
하지만 정말 즐거웠어요

08:31.160 --> 08:34.070
프라이서 모델은 많아요

08:34.070 --> 08:36.770
전부 도망치고 있어요

08:37.070 --> 08:45.260
이제 마무리할게요 방금 연결됐다가 끊겼다가 하네요

08:45.500 --> 08:55.430
훈련에 사용하는 하이퍼파라미터는 매트릭스의 차원이에요

08:55.490 --> 08:57.110
32명부터 시작할게요

08:57.140 --> 08:59.540
아까 말했듯이 8명까지 줄일 수 있어요

08:59.540 --> 09:01.520
특히 로우 박스라면요

09:01.520 --> 09:02.900
괜찮을 거예요

09:03.050 --> 09:06.140
로라, 알파는 더블 R이에요

09:06.170 --> 09:09.440
이걸 8로 내리면 16이 되는 거죠

09:10.040 --> 09:13.050
물론 목표 모듈이죠

09:13.050 --> 09:14.520
너무 잘 아시네요

09:14.520 --> 09:15.840
그게 뭔지는 말 안 해도 알겠죠

09:17.190 --> 09:19.770
이건 라마 3의 기본 모델이에요

09:19.800 --> 09:21.960
이 모듈이 목표예요

09:22.080 --> 09:26.250
로라 중퇴는 지난번에 꽤 길게 설명한 거예요

09:26.250 --> 09:33.450
정규화에 도움이 되는 방법입니다 새로운 데이터 포인트에 모델이 일반화되도록 하는

09:33.480 --> 09:40.800
거죠 이 경우에는 10%의 뉴런을 가져다가 활성화되지 않게 하고 완전히 지워버리는

09:40.800 --> 09:47.160
겁니다 훈련 과정에서 매번 다른 10%씩 뉴런을 제거하는 거죠

09:47.160 --> 09:51.780
그 결과, 모델은 특정 뉴런에 지나치게 의존하지 않아요

09:51.810 --> 09:59.160
전체 모델을 향상시키기 위해 전반적으로 학습합니다. 훈련 포인트를 받고 올바른 다음 토큰을

09:59.160 --> 10:03.390
주는 것을요. 모델이 일반화되도록 돕죠.

10:03.540 --> 10:07.020
10%도 10이에요 1은 아주 전형적인 출발점이에요

10:07.020 --> 10:13.300
5%와 20%로 해보고 퀀트 포 비츠로 어떻게 하나 봐요

10:13.300 --> 10:13.690
맞아요

10:13.720 --> 10:16.150
4분의 1로 수량화하고 있어요

10:16.690 --> 10:17.530
네

10:17.560 --> 10:20.740
hyperperameter를 위한 hyperperameter 훈련용으로요

10:20.980 --> 10:23.230
세 개 개 개혁에 사용할 계획이에요

10:23.230 --> 10:25.060
한 명만 해도 돼요

10:25.270 --> 10:30.580
한 번 만든 양이면 완벽한 결과가 나올 거예요

10:30.580 --> 10:32.980
1만 6천 달러예요

10:33.130 --> 10:36.580
같이 필요하실 것 같아요

10:36.580 --> 10:39.760
끝내주는 A100 박스로요?

10:39.760 --> 10:42.790
16번씩 포장할 수 있어요

10:42.790 --> 10:49.510
이게 최대 시퀀스 길이인 걸 고려하면 전부 밀어넣을 수 있어요 40GB 메모리에

10:49.510 --> 10:51.760
겨우 넣을 수 있죠

10:51.820 --> 10:58.210
하지만 T4처럼 낮은 박스를 사용하고 싶다면 이 제품을 사용해

10:58.210 --> 11:00.820
보는 게 좋을 거예요

11:00.850 --> 11:01.480
더 높이 해봐요

11:01.480 --> 11:04.300
GPU 메모리가 더 많으면 get get은 여전히 공짜죠

11:04.480 --> 11:09.340
배치 사이즈를 위해 파워 2를 쓰는 게 관례예요

11:09.340 --> 11:12.640
1, 2, 4, 8, 16명이요

11:12.820 --> 11:20.290
이론상으로는 2의 제곱으로 설정했을 때 GPU 성능이

11:20.290 --> 11:27.160
더 좋아진다는 여러 가지 불확실한 증거가 있는데요

11:27.250 --> 11:33.370
하지만 비트에 관한 데이터는 항상 좀 모호해요

11:33.370 --> 11:40.240
일반적으로 말하자면 GPU 위에 좀 더 많은 비트를 밀어 넣을 수 있다면

11:40.240 --> 11:41.800
그렇게 해야죠

11:41.800 --> 11:44.860
그래서 3개 분량도 주저 없이 만들 수 있어요

11:44.890 --> 11:48.940
GPU 안에 넣을 수 있다면 배치 크기 2보다 더 빨리 실행될 거예요

11:49.000 --> 11:54.820
비트가 더 잘 맞는다면 4개가 더 효율적일 거예요

11:54.820 --> 11:59.860
일반적으로 권고하자면 GPU 크기에 맞는 걸 선택해서 그걸로 시작하고

11:59.860 --> 12:02.080
상당한 공간을 확보하세요

12:02.080 --> 12:08.320
저처럼 돈을 펑펑 쓰실 게 아니라면 16단계 그러데이션 누적 과정을 거치세요

12:08.320 --> 12:09.430
저번에 설명했잖아요

12:09.430 --> 12:13.570
메모리 증진에 도움이 되는 것 같아요

12:13.600 --> 12:16.760
그래도 되지만 전 한 명과 함께 지내고 있어요

12:16.760 --> 12:19.040
2-4시에 시도해 보세요

12:19.340 --> 12:25.670
학습률은 정말 중요합니다 올바른 방향으로 나아가기 위해 최적화할 때

12:25.670 --> 12:29.210
얼마나 걸음을 내딛는지가 중요하죠

12:29.210 --> 12:34.250
학습률은 실험해 봐야 할 아주 중요한 하이퍼파라미터예요

12:34.370 --> 12:40.130
그리고 이런 질문에는 정답이 없어요

12:40.160 --> 12:42.830
학습률이 너무 높거나 너무 낮을 수 있어요

12:42.860 --> 12:46.220
손으로 물결무늬를 만들 거예요 비트를 입어요

12:46.250 --> 12:52.310
다시 말씀드리지만 여러분이 이루고자 하는 것은 모델이 사라졌다고 가정해

12:52.310 --> 12:55.910
보세요 이렇게 크게 움푹 들어간 부분이죠

12:55.910 --> 12:59.540
이 계곡을 찾고 저 계곡을 찾아야 해요

12:59.540 --> 13:05.330
이 계단을 오르다가 골짜기 방향을 따라 내려가면 골짜기

13:05.330 --> 13:07.460
바닥에 닿을 거예요

13:07.460 --> 13:13.520
학습 속도가 너무 높으면 계곡을 넘었다 다시 돌아와야 해요 계곡 아래로 내려가면

13:13.520 --> 13:14.870
안 되죠

13:14.870 --> 13:16.770
학습률이 너무 낮으면요

13:16.800 --> 13:22.170
작은 걸음을 내디디면서 계곡으로 가는 속도가 느려질 수도 있어요

13:22.200 --> 13:24.810
작은 보폭 두 개에는 또 다른 문제가 있어요

13:24.810 --> 13:31.560
하나의 큰 계곡이 아니라 작은 계곡이 있고 큰 계곡이 있다고 가정해 보죠

13:31.590 --> 13:37.410
학습률이 너무 낮으면 작은 발걸음을 떼다가 저 작은 계곡으로 가라앉을 수도

13:37.410 --> 13:38.010
있어요

13:38.010 --> 13:43.650
작은 걸음을 내디딜 때마다 작은 계곡의 두 벽을 따라 올라가는데 그 계곡을

13:43.650 --> 13:45.120
벗어나지 못해요

13:45.180 --> 13:50.340
그래서 바로 옆에 훨씬 더 큰 밸리가 있다는 걸 전혀 알 수 없죠

13:50.730 --> 13:55.860
그게 핵심적인 문제죠

13:55.860 --> 13:59.040
학습 속도가 너무 낮으면 흔한 문제예요

13:59.070 --> 14:02.400
사람들은 그걸 지역 내 최소량에 갇힌 거라고 하죠

14:02.400 --> 14:05.370
이건 최소라고 하는데 여러분이 있는 곳의 로컬이죠

14:05.370 --> 14:10.770
글로벌 최소치를 못 찾았군요 이 아래쪽에 있는 거요 로컬 최소치에

14:10.770 --> 14:12.780
갇혀 있으니까요

14:12.780 --> 14:16.290
학습률이 너무 낮으면 이런 문제가 생겨요

14:16.680 --> 14:19.210
멋진 트릭이 있어요

14:19.270 --> 14:25.390
학습률은 0으로 정했어요 00011 곱하기 10은 4죠

14:25.660 --> 14:30.820
학습률 스케줄러라는 게 있는데 이 프로그램은 학습률을

14:30.820 --> 14:36.220
다양하게 해서 세 개 시대에 걸쳐 학습률을 점점 줄여

14:36.250 --> 14:40.870
줍니다 결국 0이 될 때까지요

14:41.200 --> 14:42.340
그걸 줄 수 있어요

14:42.370 --> 14:45.700
이 중 하나를 선택하면 다른 모양을 줄 수 있어요

14:45.700 --> 14:53.950
코사인은 아주 좋은 제품이에요 학습 속도가 아주 느리게 시작하지만 점차 감소하고

14:53.950 --> 14:55.390
있어요

14:55.390 --> 14:57.250
끝에는 꼬리가 보여요

14:57.400 --> 14:59.650
곧 시각적으로 보실 거예요

14:59.650 --> 15:01.960
아주 좋은 기술이에요

15:02.110 --> 15:07.510
최종 학습률 매개변수는 웜업 비율이라고 하는데요 훈련 과정

15:07.510 --> 15:13.750
초기에 모델이 처음 데이터 포인트 몇 개에서 배울 게 많아서 불안정하다는

15:13.750 --> 15:15.190
뜻이죠

15:15.190 --> 15:20.020
처음에 학습률이 높으면 위험합니다 모든 게 뒤죽박죽이 되거든요

15:20.020 --> 15:27.190
워밍업 비율은 낮은 학습율로 시작해서 최고가 될 때까지 워밍업하는

15:27.190 --> 15:28.330
거예요

15:28.330 --> 15:31.960
이제 코사인 트레일을 만들어 보죠

15:32.230 --> 15:33.550
시각적으로도 볼 수 있어요

15:33.550 --> 15:34.420
그래야 말이 되죠

15:34.420 --> 15:36.880
하지만 이건 아주 합리적인 설정이에요

15:36.880 --> 15:41.980
하지만 학습 시작률이 높거나 낮거나 다양한 스케줄러 유형을 실험할 수

15:41.980 --> 15:42.700
있어요

15:43.090 --> 15:45.760
마지막으로 최적화 장치예요

15:45.910 --> 15:54.430
여기 peded Adam w를 선택할게요. W는 무게 감쇠 a peded Adam w 32 비트라는 뜻이에요.

15:54.460 --> 15:57.940
융합이 잘되는 최적화 장치죠

15:57.940 --> 16:04.480
최적의 장소를 찾는 건 잘하지만 메모리 소모에 따른 대가가 따르죠

16:04.660 --> 16:11.260
여기 링크를 걸어 놨어요 다양한 최적화 방법을 소개한 We't go에 관한 글이에요

16:11.320 --> 16:15.910
트랜스포머에 가장 많이 나오는 건 애덤과 애덤이에요

16:15.910 --> 16:25.310
애덤이 아주 잘하네요 이전 평균 그러데이션을 저장하고 있거든요 최근 그러데이션보다

16:25.550 --> 16:28.820
더 잘 활용하고 있어요

16:28.820 --> 16:32.420
하지만 그걸 저장하면 추가 메모리 공간이 필요해요

16:32.420 --> 16:34.640
비트는 좀 과한 메모리예요

16:34.640 --> 16:41.990
메모리가 부족하다면 더 싸고 욕심이 덜한 최적화 장치를 선택해 메모리를 저장하세요

16:41.990 --> 16:48.590
하지만 결과는 pageed Adam w보다 약간 나쁠 수도 있어요

16:48.620 --> 16:51.680
마지막으로 행정적인 설정이 있죠

16:51.710 --> 16:56.600
이 단계의 수는 제조 단계를 나타내요

16:56.630 --> 17:01.310
진척을 줄여 무게와 편견을 줄여 현재 상황을 보여 주죠

17:01.460 --> 17:10.460
이건 몇 단계에 걸쳐 허브로 모델을 업로드하고 적절한 버전을 저장하는 단계죠

17:10.460 --> 17:17.090
무게와 편향으로 로깅을 하는지 아닌지를 보여줍니다 매개 변수를 보여드린 거죠

17:17.090 --> 17:21.770
다음엔 트레이너에 대해 제대로 얘기해보죠 Get it
