WEBVTT

00:00.650 --> 00:06.890
넷째 주 둘째 날입니다 순위표를 더 살펴보죠 오늘 하루가 끝날 때쯤이면 기대했던

00:06.890 --> 00:11.690
것보다 더 많은 걸 알게 될 겁니다 순위표, 수치 벤치마크,

00:11.690 --> 00:14.060
경기장 등이요

00:14.060 --> 00:20.420
하지만 이 작업은 매우 유용하고 중요합니다 작업에 적합한 LLM을 고르는 법을 이해하는

00:20.420 --> 00:21.800
것이니까요

00:21.830 --> 00:31.820
자, 오늘의 계획은 지난 시간에 기본 특성과 벤치마크를 이용한 LM 비교를 다뤘는데요

00:31.850 --> 00:38.330
오늘 여러분은 오픈 LLM의 순위표 너머를 보게 될 겁니다 다른 순위표에서 얼굴을 맞대고 인사하는

00:38.330 --> 00:41.750
것에서부터 비교와 평가의 무대까지 말이죠

00:42.320 --> 00:50.480
또한 llms의 실제 유스케이스를 통해 상업적 문제를 해결할 겁니다 여러분에게

00:50.600 --> 00:53.420
도움이 될 것 같군요

00:53.450 --> 00:58.700
빠르게 하겠습니다 요즘은 다들 많이 접하셨겠지만 생각할

00:58.700 --> 01:05.960
거리를 줄 겁니다 요약하자면 프로젝트에 맞는 LLM을 선택할 수 있도록 지원해주는

01:05.960 --> 01:07.250
거죠

01:08.000 --> 01:16.190
그래서 6개의 필수 리더보드를 투어에 쓸 거예요 H깅페이스와 그 너머를 통해 보실

01:16.430 --> 01:17.360
수 있죠

01:17.390 --> 01:18.230
포옹하는 얼굴요

01:18.260 --> 01:23.690
LM을 비교해 보죠 첫 번째는 이미 봤던 H깅페이스 OpenAI 리더보드

01:23.900 --> 01:25.190
비교고요

01:25.220 --> 01:31.280
구 버전과 새 버전이 있는데 더 어려운 측정법이죠

01:31.310 --> 01:38.660
물론 오픈 소스 모델이지만 오픈 소스 모델을 비교하기 위한 곳이죠

01:39.230 --> 01:46.550
H깅페이스에 빅 코드라는 leaderboard도 있어요 코드를 생성하기 위해

01:46.550 --> 01:50.420
특별히 디자인된 모델들을 비교하는 거죠

01:50.420 --> 01:56.060
잠시 후 그 예시를 살펴보고 모델을 어떻게 평가하는지 살펴보죠

01:56.480 --> 02:03.530
LM 향수 보드라는 것이 있는데 H징페이스 보드로서

02:03.530 --> 02:11.090
성능과 정확성, 실제 속도와 연산 비용을 모두 고려하죠

02:11.090 --> 02:14.720
이건 다른 차원을 보는 아주 중요한 거예요

02:14.720 --> 02:20.040
기본 속성을 살펴보고 있어요 여러분을 위해 리소스로 가는 것이

02:20.040 --> 02:25.560
될 겁니다 특히 오픈 소스 모델과 폐쇄 소스 모델을 비교해 추론할 때를

02:25.560 --> 02:27.180
생각해 보세요

02:27.420 --> 02:29.850
포옹하는 얼굴 보드도 있어요

02:29.850 --> 02:31.110
포옹하는 표정에는 할 게 많아요

02:31.110 --> 02:31.920
더 있어요

02:31.950 --> 02:37.260
여러 사업 분야에 맞게 디자인된 리더 보드가 있어요

02:37.260 --> 02:44.160
여러분이 보실 것은 의료용 리더보드입니다 의료용 사용 사례를 위해 설계된

02:44.160 --> 02:44.850
것이죠

02:44.850 --> 02:47.940
다른 언어에 대한 leaderboard도 볼 수 있어요

02:47.970 --> 02:49.380
포르투갈인은 한 명이에요

02:49.380 --> 02:50.190
방금 봤어요

02:50.190 --> 02:50.820
보여드릴게요

02:50.820 --> 02:57.150
사용 사례에 따라 다양한 언어별 leaderboard가 있어요

02:57.150 --> 02:59.670
순위표에 잘 맞을지도 모르겠네요

03:00.300 --> 03:02.970
발하임의 순위표를 보도록 하죠

03:02.970 --> 03:06.720
기억하실지 모르겠지만 초기에 잠깐 살펴봤어요

03:06.720 --> 03:12.960
여러 개의 leaderboard가 있는 아주 유용한 리소스죠 오픈소스와 폐쇄소스가 있어요

03:12.960 --> 03:17.910
모든 모델이 한자리에 모일 수 있는 곳이죠

03:17.910 --> 03:24.990
또 유용한 테이블도 있어요 비용이요, API 비용과 연락처 창 길이

03:25.080 --> 03:31.840
책갈피에 추가할 만한 또 다른 장소죠 모든 정보가 한 곳에

03:31.840 --> 03:35.440
있는 몇 안 되는 곳이니까요

03:36.040 --> 03:44.140
마지막 순위표는 씰이라고 해요 전문가의 기술을 평가하는 거죠 순위표 세트에

03:44.140 --> 03:48.190
항상 새로운 순위표를 추가해요

03:48.190 --> 03:52.210
그러니 책갈피로 삼을 만한 또 다른 책이죠

03:52.210 --> 03:58.840
여러분의 책갈피가 가득 찰 거예요. 왜냐하면 좋은 리소스들이니까요. 나중에 사용해야 할 때

03:58.840 --> 04:00.850
저에게 고마워할 거예요.

04:02.350 --> 04:09.790
챗봇 아레나라는 것도 살펴볼 겁니다 LMS 챗봇 아레나죠

04:09.790 --> 04:12.100
환상적인 자원이죠

04:12.100 --> 04:19.390
채팅 사용 지시 사례를 특히 보고 있어요 모델이 채팅하는 기능이죠

04:19.390 --> 04:26.080
하지만 이건 일종의 기준 테스트가 아니라 채팅으로 지시하는

04:26.080 --> 04:34.840
모델에 어떤 모델이 더 적합한지 인간의 판단을 통해 결정하자는 거죠

04:34.990 --> 04:46.870
모델 A와 B 중 하나를 선택하는 건 질적인 결정이에요 두 모델과 함께 대화하면서요

04:46.870 --> 04:52.810
어떤 모델이 어떤 모델인지 모르는 블라인드 테스트예요 모르는 채로 투표해야 하죠

04:53.020 --> 05:01.420
모델에게 Elo 점수를 주는데 앞서 언급한 대로 여러 번의 인체

05:01.420 --> 05:08.500
실험을 통해 동료들과 비교해 종합 순위를 매겨요

05:08.500 --> 05:10.330
이걸 보죠

05:10.360 --> 05:12.700
Get it, Get it, Get it, Get it, Get it, Get it, Get, get 우리가 직접 투표를 할 거예요

05:13.300 --> 05:18.580
마지막으로 상업적 유스 케이스를 살펴볼 겁니다 법과 재능,

05:18.610 --> 05:23.050
코드 의료와 교육 llms의 실태를 살펴볼 거예요

05:23.050 --> 05:26.950
물론 이건 10배는 더 클 거예요

05:26.980 --> 05:34.060
림은 모든 비즈니스 수직에 영향을 미치고 있습니다. 하지만 몇 가지를

05:34.060 --> 05:37.270
보고 감을 잡는 것도 유용하죠.

05:37.270 --> 05:39.610
지금 다 할 거예요

05:39.610 --> 05:45.370
책갈피도 꽂아두시고요 유용한 걸 많이 보시거든요
