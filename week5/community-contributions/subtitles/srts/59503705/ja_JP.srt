WEBVTT

00:00.950 --> 00:05.870
そして今､ qとqラウラの量子化について話している｡ 

00:05.900 --> 00:07.730
Qは量子化された量子化の略｡ 

00:07.760 --> 00:08.420
ローラ

00:08.420 --> 00:12.620
確か3週目に量子化について少し触れた｡ 

00:12.620 --> 00:14.480
だから､ 覚えていることもあるだろう｡ 

00:14.480 --> 00:17.540
でも､ これからは本音で話そう｡ 

00:18.080 --> 00:20.630
問題はここからだ｡ 

00:20.630 --> 00:28.700
我々はこのような小さなモデル､ つまり80億パラメータ版のラマ3で作業している｡  1.

00:28.700 --> 00:36.950
私たちは､ より少ない次元の行列を扱うことで､ より多くのメモリを確保できるよう､ ローラという賢いスキームを考え出した｡

00:37.130 --> 00:42.740
しかし問題は､ そのベースモデルでさえ､ その小型版､ 80億版でさえ､

00:42.740 --> 00:46.730
32GBのラムを消費するということだ｡

00:46.730 --> 00:52.880
これは80億個の浮動小数点数で､ それぞれ32ビット長だ｡ 

00:53.000 --> 00:56.480
それでGPUがいっぱいになるんだ｡ 

00:56.510 --> 01:05.530
実際､ T4ボックスで使用される安価なGPUには15GBのメモリしか搭載されていないため､ ベースモデル自体にすら適合しない｡

01:05.530 --> 01:08.740
すぐにメモリ不足になるから､ それは問題だ｡ 

01:08.770 --> 01:13.570
ローラは僕らにとって物事をより良くするのにとても役に立つが､ ベースモデル自体をメモリに収めることもできないので､

01:13.570 --> 01:15.880
十分とは言えない｡

01:15.880 --> 01:23.920
というのも､ 80億という数字は､ ある世界では小さなモデルサイズかもしれないが､ それでも膨大な数のパラメーターがあることに変わりはないからだ｡

01:24.190 --> 01:27.940
そして､ それが私たちに課題を与えている｡ 

01:28.240 --> 01:31.810
そこで驚くべき発見がなされた｡ 

01:31.810 --> 01:33.970
それが私たちの仕事だ｡ 

01:34.120 --> 01:37.870
そして､ 最初は､ それは本当であるにはあまりにも良すぎるように聞こえる｡ 

01:37.900 --> 01:40.480
これはケーキを食べながらケーキを食べる方法のようだ｡ 

01:40.480 --> 01:41.950
そして､ それはある種のものだ｡ 

01:42.190 --> 01:51.310
つまり､ 80億のパラメーターがあるのだから､ 80億のパラメーターを使えばいいという考え方もある｡ 

01:51.310 --> 01:58.600
40億のパラメーターを持つモデルのようにパラメーターを少なくしようとすれば､ モデルのパワーの多くを失うことになる｡

01:58.600 --> 02:01.900
この80億のパラメーターは､ 私たちにたくさんのノブを与えてくれる｡ 

02:01.930 --> 02:04.270
それはこの非常に巧妙な建築にある｡ 

02:04.270 --> 02:05.800
大きな力を与えてくれる｡ 

02:05.830 --> 02:06.370
分かった｡ 

02:06.370 --> 02:09.080
だから､ パラメーターの数を減らすのはやめよう｡ 

02:09.080 --> 02:15.200
そうする代わりに､ それぞれのパラメーターの精度を下げよう｡ 

02:15.230 --> 02:21.050
非常に細かく砥石を回すことができる代わりに､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ カチッ､

02:21.050 --> 02:25.100
カチッ､ カチッ､ カチッ､ カチッ､ カチッ､ といくつかの設定を通過させることができるようにするんだ｡

02:25.310 --> 02:32.450
それで､ それぞれのウエイトの精度を下げて､ ウエイトの数は同じにしようと考えたんだ｡

02:32.480 --> 02:37.010
論理的に考えればそうかもしれないが､ 情報量を減らしているだけだ｡ 

02:37.010 --> 02:43.730
確かに､ 情報量が半分になれば､ ウエイトの数が半分になるのと似たようなものだろう｡

02:43.760 --> 02:46.280
そうではないことがわかったんだ｡ 

02:46.280 --> 02:52.520
どんな理由であれ､ 精度を下げればニューラルネットワークの質は多少落ちるが､

02:52.520 --> 02:55.100
思ったほどではない｡

02:55.100 --> 02:57.920
まだ多くのパワーを保っている｡ 

02:58.160 --> 03:06.560
そして､ これは同じパラメーター数でより大きなモデルをメモリにフィットさせるための素晴らしいトリックであることがわかった｡

03:06.560 --> 03:10.220
精度が低いというだけで､ より少ないメモリしか消費しない｡ 

03:10.580 --> 03:14.210
だから､ 驚くほどうまく機能している｡ 

03:14.300 --> 03:21.020
実際､ 通常の32ビットの浮動小数点数を8ビットに減らしても､

03:21.020 --> 03:26.000
十分なパフォーマンスを得ることができる｡

03:26.000 --> 03:29.540
そして､ ここからが本当にクレイジーに聞こえ始めるところだ｡ 

03:29.630 --> 03:32.480
4ビットまで減らすことができる｡ 

03:32.480 --> 03:35.210
つまり､ それぞれの数字は単なる4ビットの数字なのだ｡ 

03:35.240 --> 03:42.950
整数の観点から考えるなら､ それぞれの数字が0から15まで進んでいるようなものだ｡

03:43.160 --> 03:45.020
ただ､ 整数で｡ 

03:45.560 --> 03:47.510
それだけ精度が低いということだ｡ 

03:47.510 --> 03:51.620
16の設定があるだけのクリックとかね｡ 

03:52.010 --> 03:55.190
うーん､ それでもかなりいいパフォーマンスが得られる｡ 

03:55.340 --> 03:55.670
もちろんだ｡ 

03:55.670 --> 04:00.770
品質が落ちるのは少しだが､ ほんの少しだ｡ 

04:01.010 --> 04:03.200
それで､ これが直感だったんだ｡ 

04:03.200 --> 04:10.310
そしてもちろん､ これによって必要なメモリが劇的に削減され､ より大きなモデルをメモリに収めることができるようになる｡

04:10.790 --> 04:14.420
細かい技術的なことをいくつか挙げておこう｡ 

04:14.780 --> 04:17.810
そのひとつが､ 先ほど話したクリックスイッチだ｡ 

04:17.810 --> 04:20.510
0から15までの数字のようなものだと考えればいい｡ 

04:20.510 --> 04:31.940
一般的には､ 整数として解釈されることはなく､ 4ビットが浮動小数点数として使用される｡

04:31.940 --> 04:34.970
そして､ 現実の例を見ればわかるだろう｡ 

04:35.360 --> 04:39.740
そして､ もうひとつ指摘しておきたいのは､ カラーが初めて登場した当初は､

04:39.770 --> 04:53.390
私が理解していなかったことなのですが､ これはベースモデルを量子化しているのであって､ ローラ・アダプターを量子化しているわけではないということです｡

04:53.570 --> 05:02.300
つまり､ ベースモデルの精度を下げて､ この巨大で素晴らしいベースモデルをメモリに収まるようにしようという話なんだ｡

05:02.300 --> 05:04.490
それがクー・ロラなんだ｡ 

05:04.520 --> 05:05.840
それが量子化だ｡ 

05:05.840 --> 05:10.250
あと少しでラボで見ることができるのだから､ よりリアルに感じられるだろう｡ 

05:10.250 --> 05:16.640
その前に､ 次のビデオで3つの重要なハイパーパラメーターについてお話ししたいと思います｡ 
