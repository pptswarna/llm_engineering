WEBVTT

00:00.950 --> 00:05.870
이제 퀀타이즈 얘길 하죠 q&amp;q 로라

00:05.900 --> 00:07.730
Q는 퀀타이즈된 퀀타이즈죠

00:07.760 --> 00:08.420
로라예요

00:08.420 --> 00:12.620
3주 차에 퀀타이즈를 잠깐 언급했었죠

00:12.620 --> 00:14.480
그럼 기억날지도 모르겠네요

00:14.480 --> 00:17.540
이제 진짜 얘기를 해보죠

00:18.080 --> 00:20.630
문제는 이거예요

00:20.630 --> 00:28.700
우린 작은 모델로 작업하고 있어요 80억 매개 변수 버전의 llama 3이죠 1번요

00:28.700 --> 00:34.580
이 기발한 계획도 로라가 생각해 냈어요 소차원 행렬을 연구하면 메모리를 더 많이

00:34.940 --> 00:36.950
얻을 수 있죠 Get up

00:37.130 --> 00:42.740
그런데 문제는 기본 모델조차도 작은 버전이라도 80억 버전이라도

00:42.740 --> 00:46.730
램 32GB를 쓴다는 거예요

00:46.730 --> 00:52.880
80억 개의 부동 소수점 수죠 각각 32비츠 길이로요

00:53.000 --> 00:56.480
GPU 안을 채울 거예요

00:56.510 --> 01:03.380
T4 박스에 사용할 저렴한 GPU는 메모리가 15GB밖에 안 돼요 기본 모델

01:03.380 --> 01:05.530
자체에도 맞지 않죠

01:05.530 --> 01:08.740
메모리가 바로 바닥날 거예요 그게 문제죠

01:08.770 --> 01:13.570
로라는 우리에게 더 나은 환경을 만들어 주지만 베이스 모델 자체를 메모리에

01:13.570 --> 01:15.880
넣을 수 없으니 충분하지 않아요

01:15.880 --> 01:23.920
80억이면 어떤 세계에선 작은 모형 크기일 수 있지만 그래도 엄청난 수의 매개 변수니까요

01:24.190 --> 01:27.940
그래서 어려운 점이 많아요

01:28.240 --> 01:31.810
그래서 이 놀라운 발견이 이루어졌죠

01:31.810 --> 01:33.970
그게 우리가 작업할 거예요

01:34.120 --> 01:37.870
처음에는 너무 좋아서 믿기 힘들 정도였죠

01:37.900 --> 01:40.480
두 마리 토끼를 다 잡을 수 있는 방법 같네요

01:40.480 --> 01:41.950
그런 셈이죠

01:42.190 --> 01:51.310
그래서 어떤 사람들은 80억 개의 매개 변수가 있다고 했죠

01:51.310 --> 01:57.370
40억 개의 매개 변수 모델처럼 적은 매개 변수를 가지려고 한다면 모델의 힘을

01:57.370 --> 01:58.600
많이 잃게 돼요

01:58.600 --> 02:01.900
그 80억 개의 매개 변수 덕분에 돌릴 노브가 많아요

02:01.930 --> 02:04.270
이 기발한 건축물에 그 점이 있어요

02:04.270 --> 02:05.800
큰 힘을 주죠

02:05.830 --> 02:06.370
좋아요

02:06.370 --> 02:09.080
매개 변수의 수를 줄이지 말죠

02:09.080 --> 02:15.200
그렇게 하는 대신 각 매개 변수의 정밀도를 낮추죠

02:15.230 --> 02:21.050
곱게 갈린 바퀴를 통과하는 게 아니라 몇 가지 설정을 통해

02:21.050 --> 02:25.100
클릭, 클릭, 클릭하게 하는 거죠

02:25.310 --> 02:31.070
그래서 생각한 건데 무게의 정밀도는 줄이고 무게의 개수는 그대로

02:31.070 --> 02:32.450
두기로 했어요

02:32.480 --> 02:37.010
논리적으로 생각할 수 있지만 정보의 양을 줄이고 있어요

02:37.010 --> 02:41.390
정보가 절반만 있어도 무게의 수가 절반인

02:41.390 --> 02:43.730
것과 비슷할 거예요

02:43.760 --> 02:46.280
알고 보니 그렇지 않더군요

02:46.280 --> 02:52.520
어떤 이유에서든 정밀도를 낮추면 신경망의 질이 어느 정도 떨어지지만 생각만큼 떨어지진

02:52.520 --> 02:55.100
않아요. Get it.

02:55.100 --> 02:57.920
아직도 그 힘을 간직하고 있죠

02:58.160 --> 03:04.700
이건 메모리에 같은 수의 매개 변수로 더 큰 모델을 넣게 해주는 훌륭한

03:04.730 --> 03:06.560
트릭이에요

03:06.560 --> 03:10.220
정밀도가 낮으면 메모리 소모가 less예요

03:10.580 --> 03:14.210
놀라울 정도로 잘 작동하죠

03:14.300 --> 03:21.020
여러분이 평소에 가지고 있는 32비트 부동점 수를 취해서 8비트

03:21.020 --> 03:26.000
숫자로 줄여도 여전히 좋은 성능을 낼 수 있어요.

03:26.000 --> 03:29.540
그리고 여기서부터 정말 미친 소리처럼 들리기 시작하죠

03:29.630 --> 03:32.480
4분의 1로 줄일 수 있어요

03:32.480 --> 03:35.210
각 숫자는 네 비트짜리 숫자죠

03:35.240 --> 03:40.550
정수의 관점에서 생각해 보면 각각의 숫자는 0에서

03:40.550 --> 03:42.950
15까지밖에 없어요

03:43.160 --> 03:45.020
그냥 숫자만 따져서요

03:45.560 --> 03:47.510
그 정도로 정확성이 낮죠

03:47.510 --> 03:51.620
클릭은 16개의 설정을 가지고 있어요

03:52.010 --> 03:55.190
그래도 여전히 좋은 연기를 보여줬어요 get it get it

03:55.340 --> 03:55.670
네

03:55.670 --> 04:00.770
비트의 품질이 떨어지긴 했지만 아주 조금이에요

04:01.010 --> 04:03.200
그게 바로 직관이었어요

04:03.200 --> 04:09.200
이렇게 하면 메모리 요구 사항이 급격히 줄고 메모리에서 더 큰 모델에

04:09.200 --> 04:10.310
맞게 되죠

04:10.790 --> 04:14.420
몇 가지 사소한 기술적인 사항을 언급할게요

04:14.780 --> 04:17.810
그 중 하나는 방금 말씀드린 클릭 스위치예요

04:17.810 --> 04:20.510
0부터 15까지의 숫자로 생각해 보세요

04:20.510 --> 04:28.160
일반적으로 4비트는 정수로 해석되지 않지만 4비트는 세분성이

04:28.160 --> 04:31.940
낮은 부동점수로 간주되거든요

04:31.940 --> 04:34.970
예제에서 실제로 그걸 보실 수 있어요

04:35.360 --> 04:39.740
또 하나 지적하고 싶은 건 색깔이 처음 나왔을 때

04:39.770 --> 04:47.030
제가 이해 못 했던 건데요 이건 베이스 모델을 수량화하는 거지 로라 어댑터를

04:47.030 --> 04:53.390
수량화하는 게 아니에요 보다시피 여전히 32 비트 플로트죠

04:53.570 --> 04:59.630
베이스모델의 정밀도를 줄이는 것에 대해 얘기하고 있어요 이 거대하고 훌륭한 베이스모델을

04:59.630 --> 05:02.300
메모리에 넣어야 해요

05:02.300 --> 05:04.490
저게 쿠 로라예요

05:04.520 --> 05:05.840
그게 퀀타이즈예요

05:05.840 --> 05:10.250
잠시 후 실험실에서 보면 더 진짜 같을 거예요

05:10.250 --> 05:16.640
하지만 먼저 다음 비디오에서 중요한 하이퍼파라미터 세 가지를 말씀드릴게요
