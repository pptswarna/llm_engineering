WEBVTT

00:00.050 --> 00:02.540
これからモデルについて少し話をしようと思う｡ 

00:02.570 --> 00:10.190
よく耳にする言葉にフロンティアモデルというものがあるが､ これは現在可能なことを開拓しているllms､

00:10.190 --> 00:13.760
つまり可能性のある最大のモデルを指す｡

00:13.760 --> 00:21.230
そして､ フロンティア・モデルと言われる場合､ クローズド・ソース・モデル､ GPTやクロードのような有料モデルを指すことが多い｡

00:21.260 --> 00:28.160
実際､ 最大で最強のオープンソース・モデルを指してフロンティア・モデルと言うこともある｡

00:28.160 --> 00:30.770
だから文脈によっては､ どちらの意味にもなる｡ 

00:30.770 --> 00:35.300
しかし､ まずはクローズドソースのフロンティアモデルについて話そう｡ 

00:35.780 --> 00:38.450
また､ これらはスーパースケーラーと呼ばれることもある｡ 

00:38.480 --> 00:42.140
モデルの中で最も大きく､ スケールが大きい｡ 

00:42.140 --> 00:47.120
まず､ 紹介するまでもないだろう､ OpenAIのGPTだ｡ 

00:47.210 --> 00:54.020
2022年末にChatGPTが登場したとき､ そのパワーは私たちを驚かせた｡ 

00:54.350 --> 00:56.210
あなたもよくご存知でしょう｡ 

00:56.240 --> 00:58.370
アントロピックのクロード｡ 

00:58.490 --> 01:01.700
AnthropicはOpenAIのライバルだ｡ 

01:01.730 --> 01:02.840
それはよく知られていることだ｡ 

01:02.840 --> 01:06.530
そしてクロードは､ 通常データサイエンティストに好まれるものだ｡ 

01:06.560 --> 01:12.230
しかし､ クロードとGPTは今､ 首位争いを繰り広げている｡ 

01:12.260 --> 01:14.060
クロードがやや優勢｡ 

01:14.150 --> 01:17.090
ジェミニはグーグルの参入企業である｡ 

01:17.330 --> 01:21.800
グーグルにはジェンマもいる｡ 

01:21.830 --> 01:23.900
オープンソースのバリエーションもある｡ 

01:23.900 --> 01:28.370
そしてコマンドRは､ コヒーレで見たことがあるかもしれないし､ ないかもしれない｡ 

01:28.400 --> 01:29.750
カナダのAI企業｡ 

01:29.750 --> 01:37.700
そして "perplexity "は､ 実際に他のモデルのいずれかを使用することができる検索エンジンであるが､ 同時にモデル自体も持っている｡

01:37.730 --> 01:41.210
これが大きなフロンティアモデルだ｡ 

01:41.420 --> 01:44.780
そして､ オープンソースのいくつかのモデルについて話そう｡ 

01:44.780 --> 01:52.610
というのも､ metaはオリジナルのllamaをオープンソース化することで､

01:52.610 --> 02:00.080
オープンソースLMSの分野を切り開いたからだ｡

02:00.200 --> 02:07.010
フランスのミストラル社のミストラルというモデルがある｡

02:07.010 --> 02:09.770
複数の小型モデルが含まれている｡ 

02:10.160 --> 02:14.210
クエンは､ 私たちがオバマとプレーしていたときに話したモデルだ｡ 

02:14.210 --> 02:16.010
強力なモデルだ｡ 

02:16.010 --> 02:26.750
アリババクラウドは本当に素晴らしく､ 私たちも時々Quanを使うつもりです｡

02:26.780 --> 02:34.490
私が述べたGemmaはGoogleの小型モデルで､ FiはMicrosoftの小型オープンソースモデルだ｡ 

02:35.510 --> 02:38.990
だから､ これは混乱しているし､ 超重要なことなんだ｡ 

02:38.990 --> 02:45.470
すでにご存知の方もいらっしゃるかもしれませんが､ 何人かの方にとっては､ ずっと気になっていたことかもしれません｡

02:45.470 --> 02:47.840
これだけははっきりさせておきたい｡ 

02:47.840 --> 02:54.020
モデルにはまったく異なるアプローチの使い方があり､

02:54.020 --> 02:59.180
その違いを理解することが重要だ｡

02:59.180 --> 03:01.730
ここで何が起こっているのか､ 頭の中に思い浮かべてください｡ 

03:01.730 --> 03:21.530
ChatGPTはウェブフロントエンドで､ チャットをしながらクラウド上で動作している何かを呼び出すことができます｡

03:21.530 --> 03:26.900
ChatGPTの場合､ もちろんクラウドやジェミニ・アドバンスなどもある｡ 

03:27.560 --> 03:29.960
クラウドAPIがある｡ 

03:29.960 --> 03:35.780
ここでもまた､ クラウド上で実行されている何かを呼び出しているわけだが､ ユーザー・インターフェースを介してではなく､

03:35.780 --> 03:38.270
コードを使っている｡

03:38.270 --> 03:45.950
Jupyterラボの要約Jupyterノートブックでやったことは､ OpenAIのAPIを呼び出すことだった｡ 

03:45.980 --> 03:52.310
私たちはOpenAIに接続し､ 彼らのAPIを呼び出していました｡ 一般的にチャット・インターフェースでは､

03:52.310 --> 04:00.380
無料のティアか､ ユーザー・インターフェースのチャットをほぼ好きなだけ使うために月額利用料を支払うかのどちらかです｡

04:00.410 --> 04:01.820
そこには限界がある｡ 

04:02.360 --> 04:04.280
APIは違う｡ 

04:04.280 --> 04:10.910
APIにはサブスクリプションも月額料金もなく､ APIリクエストのたびに料金を支払う｡

04:10.940 --> 04:12.260
有料APIの場合｡ 

04:12.290 --> 04:17.900
オープンソースの無料APIもあるので､ APIを直接呼び出すこともできる｡ 

04:17.900 --> 04:24.820
ラング・チェインのような抽象化レイヤーを提供するライブラリもあり､ ラング・チェインを使うこともできる｡

04:24.820 --> 04:27.130
そして､ その中でさまざまなAPIを呼び出すことができる｡ 

04:27.130 --> 04:31.210
そして､ それらを統一した1つのAPIを提供する｡ 

04:31.210 --> 04:34.210
ラングチェーンのようなフレームワークもある｡ 

04:34.210 --> 04:39.460
ラングチェーンを使っている人を見かけたら､ それは単にlm APIを使っているだけだ｡ 

04:39.460 --> 04:45.850
より良いユーザー､ より良いAPIインターフェイス､ より一貫性のあるものを提供するだけだ｡ 

04:46.360 --> 04:51.250
マネージドAIクラウドサービスと呼ばれるもので､

04:51.250 --> 05:00.160
アマゾンやグーグル､ マイクロソフト・アズールなどのプロバイダーと接続するものだ｡

05:00.370 --> 05:09.550
そして､ 彼らはクラウド上でモデルを実行し､ 裏側で実行できるように共通のインターフェイスを提示している｡

05:09.550 --> 05:11.680
オープンソースかもしれないし､ クローズドソースかもしれない｡ 

05:11.680 --> 05:13.510
アマゾン・ベッドロックのことも耳にするだろう｡ 

05:13.540 --> 05:14.650
それがアマゾンの提案だ｡ 

05:14.650 --> 05:18.820
Google vertex AIはGoogleとAzureのMLである｡ 

05:18.850 --> 05:21.850
マイクロソフトが提供するこの製品は､ 他の名前でも呼ばれている｡ 

05:21.850 --> 05:26.230
これがマネージドAIクラウドサービスだ｡ 

05:26.230 --> 05:30.220
しかし､ これらに共通しているのは､ ローカルでコードを書いているということだ｡ 

05:30.220 --> 05:35.860
そして､ クラウド上で稼働しているLMを呼び出す｡ それがクラウドAPIだ｡ 

05:35.980 --> 05:43.660
第3のアプローチは､ LMのコードとウェイトを自分で入手し､

05:43.660 --> 05:52.210
それを自分のマシンで､ あるいはリモートを使って実行する方法だ｡

05:52.210 --> 05:56.650
そしてここでも､ このコースでは2つの異なる方法がある｡ 

05:56.650 --> 05:58.930
そして､ 両者の違いを理解することが重要だ｡ 

05:58.930 --> 06:09.250
PythonのコードやPyTorchのコードにアクセスできるようになる｡

06:09.250 --> 06:14.770
そして､ そのモデルを使ってかなり細かい作業ができるようになり､ トークン化されたテキストのようなことができるようになり､

06:14.770 --> 06:21.520
トークンを使ってモデルを呼び出し､ 実際にモデルを操作することになります｡

06:21.520 --> 06:26.980
通常､ Google Colabのようなものを使い､

06:26.980 --> 06:37.210
クラウド上の非常に高性能なマシンでそれを実行する｡

06:37.630 --> 06:45.730
それに代わるものとして､ 人々はこのコードを高性能のC＋＋＋コードに最適化し､ コンパイルして､

06:45.760 --> 06:51.460
自分のコンピュータ上でローカルに実行できるようにした｡

06:51.460 --> 06:53.890
それがオラマだ｡ 

06:54.070 --> 06:58.900
C＋＋のコードとして､ 裏ではllama CPWというものを使っている｡ 

06:58.930 --> 07:01.420
つまり､ ローカルで実行できるということだ｡ 

07:01.420 --> 07:12.550
しかし､ 完全にコンパイルされたコードなので､ 何が起こっているのかをコントロールすることはできない｡

07:12.550 --> 07:14.650
だから､ これで少しはわかってもらえると思う｡ 

07:14.650 --> 07:19.510
細かいことは省くが､ モデルを使ってできる3つの異なる方法と､

07:19.510 --> 07:24.670
その下にあるいくつかのサブ・テクニックを紹介したい｡

07:25.210 --> 07:27.430
それを踏まえて､ これからどうするか｡ 

07:27.430 --> 07:32.080
これから練習をしますが､ オラマを使うので､ コースを通して使い続けられる､

07:32.080 --> 07:36.610
役に立つ練習になるでしょう｡

07:36.850 --> 07:42.520
では､ 早速JupyterLabでエクササイズを説明しよう｡ 
