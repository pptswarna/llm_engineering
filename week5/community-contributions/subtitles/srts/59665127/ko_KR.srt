WEBVTT

00:00.560 --> 00:02.330
안녕하세요, 여러분

00:02.330 --> 00:06.890
얼마나 신났는지 평소처럼 말하진 않을게요 저도 그렇고 당신도

00:06.890 --> 00:08.510
신났으니까요

00:08.540 --> 00:14.930
밤새도록 저처럼 달리셨을 테니 결과를 빨리 보고 싶으시겠죠

00:14.930 --> 00:17.030
하지만 할 일이 있어요

00:17.060 --> 00:24.050
우선, 이미 많은 것을 할 수 있습니다 프론티어에 반하는 코딩과 오픈 소스 모델은

00:24.050 --> 00:31.550
데이터 세트 큐레이션, 기준선 미세 튜닝 프론티어 모델과 실행을 이용하죠

00:31.550 --> 00:37.880
Q 로라, 아까 총알 두 개를 넣겠다고 했는데 여기 하나 더

00:37.880 --> 00:42.890
넣었어요 훈련 과정부터 시작하죠 Put

00:42.920 --> 00:48.320
생각해보니 훈련 과정 자체에 대해 꽤 꼼꼼하게 알고 있었어요

00:48.320 --> 00:53.330
이제 여러분은 달리기와 훈련 로라의 활약을 직접 목격한 경험이

00:53.330 --> 00:57.680
있으니 잠시 시간을 내서 제대로 설명해 드릴게요

00:57.680 --> 00:59.270
그런 기본이 있어요

00:59.300 --> 01:00.560
이미 다 알고 계실지도 몰라요

01:00.560 --> 01:05.300
그 시점에서 뭔가 알아차렸거나 이미 마주쳤을 수도 있죠

01:05.360 --> 01:09.070
어느 쪽이든 그걸 분명히 하는 게 중요해요

01:09.070 --> 01:11.140
몇 분 정도 걸릴 거예요

01:11.260 --> 01:16.300
첫 달리기 훈련을 받은 건 좋은 일이라고 생각해요 이번

01:16.300 --> 01:20.110
훈련을 통해 모든 게 잘 연결되길 바라요

01:20.140 --> 01:29.200
그런 다음 잘 조율된 모델에 대한 추론을 실행합니다 그럼 7주 차에 마무리되는 거죠 정말

01:29.200 --> 01:31.120
흥미진진해요

01:31.150 --> 01:32.890
Get it, get it, get it, get it, get, it! 자, 그럼 시작해 볼까요?

01:33.550 --> 01:40.930
제가 말씀드리고 싶은 것은 모델을 개선하는 훈련 과정으로 작업을 수행하는

01:40.930 --> 01:46.120
데 있어서 4단계를 거쳐야 한다는 점인데요

01:46.150 --> 01:52.570
첫 단계는 전진 패스라고 알려진 겁니다 일종의 실행 추론에 쓰이는 또 다른

01:52.570 --> 01:53.440
이름이죠

01:53.440 --> 02:01.480
데이터 집합에 데이터가 있는데 특정 훈련 데이터 포인트가 있고 그 훈련 프롬프트를

02:01.480 --> 02:08.590
신경망을 통해 전달해 다음 토큰에 대한 예측을 얻죠.

02:08.590 --> 02:13.510
그걸 포워드 패스라고 하죠 왜냐하면 입력값이 들어와 통과하고

02:13.510 --> 02:18.080
결과물이 변압기 끝에 튀어나온다고 생각하니까요

02:19.100 --> 02:22.700
그리고 손실 계산이라는 게 있어요

02:23.240 --> 02:25.340
이 비트에 대해선 나중에 더 얘기하죠

02:25.340 --> 02:26.810
이건 괜찮다고 하네요

02:26.810 --> 02:31.010
네트워크는 이게 출력물이라고 예상했어요

02:31.010 --> 02:35.750
사실 이게 진정한 다음 토큰이에요 우린 훈련 중이니까요

02:35.750 --> 02:41.030
데이터에서 다음에 무엇이 올지 포함하는 실제 예제들이 있어요

02:41.360 --> 02:46.610
이제 예측과 진실을 알았으니 손실을 계산할 방법을 생각해낼 수 있겠죠

02:46.610 --> 02:48.890
얼마나 틀렸는지도 알고요

02:48.920 --> 02:53.960
정확성의 반대인 걸 잃었을 때 얼마나 심각했나요?

02:54.590 --> 02:58.340
손실이 크다는 건 상황이 악화됐다는 뜻이죠

02:58.700 --> 02:59.750
이게 2단계예요

02:59.780 --> 03:01.160
손실 계산요

03:01.160 --> 03:04.580
세 번째 단계는 뒤로 가기예요

03:04.580 --> 03:06.320
다른 용어도 많이 들리죠

03:06.350 --> 03:08.900
백프롭 백 전파라고 하죠

03:09.230 --> 03:17.840
이 원리는 이 역과정에서 손실을 감안하고 신경망을 통해 되돌아보며 이런 질문을

03:17.840 --> 03:25.540
하는 겁니다 신경망의 각 매개 변수를 조금씩만 변경한다면요

03:25.570 --> 03:29.140
이 손실이 커졌을까요? 작아졌을까요?

03:29.170 --> 03:31.300
손실이 얼마나 크죠?

03:31.330 --> 03:33.100
이 무게는 어떻게 정했죠?

03:33.100 --> 03:35.980
그게 손실과 무슨 상관이죠?

03:36.130 --> 03:39.700
몸무게에 따라 감량량은 어떻게 달라지죠?

03:39.910 --> 03:43.540
그 민감한 부분을 그러데이션이라고 해요

03:43.570 --> 03:44.290
물론이죠

03:44.500 --> 03:47.230
일반적으로 수학에서 그렇듯이요

03:47.410 --> 03:56.020
각 무게의 경사도를 계산하는 과정이에요 조금만 변경해도 손실이 발생하죠

03:56.020 --> 04:01.870
그래서 모든 무게의 경사도를 계산하는 과정을 백워드

04:01.900 --> 04:04.240
패스라고 해요

04:04.570 --> 04:12.970
마지막으로 4단계 최적화는 무게 감쇠 기능을 갖춘 Adam

04:12.970 --> 04:19.180
W 최적화기로 훈련에 사용하기로 했어요

04:19.210 --> 04:24.280
최적화는 모든 그러데이션을 계산했다는 뜻이에요

04:24.400 --> 04:32.270
이제 모든 무게 비트를 조금씩 조정할 겁니다 그래야 다음번에

04:32.270 --> 04:37.850
같은 프롬프트를 받으면 좀 더 잘 되겠죠

04:37.880 --> 04:39.950
비트 박스는 좀 낮겠죠

04:40.400 --> 04:45.200
기울기 방향과 반대 방향으로 살짝 조정해서 손실을

04:45.200 --> 04:46.970
줄일 거예요

04:47.240 --> 04:52.670
학습 비율에 따라 한 단계씩 나아가는 겁니다 학습 비율이 얼마나 큰지에

04:52.670 --> 04:56.750
따라서요 이건 좀 더 나은 환경을 위해 고안된 거죠

04:56.840 --> 05:00.290
일반화할 수 있는 방법으로 항상 시도하고 싶죠

05:00.290 --> 05:04.610
이 입력 프롬프트를 정확히 해결하고 싶진 않죠

05:04.610 --> 05:11.090
이런 비트로 모델이 좀 더 나아지도록 배우길 바랄 뿐이죠

05:11.450 --> 05:17.660
이 모든 과정이 미니 배치에서 동시에 발생합니다 데이터 전체에서

05:17.690 --> 05:23.780
계속 반복되죠 데이터는 한 발생을 거치고 여러 발생을 거치면서

05:23.780 --> 05:27.740
또 다른 발생이 반복돼요

05:27.740 --> 05:33.050
그 반복적인 과정을 훈련이라고 하죠

05:33.380 --> 05:38.600
다음 비디오에서는 그걸 설명하기 위해 다이어그램을 보여드리죠
