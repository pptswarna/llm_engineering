WEBVTT

00:00.620 --> 00:07.340
이제 4비트 퀀타이즈를 살펴볼 겁니다 정밀도를 끝까지 낮추는 놀라운

00:07.340 --> 00:08.330
효과죠

00:08.330 --> 00:13.460
비트와 바이트를 이용한 퀀트 구성 이전과 아주 유사하죠

00:13.700 --> 00:17.150
하지만 이번엔 4 비트 =으로 로드하라고 했죠

00:17.150 --> 00:17.750
맞아요

00:17.780 --> 00:19.370
다른 설정도 있어요

00:19.370 --> 00:24.710
이번엔 더블 퀀트 사용이란 게 있는데 이것도 살짝 수수께끼죠

00:24.710 --> 00:30.710
모든 무게를 수량화해서 통과하게 하는 거예요 그리고 다시

00:30.710 --> 00:32.270
한번 통과하죠

00:32.270 --> 00:36.830
그렇게 함으로써 메모리를 10에서 20퍼센트 더 줄일 수 있어요

00:36.860 --> 00:38.990
비트를 더 짜내는 거예요

00:39.080 --> 00:46.940
그리고 이건 실험적으로 입증된 건데 신경망의 힘에 아주 미세한 차이를 만들어냈어요

00:46.940 --> 00:48.470
거의 공짜나 다름없죠

00:48.500 --> 00:51.020
둘 다 해당하는 상황이죠

00:51.020 --> 00:54.980
그래서 더블 퀀트를 참으로 사용하라고 권장했죠

00:55.130 --> 01:01.280
계산 dtype은 계산에 사용되는 데이터 타입에 관한 거죠

01:01.490 --> 01:06.350
일반적으로 32 비트 플로트로 작업할 수 있어요

01:06.350 --> 01:14.240
Bfloat16 데이터 유형의 플로트 이진법 16을 사용하면 훈련

01:14.240 --> 01:20.480
속도가 빨라집니다 훈련의 질을 조금만 희생해도요

01:20.630 --> 01:26.450
확실히 제가 이걸 시도했을 땐 더 빨리 실행되는 걸 봤어요 최적화 속도에서

01:26.450 --> 01:30.470
실제적인 변화는 감지할 수 없었죠

01:30.710 --> 01:32.750
그러니 이건 확실히 추천해요

01:32.750 --> 01:35.750
하지만 이것도 하이퍼 매개 변수라 실험할 수 있어요

01:35.750 --> 01:40.220
이건 4 비트 퀀트 유형이에요

01:40.220 --> 01:45.800
4 비트 숫자로 정밀도를 낮출 때 4 비트 숫자를 어떻게 해석해야

01:45.800 --> 01:46.730
할까요?

01:46.760 --> 01:54.350
4bit00부터 1111까지라고 하면 0부터 15까지 정수라고 생각할

01:54.350 --> 01:55.790
수도 있죠

01:55.820 --> 01:57.800
그것도 한 방법이죠

01:57.830 --> 02:02.690
부동점 숫자에 맞춰 지도를 만드는 게 일반적이죠

02:02.810 --> 02:08.750
이 nf4 접근법은 정규 배포된 뭔가에 매핑해요

02:08.780 --> 02:11.540
이건 아주 흔한 설정이에요

02:11.540 --> 02:13.550
제가 쓴 거예요

02:13.580 --> 02:16.010
다른 걸 해 봤는데 별로였어요

02:16.040 --> 02:18.620
일반적으로 권장하는 게 이거죠

02:18.620 --> 02:23.090
하이퍼 매개 변수라서 시행착오를 겪어도 사용 가능해요

02:23.600 --> 02:31.370
그걸 염두에 두고 퀀트 구성을 만드는데 4비트 퀀트화를 위한 아주 표준적인 퀀트 구성이죠

02:31.370 --> 02:38.570
기본 모델을 생성합니다 메모리 공간을 프린트할 거예요

02:38.570 --> 02:43.010
놀랍게도 현재는 5마리로 줄었죠 6GB요

02:43.040 --> 02:48.770
제 리소스에서 이미 보셨을 수도 있지만 기본 모델을 기억하시면

02:48.770 --> 02:54.470
실제 크기는 32GB였죠 정말 많이 줄었어요

02:54.470 --> 03:01.580
이 정도면 GPU 메모리에 저렴한 T4 박스에 쉽게 들어갈 거예요

03:01.580 --> 03:04.940
기본 모델을 보면 아키텍처가 있어요

03:04.970 --> 03:07.580
그 멍청한 농담은 다시 안 할 거예요

03:07.670 --> 03:15.920
물론 더 우람했던 80억 라마 모델의 구조와 동일하죠

03:16.280 --> 03:22.130
이 깊숙한 곳에서는 무게추의 정확도가 낮아요

03:22.130 --> 03:23.150
4비트예요

03:23.960 --> 03:24.740
네

03:24.740 --> 03:28.160
다음 비디오에선 이 시점에서 세션을 다시 시작하지 마세요

03:28.160 --> 03:34.490
이번 세션은 이대로 유지해야 합니다 다음 비디오에선 미세 조정 모델의 예제를 불러오겠습니다

03:34.490 --> 03:40.160
로라가 적응한 것이 이 구조에 어떻게 적용되는지 보죠

03:40.190 --> 03:41.030
거기서 봐요
