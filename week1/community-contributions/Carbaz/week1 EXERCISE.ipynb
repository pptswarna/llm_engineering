{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Globals\n",
    "# (This are not constants as they actually are \"variable\", and \"global\" is a technical\n",
    "#  term used on Python to refer them programmatically in case of doubts with \"locals\".)\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_OLLAMA = 'llama3.2'\n",
    "\n",
    "SERVER_GPT = None  # Set to `None` for OpenAI default.\n",
    "SERVER_OLLAMA = 'http://localhost:11434/v1'\n",
    "\n",
    "API_KEY_GPT = os.getenv('OPENAI_API_KEY')\n",
    "API_KEY_OLLAMA = 'Ollama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LLM clients.\n",
    "\n",
    "gpt_client = OpenAI(base_url=SERVER_GPT, api_key=API_KEY_GPT)\n",
    "ollama_client = OpenAI(base_url=SERVER_OLLAMA, api_key=API_KEY_OLLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt.\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a expert in Python programming who's here to assist me on my coding tasks.\n",
    "\n",
    "I'll be asking you about code snippets or code concepts and you'll answer me on a simple,\n",
    "but clear and precise way.\n",
    "\n",
    "If I provide a code snippet, you'll explain the snippet given and provide suggestions\n",
    "about it.\n",
    "\n",
    "If I just provide a concept you will explain the concept and provide examples of usage.\n",
    "\n",
    "Finally, your answer will always be formatted in markdown, using \"fenced code blocks\"\n",
    "at all times while presenting code even on one liners.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query user question and Get Llama 3.2 to answer.\n",
    "\n",
    "# Some short of pop up requiring the input should appear.\n",
    "# (Aspect may vary depending on your system and/or working environment or IDE)\n",
    "\n",
    "question = input(\"What do you want to know today? \")\n",
    "print(f\"User's question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = gpt_client.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    # response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")  # Why removing those???\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "response = ollama_client.chat.completions.create(\n",
    "    model=MODEL_OLLAMA,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering-uBVcpfO6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
